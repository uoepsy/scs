---
title: "<b> Introduction to the Linear Model </b>"
subtitle: "DPUK Spring Academy<br><br> "
author: "Tom Booth, Josiah King, Umberto Noe"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "April 2024"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.height = 6)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #  base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
    base_color = "#95A5A6", #intro
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)

library(tidyverse)
library(kableExtra)
library(car)

theme_set(theme_gray(base_size = 15))
```


# Overview

- Day 1: What is a linear model?
- Day 2: But I have more variables, what now?
- Day 3: Interactions
- Day 4: Is my model any good?


---
class: center, middle
# Day 1
**What is a linear model?**

---
class: inverse, center, middle

<h2>Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# What is a model?
+ Pretty much all statistics is about models.

+ A model is an idea about the way the world is.
  + A formal representation of a system or relationships

+ Typically we represent models as functions.
  + We input data
  + Specify a set of relationships
  + We output a prediction

---
# An Example
+ To think through these relations, we can use a simple example.

+ Suppose I have a model for growth of babies.<sup>1</sup>

$$
Length = 55 + 4 * Month
$$

.footnote[
[1] Length is measured in cm.
]

---
# Visualizing a model

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
fun1 <- function(x) 55 + (4*x)
m1 <- ggplot(data = data.frame(x=0), aes(x=x)) +
  stat_function(fun = fun1) +
  xlim(0,24) +
  ylim(0,150) +
  ylab("Length (cm)") +
  xlab("Age (months)") #+
  #geom_point(colour = "red", size = 3) #+
 # geom_segment(aes(x = x, y = fx, xend = x, yend = 0), 
  #             arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  #geom_segment(aes(x = x, y = fx, xend = 0, yend = fx), 
   #            arrow=arrow(type = "closed", length = unit(0.25, "cm")))
m1
```
]

.pull-right[

{{content}}
]

--
+ The black line represents our model
{{content}}

--
+ The x-axis shows `Age` $(x)$
{{content}}

--
+ The y-axis values for `Length` our model predicts
{{content}}



---
# Models as "a state of the world"
+ Let's suppose my model is true.
  + That is, it is a perfect representation of how babies grow.
  
+ My models creates predictions.

+ **IF** my model is a true representation of the world, **THEN** data from the world should closely match my predictions.


---
# Predictions and data

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
m1+
  geom_segment(aes(x = 11, y = 99, xend = 11, yend = 0),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  geom_segment(aes(x = 11, y = 99, xend = 0, yend = 99),
               col = "red", lty = 2,
               arrow=arrow(type = "closed", length = unit(0.25, "cm")))
```
]

.pull-right[

```{r, echo=FALSE}
tibble(
  Age = seq(10, 12, 0.25)
) %>%
  mutate(
    Prediction = 55 + (Age*4)
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

???
+ Our predictions are points which fall on our line (representing the model, as a function)
+ Here the arrows are showing how we can use the model to find a predicted value.
+ we find the value of the input on the x-axis (here 11), read up to the line, then across to the y-axis




---
# Predictions and data

.pull-left[

+ Consider the predictions when the children get a lot older...

{{content}}

]


.pull-right[
```{r, echo=FALSE}
tibble(
  Age = seq(216,300, 12)
) %>%
  mutate(
    Year = Age/12,
    Prediction = 55 + (Age*4),
    Prediction_M = Prediction/100
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

--
+ What do you think this would mean for our actual data?
{{content}}

--
+ Will the data fall on the line?
{{content}}


---
# How good is my model?
+ How might we judge how good our model is?

  1. Model is represented as a function
  
  2. We see that as a line (or surface if we have more things to consider)
  
  3. That yields predictions (or values we expect if our model is true)
  
  4. We can collect data
  
  5. If the predictions do not match the data (points deviate from our line), that says something about our model.



---
# Linear model
+ The linear model is the workhorse of statistics.

+ When using a linear model, we are typically trying to explain variation in an **outcome** (Y, dependent, response) variable, using one or more **predictor** (x, independent, explanatory) variable(s).


---
# Example

.pull-left[

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)

kable(test)
```

]

.pull-right[

**Simple data**

+ `student` = ID variable unique to each respondent

+ `hours` = the number of hours spent studying. This will be our predictor ( $x$ )

+ `score` = test score ( $y$ )

**Question: Do students who study more get higher scores on the test?**
]

---
# Scatterplot of our data

.pull-left[
```{r, echo=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  ylab("Test Score") +
  xlab("Hours Studied")

```
]

.pull-right[

{{content}}

]

--

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  stat_smooth(method="lm", se=FALSE, col="red") +
  ylab("Test Score") +
  xlab("Hours Studied")

```

{{content}}

???
+ we can visualize our data. We can see points moving bottom left to top right
+ so association looks positive
+ Now let's add a line that represents the best model

---
# Definition of the line
+ The line can be described by two values:

+ **Intercept**: the point where the line crosses $y$, and $x$ = 0

+ **Slope**: the gradient of the line, or rate of change

???
+ In our example, intercept = for someone who doesn't study, what score will they get?
+ Slope = for every hour of study, how much will my score change

---
# Intercept and slope

```{r, echo=FALSE, message=FALSE, warning=FALSE}

intercept <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 3, slope = .3) +
  geom_abline(intercept = 4, slope = .3) + 
  geom_abline(intercept = 5, slope = .3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Different intercepts, same slopes")

slope <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 4, slope = .3) +
  geom_abline(intercept = 4, slope = 0) + 
  geom_abline(intercept = 4, slope = -.3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Same intercepts, different slopes")

```

.pull-left[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
intercept

```

]

.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
slope

```

]

---
# How to find a line?
+ The line represents a model of our data.
  + In our example, the model that best characterizes the relationship between hours of study and test score.

+ In the scatterplot, the data is represented by points.

+ So a good line, is a line that is "close" to all points.


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$


+ $y_i$ = the outcome variable (e.g. `score`) 

+ $x_i$ = the predictor variable, (e.g. `hours`)

+ $\beta_0$ = intercept

+ $\beta_1$ = slope

+ $\epsilon_i$ = residual (we will come to this shortly)

where $\epsilon_i \sim N(0, \sigma)$ independently.
  + $\sigma$ = standard deviation (spread) of the errors
  + The standard deviation of the errors, $\sigma$, is constant


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ **Why do we have $i$ in some places and not others?**


--

+ $i$ is a subscript to indicate that each participant has their own value.

+ So each participant has their own: 
    + score on the test ( $y_i$ )
    + number of hours studied ( $x_i$ ) and
    + residual term ( $\epsilon_i$ )

--
+ **What does it mean that the intercept ( $\beta_0$ ) and slope ( $\beta_1$ ) do not have the subscript $i$?**

--

+ It means there is one value for all observations.
    + Remember the model is for **all of our data**

---
# What is $\epsilon_i$?

.pull-left[
+ $\epsilon_i$, or the residual, is a measure of how well the model fits each data point.

+ It is the distance between the model line (on $y$-axis) and a data point.

+ $\epsilon_i$ is positive if the point is above the line (red in plot)

+ $\epsilon_i$ is negative if the point is below the line (blue in plot)

]


.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")


```

]

???
+ comment red = positive and bigger (longer arrow) model is worse
+ blue is negative, and smaller (shorter arrow) model is better
+ key point to link here is the importance of residuals for knowing how good the model is
+ Link to last lecture in that they are the variability 
+ that is the link into least squares


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2>Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# Principle of least squares

+ The numbers $\beta_0$ and $\beta_1$ are typically **unknown** and need to be estimated in order to fit a line through the point cloud.

+ We denote the "best" values as $\hat \beta_0$ and $\hat \beta_1$

+ The best fitting line is found using **least squares**
    + Minimizes the distances between the actual values of $y$ and the model-predicted values of $\hat y$
    + Specifically minimizes the sum of the *squared* deviations

---
# Principle of least squares

+ Actual value = $y_i$

+ Model-predicted value = $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$

+ Deviation or residual = $y_i - \hat y_i$

+ Minimize the **residual sum of squares**, $SS_{Residual}$, which is

$$SS_{Residual} = \sum_{i=1}^{n} [y_i - (\hat \beta_0 + \hat \beta_1 x_{i})]^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

---
# Data, predicted values and residuals

+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$ = the y-value on the line at specific values of $x$
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example, given you study for 4 hrs, what test score does our model predict you will get.

+ Residual = Difference between $y_i$ and $\hat{y}_i$. So;

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

???
+ these are important distinctions for understanding linear models
+ return to them a lot.


---
# Data, predicted values and residuals

.pull-left[

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ Squared distance of each point from the predicted value.
]

.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")

```

]



---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2>Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# `lm` in R
```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
```

```{r}
res <- lm(score ~ hours, data = test)
summary(res)
```

---
# Interpretation

+ **Slope is the number of units by which Y increases, on average, for a unit increase in X.**

--
    + Unit of Y = 1 point on the test
    + Unit of X = 1 hour of study
    
--

+ So, for every hour of study, test score increases on average by 1.055 points.

--

+ **Intercept is the expected value of Y when X is 0.**

--

    + X = 0 is a student who does not study.

--

+ So, a student who does no study would be expected to score 0.40 on the test.

???
+ So we know in a general sense what the intercept and slope are, but what do they mean with respect to our data and question?

---
# Note of caution on intercepts
+ In our example, 0 has a meaning.
    + It is a student who has studied for 0 hours.
  + But it is not always the case that 0 is meaningful.

+ Suppose our predictor variable was not hours of study, but age.

+ **A person of 0 age has a test score of 0.40.**

---
# Unstandardized vs standardized coefficients
- In this example, we have unstandardized $\hat \beta_1$.

+ We interpreted the slope as the change in $y$ units for a unit change in $x$
  + Where the unit is determined by how we have measured our variables.

+ However, sometimes we may want to represent our results in standard units.
  + If the scales of our variables are arbitrary.
  + If we want to compare the effects of variables on different scales.


---
# Standardized results
+ We can either...

+ Standardized coefficients: 

$$\hat{\beta_1^*} = \hat \beta_1 \frac{s_x}{s_y}$$

+ where;
  + $\hat{\beta_1^*}$ = standardized beta coefficient
  + $\hat \beta_1$ = unstandardized beta coefficient
  + $s_x$ = standard deviation of $x$
  + $s_y$ = standard deviation of $y$

---
# Standardizing the variables

+ Alternatively, for continuous variables, transforming both the IV and DV to $z$-scores (mean=0, SD=1) prior to fitting the model yields standardised betas.

+ $z$-score for $x$:

$$z_{x_i} = \frac{x_i - \bar{x}}{s_x}$$

+ and the $z$-score for $y$:

$$z_{y_i} = \frac{y_i - \bar{y}}{s_y}$$

+ That is, we divide the individual deviations from the mean by the standard deviation

---
# `lm()` using z-scores

```{r}
test <- test %>%
  mutate(
    z_score = scale(score, center = T, scale = T),
    z_hours = scale(hours, center = T, scale = T)
  )

res_z <- lm(z_score ~ z_hours, data = test)
summary(res_z)$coefficients
```


---
#  Interpreting standardized regression coefficients  

+ $b_0$ (intercept) = zero when all variables are standardized:

+ The interpretation of the coefficients becomes the increase in $y$ in standard deviation units for every standard deviation increase in $x$

+ So, in our example:

>**For every standard deviation increase in hours of study, test score increases by 0.72 standard deviations**

---
# Relationship to r
+ Standardized slope ( $\hat \beta_1^*$ ) = correlation coefficient ( $r$ ) for a linear model with a single continuous predictor.

+ In our example, $\hat \beta_{hours}^*$ = 0.72

```{r}
cor(test$hours, test$score)
```

+ $r$ is a standardized measure of linear association

+ $\hat \beta_1^*$ is a standardized measure of the linear slope.


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2>Part 4: Single binary predictor = t-test</h2>

---
# Binary variable
+ Binary variable is a categorical variable with two levels.

+ Traditionally coded with a 0 and 1
  + Referred to as dummy coding
  + We will come back to this for categorical variables with 2+ levels

--

+ Why 0 and 1?
  + Quick version: It has some nice properties when it comes to interpretation.


---
# Extending our example

.pull-left[
+ Our in class example so far has used test scores and revision time for 10 students.

+ Let's say we collect this data on 150 students.

+ We also collected data on who they studied with;
  + 0 = alone
  + 1 = with others
  
+ So our variable `study` is a binary
]

.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}
df <- read_csv("./dapr2_lec07.csv") 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
slice(df, 1:10)
```

]

---
#  LM with binary predictors 
+ Now we can ask the question:

  + **Do students who study with others score better than students who study alone?**

$$score_i = \beta_0 + \beta_1 study_{i} + \epsilon_i$$

---
# In `R`

```{r}
res2 <- lm(score ~ study, data = df)
summary(res2)
```


---
# Interpretation

.pull-left[
+ As before, the intercept $\hat \beta_0$ is the expected value of $y$ when $x=0$

+ What is $x=0$ here?
  + It is the students who study alone.

+ So what about $\hat \beta_1$?

+ **Look at the output on the right hand side.** 
  + What do you notice about the difference in averages?

]

.pull-right[
```{r warning=FALSE, message=FALSE}
df %>%
  group_by(., study) %>% #<<
  summarise(
    Average = round(mean(score),4) #<<
  )
```


]


---
# Interpretation
+ $\hat \beta_0$ = predicted expected value of $y$ when $x = 0$
  + Or, the mean of group coded 0 (those who study alone)
  
+ $\hat \beta_1$ = predicted difference between the means of the two groups.
  + Group 1 - Group 0 (Mean `score` for those who study with others - mean `score` of those who study alone)
  
+ Notice how this maps to our question. 
  + Do students who study with others score better than students who study alone?
  

---
#  Visualize the model

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(latex2exp)
gpM <- df %>%
  group_by(study) %>%
  summarise(
    score = mean(score)
  )

df %>%
  ggplot(., aes(x=factor(study), y=score, colour = study)) +
  geom_point(alpha=0.4) +
  labs(x = "\n Study", y = "Test Score \n") +
  ylim(0,10) +
  scale_x_discrete(labels = c("alone", "others")) +
  theme(legend.position = "none") +
  geom_jitter(width = .1, height = 0, alpha=0.4) +
  geom_errorbar(data = gpM, width=0.6, aes(ymax=..y..,ymin=..y..), size=1)+
  geom_segment(x=1.5, y=gpM[[1,2]], xend=1.5, yend=gpM[[2,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[1,2]], xend=1.52, yend=gpM[[1,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[2,2]], xend=1.52, yend=gpM[[2,2]], size =1, col="red") +
  geom_text(x=1.55, y = 5.5, label = TeX('$\\hat{\\beta}_1$'), size=5, col = "red") +
  geom_text(x=0.65, y = gpM[[1,2]] , label = TeX('$\\hat{\\beta}_0$'), size=5, col = "red")

```


---
# Hold on... it's a t-test

```{r}
df %>%
  t.test(score ~ study, .)
```

???
Yup!


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 2
**But I have more variables, now what?**

---
class: inverse, center, middle

<h2>Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>

---
#  Linear model with more predictors (multiple regression)
+ The aim of a linear model is to explain variance in an outcome.

+ In simple linear models, we have a single predictor, but the model can accommodate (in principle) any number of predictors. 

+ However, when we include multiple predictors, those predictors are likely to correlate

+ Thus, a linear model with multiple predictors finds the optimal prediction of the outcome from several predictors, **taking into account their redundancy with one another**


---
#  Uses of multiple regression 
+ **For prediction:** multiple predictors may lead to improved prediction. 

+ **For theory testing:** often our theories suggest that multiple variables together contribute to variation in an outcome

+ **For covariate control:** we might want to assess the effect of a specific predictor, controlling for the influence of others.
	+ E.g., effects of personality on health after removing the effects of age and sex


---
#  Extending the regression model 

+ Our model for a single predictor:

$$y_i = \beta_0 + \beta_1 x_{1i} + \epsilon_i$$ 

+ is extended to include additional $x$'s:

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i$$  

+ For each $x$, we have an additional $\beta$
  + $\beta_1$ is the coefficient for the 1st predictor
  + $\beta_2$ for the second etc.


---
#  Interpreting coefficients in multiple regression 

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_j x_{ji} + \epsilon_i$$

+ Given that we have additional variables, our interpretation of the regression coefficients changes a little

+ $\beta_0$ = the predicted value for $y$ **all** $x$ are 0.
	
+ Each $\beta_j$ is now a **partial regression coefficient**
	+ It captures the change in $y$ for a one unit change in , $x$ **when all other x's are held constant**

+ What does holding constant mean? 
  + Refers to finding the effect of the predictor when the values of the other predictors are fixed
		+ It may also be expressed as the effect of **controlling for**, or **partialling out**, or **residualizing for** the other $x$'s

+ With multiple predictors `lm` isolates the effects and estimates the unique contributions of predictors. 

---
# Example with interpretations 

```{r, echo=FALSE}
set.seed(19981)
dept <- rbinom(100, 1, 0.4)
location <- rbinom(100, 1, 0.35)
serv <- rnorm(100, 0, 1.4)
serv <- serv+3.5
dl <- dept*location

omit <- rbinom(100, 1, .3)

b0 = 20
b1 = 3.1
b2 = -4.5
b3 = -4.2
b4 = -6.5
b5 = 2

eps <- rnorm(100, 0, 4)

salary <- b0 + (b1*serv) + (b2*dept) + (b3*location) + (b4*dl) + (b5*omit) + eps
salary <- salary+20

salary3 <- tibble(
  ID = paste("ID", 101:200, sep=""),
  salary = round(salary,0),
  department = factor(dept,labels = c("Accounts", "Manager")),
  location = factor(location, labels = c("London", "Birmingham")),
  serv = round(serv,1)
)
```

.pull-left[
+ Suppose I am conducting a study on work place factors that predict salary.

+ $y$ = salary (unit = thousands of pounds)

+ $x_1$ = years of service

+ $x_2$ = Department (0 = Store managers, 1 = Accounts)

+ $x_3$ = Location (0 = Birmingham, 1 = London)
]

.pull-right[

```{r}
salary3 %>%
  slice(1:10)
```

]


---
# Our model
```{r}
res_multi <- lm(salary ~ serv + department + location, data = salary3)
```

---
# Our model
```{r}
summary(res_multi)
```

---
# Our model

.pull-left[
+ $b_0$: The predicted salary for member of accounts, in London, with 0 years service is £41,684 

+ $b_1$: For each year of service, salary increases by £2953, holding deparment and location constant.

+ $b_2$: Holding years of service and location constant, store managers earn £6952 pounds less than accounts.

+ $b_3$: Holding years of service and department constant, those in Birmingham earn £6283 pounds less than those in London.

]

.pull-right[

```{r, echo = FALSE}
multi_sum <- summary(res_multi)
round(multi_sum$coefficients,3)[,1:3]
```


]

---
# Categorical predictors with 2+ levels

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MASS)
set.seed(1066)
mu <- c(54, 2)
Sigma <- matrix(c(7, 1.4,
                  1.4, .5), byrow = T, ncol = 2)

rawvars <- mvrnorm(n=10000, mu=mu, Sigma=Sigma)

set.seed(7284)
df <- rawvars[sample(nrow(rawvars), 100),]

set.seed(7284)
dum_dat <- tibble(
  ID = c(paste("ID", 101:200, sep = "")),
  exam = round(df[,1], 0),
  method = round(df[,2], 0)
) %>%
  mutate(
    method = factor(if_else(method <= 1, 1,
                     if_else(method >= 3, 3, 2))),
    dummy1 = if_else(method == 2, 1, 0), 
    dummy2 = if_else(method == 3, 1, 0),
  )
detach("package:MASS", unload = T)
```

+ When we have a categorical variable with 2+ levels, we will typically assign integers

+ For example: What city do you live in?
  + 1 = Edinburgh; 2 = Glasgow, 3 = Birmingham etc.
  + Note these numbers are not meaningful, they just denote groups

+ When analysing a categorical predictor with $k$ levels, we need to take an additional step.

+ This step involves applying a coding scheme, where by each regressor = a difference in means between levels, or sets of levels.

+ There are lots of coding schemes.
  + We will just look at dummy coding (R default)

---
#  Dummy coding 
+ Dummy coding uses 0's and 1's to represent group membership
	+ One level is chosen as a baseline
	+ All other levels are compared against that baseline
	
+ Notice, this is identical to binary variables already discussed.

+ Dummy coding is simply the process of producing a set of binary coded variables

+ For any categorical variable, we will create $k$-1 dummy variables
  + $k$ = number of levels

---
#  Dummy coding 
+ Imagine 100 students took an exam and were each assigned to use one of three `study methods`
	+ 1 = Notes re-reading 
	+ 2 = Notes summarising
	+ 3 = Self-testing ([see here](https://www.psychologicalscience.org/publications/journals/pspi/learning-techniques.html))

```{r tbl23, echo = FALSE}
dummy <- tibble(
  Level = c("Notes re-reading", "Notes summarising", "Self-testing"),
  D1 = c(0,1,0),
  D2 = c(0,0,1)
) 

kable(dummy)%>%
  kable_styling(., full_width = F)
```


---
#  Dummy coding with `lm` 

+ `lm` automatically applies dummy coding when you include a variable of class `factor` in a model.

+ It selects the first group as the baseline group

+ We write:

```{r, eval=FALSE}

dummy1 <- lm(exam ~ method, data = dum_dat)

```


+ And `lm` does all the dummy coding work for us

---
#  Dummy coding with `lm`

.pull-left[

```{r, echo=FALSE}

dum_dat %>%
  group_by(method) %>%
  summarise(
    Mean = round(mean(exam), 3)
  )
```


+ The intercept is the mean of the baseline group (notes re-reading)

+ The coefficient for `method2` is the mean difference between the notes summarising group and the baseline group

+ The coefficient for `method3` is the mean difference between the self-test group and the baseline group

]

.pull-right[

```{r}
dummy1 <- lm(exam ~ method, data = dum_dat)
dummy1
```

]


---
#  Dummy coding with `lm` (full results)

```{r}
summary(dummy1)
```


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2>Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>


---
#  Evaluating individual predictors 
+ Steps in hypothesis testing:

--
  + Research questions
    
--
  
  + Statistical hypothesis
    
--
  
  + Define the null
    
--
  
  + Calculate an estimate of effect of interest.
  
--
  
  + Calculate an appropriate test statistic.
    
--
  
  + Evaluate the test statistic against the null.
    

---
# Research question and hypotheses

+ **Statistical hypotheses** are testable mathematical statements.

+ In typical testing in Psychology, we define have a **null ( $H_0$ )** and an **alternative ( $H_1$ )** hypothesis.

+ $H_0$ is precise, and states a specific value for the effect of interest.

+ $H_1$ is not specific, and simply says "something else other than the null is more likely"


---
# Defining null

+ Conceptually:
	+ If $x$ yields no information on $y$, then $\beta_1 = 0$
	
+ **Why would this be the case?**

--
	+ $\beta$ gives the predicted change in $y$ for a unit change in $x$.
	+ If $x$ and $y$ are unrelated, then a change in $x$ will not result in any change to the predicted value of $y$
	+ So for a unit change in $x$, there is no (=0) change in $y$.
	
+ We can state this formally as a null and alternative:

$$H_0: \beta_1 = 0$$
$$H_1: \beta_1 \neq 0$$


---
# Point estimate and test statistic

+ We have already discussed $\hat \beta_1$.

+ The associated test statistic to for $\beta$ coefficients is a $t$-statistic

$$t = \frac{\hat \beta}{SE(\hat \beta)}$$

+ where

  + $\hat \beta$ = any beta coefficient we have calculated
  + $SE(\hat \beta)$ = standard error of $\beta$ 

+ The standard error (SE) provides a measure of sampling variability
  + Smaller SE's suggest more precise estimate (=good)
  + For details on the calculation of $SE(\hat \beta)$ , see linked material.


---
# Back to the example

```{r}
summary(res_multi)
```


---
# Sampling distribution for the null

+ Now we have our $t$-statistic, we need to evaluate it.

+ For that, we need sampling distribution for the null.

+ For $\beta$, this is a $t$-distribution with $n-k-1$ degrees of freedom.
	+ Where $k$ is the number of predictors, and the additional -1 represents the intercept.

--

+ So for our model above, we have 3 predictors, and n = 100 
  + this is $n-k-1$ = $100-3-1$
  + 96

---
# Visualize our result: service

```{r, echo=FALSE}
ggplot() + 
  xlim(-12, 12) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=96)) +
  stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.025, 96), -12),
                alpha=.25,
                fill = "blue",
                args = list(df=96)) +
    stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.975, 96), 12),
                alpha=.25,
                fill = "blue",
                args = list(df=96)) +
  geom_vline(xintercept = 10.105, col="red") +
  xlab("\n t") +
  ylab("") +
  ggtitle("t-distribution (df=96); t-statistic (10.105; red line)")
```


???
+ discuss this plot.
+ remind them of 2-tailed
+ areas
+ % underneath each end
+ comment on how it would be different one tailed
+ remind about what X is, thus where the line is


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2>Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>

---
#  Quality of the overall model 
+ When we measure an outcome ( $y$ ) in some data, the scores will vary (we hope).
  + Variation in $y$ = total variation of interest.

--

+ The aim of our linear model is to build a model which describes $y$ as a function of $x$.
	+ That is we are trying to explain variation in $y$ using $x$.

--

+ But it won't explain it all.
  + What is left unexplained is called the residual variance.

--

+ So we can breakdown variation in our data based on sums of squares as;

$$SS_{Total} = SS_{Model} + SS_{Residual}$$

---
#  Coefficient of determination 
+ One way to consider how good our model is, would be to consider the proportion of total variance our model accounts for. 

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$$

+ $R^2$ = coefficient of determination

--

  + Quantifies the amount of variability in the outcome accounted for by the predictors.
  + More variance accounted for, the better.
  + Represents the extent to which the prediction of $y$ is improved when predictions are based on the linear relation between $x$ and $y$.




---
# Total Sum of Squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

+ Squared distance of each data point from the mean of $y$.

+ Mean is our baseline. 

+ Without any other information, our best guess at the value of $y$ for any person is the mean.

]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Total}')) +
  geom_hline(aes(yintercept = mean(score)),color="blue", size=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(rep(mean(score),10))), color = "red", lty =2)
```

]


---
# Residual sum of squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ Which you may recognise.

+ Squared distance of each point from the predicted value.
]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Residual}')) +
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", size=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(res$fitted.values)), color = "red", lty =2)

```

]



---
# Model sums of squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$

+ That is, it is the deviance of the predicted scores from the mean of $y$.

+ But it is easier to simply take:

$$SS_{Model} = SS_{Total} - SS_{Residual}$$

]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Model}')) +
  geom_hline(aes(yintercept = mean(score)),color="blue", size=1) + 
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", size=1) + 
  geom_segment(aes(x = hours, c(res$fitted.values), xend = hours, yend = c(rep(mean(score),10))), color = "red", lty =2)
```

]




---
#  Coefficient of determination: Our example

```{r}
summary(res_multi)
```


---
#  Adjusted $R^2$ 
+ We can also compute an adjusted $R^2$ when our lm has 2+ predictors.
  + $R^2$ is an inflated estimate of the corresponding population value

+ Due to random sampling fluctuation, even when $R^2 = 0$ in the population, it's value in the sample may $\neq 0$ 

+ In **smaller samples** , the fluctuations from zero will be larger on average

+ With **more IVs** , there are more opportunities to add to the positive fluctuation


$$\hat R^2 = 1 - (1 - R^2)\frac{N-1}{N-k-1}$$

+ Adjusted $R^2$ adjusts for both sample size ( $N$ ) and number of predictors ( $k$ )

---
# Adjusted R-square: Our example

```{r}
summary(res_multi)
```


---
#  Significance of the overall model 
+ The test of the individual predictors (IVs, or $x$'s) does not tell us if the overall model is significant or not.
	+ Neither does R-square
	+ But both are indicative

+ To test the significance of the model as a whole, we conduct an $F$-test.

---
#  F-ratio
+ $F$-ratio tests the null hypothesis that all the regression slopes in a model are all zero

+ $F$-ratio is a ratio of the explained to unexplained variance:

$$F = \frac{MS_{Model}}{MS_{Residual}}$$

+ Where MS = mean squares

--

+ **What are mean squares?**
  + Mean squares are sums of squares calculations divided by the associated degrees of freedom.
  + The degrees of freedom are defined by the number of "independent" values associated with the different calculations.


---
# F-ratio
+ Bigger $F$-ratios indicate better models.
  + It means the model variance is big compared to the residual variance.

--

+ The null hypothesis for the model says that the best guess of any individuals $y$ value is the mean of $y$ plus error.
	+ Or, that the $x$ variables carry no information collectively about $y$.

--

+ $F$-ratio will be close to 1 when the null hypothesis is true
  + If there is equivalent residual to model variation, $F$=1
	+ If there is more model than residual $F$ > 1

--

+ $F$-ratio is then evaluated against an $F$-distribution with $df_{Model}$ and $df_{Residual}$ and a pre-defined $\alpha$

--

+ Testing the $F$-ratio evaluates statistical significance of the overall model


---
#  Our example 

```{r}
summary(res_multi)
```


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2>Part 4: Comparing models </h2>

---
# Why might we compare models?

+ Suppose we wanted to know the effect of a key predictor, after first having controlled for some covariates.
  + In many places described as hierarchical regression

+ We can't do this with the skills learned so far. 

+ We can using model comparison tools. 

---
# $F$-test as an incremental test

+ One important way we can think about the $F$-test and the $F$-ratio is as an incremental test against an "empty" or null model.

+ A null or empty model is a linear model with only the intercept.
  + In this model, our predicted value of the outcome for every case in our data set, is the mean of the outcome.
  + That is, with no predictors, we have no information that may help us predict the outcome.
  + So we will be "least wrong" by guessing the mean of the outcome.

+ An empty model is the same as saying all $\beta$ = 0.

+ So in this way, the $F$-test we have already seen **is comparing two models**.

+ We can extend this idea, and use the $F$-test to compare two models that contain different sets of predictors.
  + This is the **incremental $F$-test**

---
# Incremental $F$-test
.pull-left[
+ The incremental $F$-test evaluates the statistical significance of the improvement in variance explained in an outcome with the addition of further predictor(s)

+ It is based on the difference in $F$-values between two models.
  + We call the model with the additional predictor(s) model 1 or full model
  + We call the model without model 0 or restricted model

]

.pull-right[
$$F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F}$$



$$
\begin{align}
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
]


---
# Incremental $F$-test in R

+ In order to apply the $F$-test for model comparison in R, we use the `anova()` function.

+ `anova()` takes as its arguments models that we wish to compare
  + Here we will show examples with 2 models, but we can use more.

---
# Example
```{r}
m0 <- lm(salary ~1, data = salary3)
m1 <- lm(salary ~ location + department, data = salary3)
m2 <- lm(salary ~ location + department + serv, data = salary3)

anova(m0, m1, m2)

```


---
# Other uses for `anova()`

+ Another main use of the `anova()` function is when analysing linear models with categorical variables with 2+ levels.

+ This type of model is common in experimental designs
  + It is the type of model typically described in the literature as an ANOVA model (or analysed using ANOVA)
  + Note ANOVA = linear model
  
---
# Categorical data and `anova()`

```{r}
anova(dummy1)
```


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 3
**Interactions (uh-oh)**

---
class: inverse, center, middle

<h2>Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>

---
#  Lecture notation 

+ For today, we will work with the following equation and notation:

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ $y$ is a continuous outcome

+ $x$ is our first predictor

+ $z$ is our second predictor
	
+ $xz$ is their product or interaction predictors

---
#  General definition of interaction

+ When the effects of one predictor on the outcome differ across levels of another predictor.

+ Note interactions are symmetrical. 

+ What does this mean?
  + We can talk about interaction of X with Z, or Z with X.
  + These are identical.

---
#  General definition 

+ Categorical*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the outcome is different across levels of a categorical predictor.

--

+ Continuous*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the outcome changes as the values of a second continuous predictor change.
	+ May have heard this referred to as moderation.

--

+ Categorical*categorical interaction:
	+ There is a difference in the differences between groups across levels of a second factor.
	+ We will discuss this in the context of linear models for experimental design

---
# Why are we interested in interactions?
+ Often we have theories or ideas which relate to an interaction.

+ For example: 
  + different relationships of mood state to cognitive score dependent on disease status
  + different rates of cognitive decline by disease status. 

+ Questions like these would be tested via inclusion of an interaction term in our model.

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2>Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>


---
#  Interpretation: Categorical*Continuous 


$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Where $z$ is a binary predictor

  + $\beta_0$ = Value of $y$ when $x$ and $z$ are 0

  + $\beta_1$ = Effect of $x$ (slope) when $z$ = 0 (reference group)

  + $\beta_2$ = Difference intercept between $z$ = 0 and $z$ = 1, when $x$ = 0.

  + $\beta_3$ = Difference in slope across levels of $z$

---
#  Example: Categorical*Continuous 

.pull-left[
+ Suppose I am conducting a study on how years of service within an organisation predicts salary in two different departments, accounts and store managers.

+ y = salary (unit = thousands of pounds)

+ x = years of service

+ z = Department (0=Store managers, 1=Accounts)
]

.pull-right[

```{r, warning=FALSE, message=FALSE, echo=FALSE}
salary1 <- read_csv("./salary_lec.csv")
salary1 <- salary1 %>%
  mutate(
    service = round(service,1),
    salary = round(salary, 3),
    dept = factor(dept, labels = c("StoreManager", "Accounts"))
  )
```


```{r}
salary1 %>%
  slice(1:10)
```

]

---
#  Visualize the data

.pull-left[

```{r eval=FALSE, message=FALSE, warning=FALSE}
salary1 %>%
  ggplot(., aes(x = service, y = salary, 
                colour = dept)) +
  geom_point() +
  xlim(0,8) +
  labs(x = "Years of Service", 
       y = "Salary (£1000)")
```


]

.pull-right[

```{r echo=FALSE}
salary1 %>%
  ggplot(., aes(x = service, y = salary, 
                colour = dept)) +
  geom_point() +
  xlim(0,8) +
  labs(x = "Years of Service", 
       y = "Salary (£1000)")
```

]





---
# Example: Full results

```{r, eval=FALSE}
int <- lm(salary ~ service + dept + service*dept, data = salary1)
summary(int) #<<
```

---
# Example: Full results

```{r, echo=FALSE}
int <- lm(salary ~ service + dept + service*dept, data = salary1)
summary(int) #<<
```


---
#  Example: Categorical*Continuous 

.pull-left[
+ **Intercept** ( $\beta_0$ ): Predicted salary for a store manager (`dept`=0) with 0 years of service is £16,894.

+ **Service** ( $\beta_1$ ): For each additional year of service for a store manager (`dept` = 0), salary increases by £2,736.

+ **Dept** ( $\beta_2$ ): Difference in salary between store managers (`dept` = 0) and accounts (`dept` = 1) with 0 years of service is £4,489.
	
+ **Service:dept** ( $\beta_3$ ): The difference in slope. For each year of service, those in accounts (`dept` = 1) increase by an additional £3,117.

]


.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
salary1 %>%
  ggplot(., aes(x = service, y = salary, colour = dept)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE)+
  xlim(0,8) +
  labs(x = "Years of Service", y = "Salary (£1000)") 
```
]



---
#  Marginal effects 

+ Recall when we have a linear model with multiple predictors, we interpret the $\beta_j$ as the effect "holding all other variables constant".

+ Also note, with interactions, the effect of $x$ on $y$ changes dependent on the value of $z$.
  + More formally, it is the effect of $x$ is conditional on $z$ and vice versa.

+ What this means is that we can no longer talk about holding an effect constant.
  + In the presence of an interaction, by definition, this effect changes.

+ So where as in a linear model without an interaction $\beta_j$ = main effects, with an interaction we refer to **marginal** or **conditional** effects.


---
#  Centering predictors

**Why centre?** 
+ Meaningful interpretation.

  + Interpretation of models with interactions involves evaluation when other variables = 0.
  
  + This makes it quite important that 0 is meaningful in some way.
  	+ Note this is simple with categorical variables.
  	+ We code our reference group as 0 in all dummy variables.
  
  + For continuous variables, we need a meaningful 0 point.

---
#  Example of age 
+ Suppose I have age as a variable in my study with a range of 30 to 85.

+ Age = 0 is not that meaningful.
	+ Essentially means all my parameters are evaluated at point of birth.

+ So what might be meaningful?
	+ Average age? (mean centering)
	+ A fixed point? (e.g. 66 if studying retirement)

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2>Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>


---
#  Interpretation: Continuous*Continuous 

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Lecture notation:
  
  + $\beta_0$ = Value of $y$ when $x$ and $z$ are 0
  
  + $\beta_1$ = Effect of $x$ (slope) when $z$ = 0
  
  + $\beta_2$ = Effect of $z$ (slope) when $x$ = 0
  
  +  $\beta_3$ = Change in slope of $x$ on $y$ across values of $z$ (and vice versa).
	    + Or how the effect of $x$ depends on $z$ (and vice versa)


---
#  Example: Continuous*Continuous 

+ Conducting a study on how years of service and employee performance ratings predicts salary in a sample of managers.

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ $y$ = Salary (unit = thousands of pounds ).

+ $x$ = Years of service.

+ $z$ = Average performance ratings.


---
#  Example: Continuous*Continuous 

```{r echo=FALSE, warning=FALSE, message=FALSE}
salary2 <- read_csv("./salary2.csv")
```

```{r, echo=FALSE}
int2 <- lm(salary ~ serv*perf, data = salary2)
summary(int2)
```

---
#  Example: Continuous*Continuous 

.pull-left[

+ **Intercept**: a manager with 0 years of service and 0 performance rating earns £87,920

+ **Service**: for a manager with 0 performance rating, for each year of service, salary decreases by £10,940
  + slope when performance = 0
  
+ **Performance**: for a manager with 0 years service, for each point of performance rating, salary increases by £3,150.
  + slope when service = 0
  
+ **Interaction**: for every year of service, the relationship between performance and salary increases by £3250.

]


.pull-right[
```{r, echo=FALSE}
test <- summary(int2)
round(test$coefficients,2)
```

]

???
+ What do you notice here?
+ 0 performance and 0 service are odd values
+ lets mean centre both, so 0 = average, and look at this again.


---
# Mean centering

```{r}

salary2 <- salary2 %>%
  mutate(
    perfM = c(scale(perf, scale = F)), #<<
    servM = c(scale(serv, scale = F)) #<<
  )

int3 <- lm(salary ~ servM*perfM, data = salary2)

```


---
# Mean centering

```{r echo=FALSE}
summary(int3)
```

---
#  Example: Continuous*Continuous 

.pull-left[

+ **Intercept**: a manager with average years of service and average performance rating earns £104,850

+ **Service**: a manager with average performance rating, for every year of service, salary increases by £1,420
  + slope when performance = 0 (mean centered)
  
+ **Performance**: a manager with average years service, for each point of performance rating, salary increases by £14,450.
  + slope when service = 0 (mean centered)
  
+ **Interaction**: for every year of service, the relationship between performance and salary increases by £3,250.

]


.pull-right[
```{r, echo=FALSE}
test2 <- summary(int3)
round(test2$coefficients,2)
```

]

---
#  Plotting interactions

+ In our last block we saw we could produce a line for each group of a binary (extends to categorical) variable.

+ These are called simple slopes:
	+ **Regression of the outcome Y on a predictor X at specific values of an interacting variable Z.**

+ For a continuous variable, we could choose any values of Z. 
  + Typically we plot at the mean and +/- 1SD


---
#  `sjPlot`: Simple Slopes 

.pull-left[
```{r, eval = FALSE, warning=FALSE, message=FALSE}
library(sjPlot)
plot_model(int3, type = "int")

```
]

.pull-right[
```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(sjPlot)
plot_model(int3, type = "int")

```

]

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2>Part 4: Categorical*categorical interactions </h2>

---
#  General definition 

+ When the effects of one predictor on the outcome differ across levels of another predictor.

+ Categorical*categorical interaction:
	+ There is a difference in the differences between groups across levels of a second factor.

+ This idea of a difference in differences can be quite tricky to think about.
  + So we will start with some visualization, and then look at two examples.
  
---
# Difference in differences (1)


.pull-left[
```{r,, echo=FALSE}
dat1 <- tibble(
  location = c("London", "London", "Birmingham", "Birmingham"),
  department = c("Accounts","Manager","Accounts","Manager"),
  salary = c(50, 30, 40, 20)
)
```

```{r, echo = FALSE}
eg1 <- matrix(c(50,40,30,20),ncol=2,byrow=TRUE)
colnames(eg1) <- c("London","Birmingham")
rownames(eg1) <- c("Accounts","Manager")
eg1 <- as.table(eg1)
kable(eg1)
```

+ In each plot we look at, think about subtracting the average store managers salary (blue triangle) from the average accounts salary (red circle)

+ In both cases, it is £20,000.

+ Note, the lines are parallel
  + Remember what we have said about parallel lines...no interaction

]

.pull-right[

```{r, echo=FALSE, fig.height=6}
dat1 %>%
  ggplot(., aes(x = location, y = salary, group = department, colour = department, shape = department)) +
  geom_line() +
  geom_point(aes(size = 1.5), show.legend = FALSE) +
  ylim(c(0,60)) +
  geom_segment(aes(x= 0.9 , xend = 0.9 , y=20, yend=40), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  geom_segment(aes(x= 2.1 , xend = 2.1 , y=30, yend=50), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  annotate("text", x = 0.8 , y = 30, label = "20") +
  annotate("text", x = 2.2 , y = 40, label = "20") +
  ggtitle("No difference")

```

]

---
# Difference in differences (2)


.pull-left[
```{r,, echo=FALSE}
dat2 <- tibble(
  location = c("London", "London", "Birmingham", "Birmingham"),
  department = c("Accounts","Manager","Accounts","Manager"),
  salary = c(50, 40, 40, 20)
)
```

```{r, echo = FALSE}
eg2 <- matrix(c(50,40,40,20),ncol=2,byrow=TRUE)
colnames(eg2) <- c("London","Birmingham")
rownames(eg2) <- c("Accounts","Manager")
eg2 <- as.table(eg2)
kable(eg2)
```

+ This time we can see the difference differs.
  + £20,000 in Birmingham
  + £10,000 in London.
  
+ Note the lines are no longer parallel.
  + Suggests interaction.
  + But not crossing (so ordinal interaction)

]

.pull-right[

```{r, echo=FALSE, fig.height=6}
dat2 %>%
  ggplot(., aes(x = location, y = salary, group = department, colour = department, shape = department)) +
  geom_line() +
  geom_point(aes(size = 1.5), show.legend = FALSE) +
  ylim(c(0,60)) +
  geom_segment(aes(x= 0.9 , xend = 0.9 , y=20, yend=40), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  geom_segment(aes(x= 2.1 , xend = 2.1 , y=40, yend=50), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  annotate("text", x = 0.8 , y = 30, label = "20") +
  annotate("text", x = 2.2 , y = 45, label = "10") +
  ggtitle("Difference")

```

]

---
# Difference in differences (3)

.pull-left[
```{r,, echo=FALSE}
dat3 <- tibble(
  location = c("London", "London", "Birmingham", "Birmingham"),
  department = c("Accounts","Manager","Accounts","Manager"),
  salary = c(40, 60, 40, 20)
)
```

```{r, echo = FALSE}
eg3 <- matrix(c(40,60,40,20),ncol=2,byrow=FALSE)
colnames(eg3) <- c("London","Birmingham")
rownames(eg3) <- c("Accounts","Manager")
eg3 <- as.table(eg3)
kable(eg3)
```

+ This time we can see the difference differs.
  + £20,000 in Birmingham
  + -£20,000 in London
  
+ Note the lines are no longer parallel.
  + Suggests interaction.
  + Now crossing (so disordinal interaction)

]

.pull-right[

```{r, echo=FALSE, fig.height=6}
dat3 %>%
  ggplot(., aes(x = location, y = salary, group = department, colour = department, shape = department)) +
  geom_line() +
  geom_point(aes(size = 1.5), show.legend = FALSE) +
  ylim(c(0,60)) +
  geom_segment(aes(x= 0.9 , xend = 0.9 , y=20, yend=40), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  geom_segment(aes(x= 2.1 , xend = 2.1 , y=40, yend=60), 
               arrow=arrow(type = "closed", end = "both", length = unit(0.2, "cm")), 
               colour = "black", linetype = "dashed") +
  annotate("text", x = 0.8 , y = 30, label = "20") +
  annotate("text", x = 2.2 , y = 50, label = "-20") +
  ggtitle("Big difference")

```

]

---
#  Interpretation: Categorical*categorical interaction (dummy codes)

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ $\beta_0$ = Value of $y$ when $x$ and $z$ are 0 
  + Expected salary for Accounts in London.
  
+ $\beta_1$ = Difference between levels of $x$ when $z$ = 0 
  + The difference in salary between Accounts in London and Birmingham

+ $\beta_2$ = Difference between levels of $z$ when $x$ = 0.
  + The difference in salary between Accounts and Store managers in London.

+  $\beta_3$ = Difference between levels of $x$ across levels of $z$
  + The difference between salary in Accounts and Store managers between London and Birmingham


---
#  Example: Categorical*categorical
```{r}
int4 <- lm(salary ~ location*department, salary3)
```

```{r, echo=FALSE}
summary(int4)
```

---
#  Example: Categorical*categorical

.pull-left[
```{r, eval=FALSE}
plot_model(int4, type = "int")

```
]

.pull-right[
```{r, echo=FALSE}
plot_model(int4, type = "int")
```

]



---
#  Example: Categorical*categorical
```{r, echo=FALSE}
res <- summary(int4)
round(res$coefficients,2)
```

.pull-left[
+ $\beta_0$ = Value of $y$ when $x$ and $z$ are 0

+ Expected salary for Accounts in London is £50,670.
]


.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
salary3 %>%
  group_by(location, department) %>%
  summarise(
    Salary = mean(salary)
  ) %>%
  kable(.)
```
]

---
#  Example: Categorical*categorical
```{r, echo=FALSE}
res <- summary(int4)
round(res$coefficients,2)
```

.pull-left[
+ $\beta_1$ = Difference between levels of $x$ when $z$ = 0

+ The difference in salary between Accounts in London and Birmingham is £1,980. The salary is lower in Birmingham. 

]


.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
salary3 %>%
  group_by(location, department) %>%
  summarise(
    Salary = mean(salary)
  ) %>%
  kable(.)
```
]


---
#  Example: Categorical*categorical
```{r, echo=FALSE}
res <- summary(int4)
round(res$coefficients,2)
```

.pull-left[
+ $\beta_2$ = Difference between levels of $z$ when $x$ = 0.

+ The difference in salary between Accounts and Store managers in London is £3,460. The salary is lower for Store Managers. 
]


.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
salary3 %>%
  group_by(location, department) %>%
  summarise(
    Salary = mean(salary)
  ) %>%
  kable(.)
```
]

---
#  Example: Categorical*categorical
```{r, echo=FALSE}
res <- summary(int4)
round(res$coefficients,2)
```

.pull-left[
+  $\beta_3$ = Difference between levels of $x$ across levels of $z$

+ The difference between salary for Accounts and Store managers between London and Birmingham, differs by £8,420. The difference is greater in Birmingham than in London.
]


.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
salary3 %>%
  group_by(location, department) %>%
  summarise(
    Salary = mean(salary)
  ) %>%
  kable(.)
```
]


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 4
**Is my model any good?**

---
class: inverse, center, middle

<h2>Part 1: Key model assumptions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Basic model diagnostics </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: What we have not covered? </h2>

---
# Linear model assumptions 
+ So far, we have discussed evaluating linear models with respect to:
	+ Overall model fit ( $F$ -ratio, $R^2$)
	+ Individual predictors

+ However, the linear model is also built on a set of assumptions.

+ If these assumptions are violated, the model will not be very accurate.

+ Thus, we also need to assess the extent to which these assumptions are met.


---
# Some data for today

.pull-left[
+ Let's look again at our data predicting salary from years or service and performance ratings (no interaction).

$$y_i = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon_i$$

+ $y$ = Salary (unit = thousands of pounds ).

+ $x_1$ = Years of service.

+ $x_2$ = Average performance ratings.
 
]

.pull-right[

```{r, echo=FALSE}
salary2 %>%
  slice(1:10) %>%
  kable(.) %>%
  kable_styling(full_width = F)
```


]

---
# Our model
```{r}
m1 <- lm(salary ~ perf + serv, data = salary2)
```

+ We will run all our assumptions based on the object `m1`

---
# Visualizations vs tests
+ There exist a variety of ways to assess assumptions, which broadly split into statistical tests and visualizations.

+ We will focus on visualization:
	+ Easier to see the nature and magnitude of the assumption violation
	+ There is also a very useful function for producing them all.

+ Statistical tests often suggest assumptions are violated when problem is small.
  + This is to do with the statistical power of the tests.
  + Give no information on what the actual problem is.
  + A summary table of tests will be given at the end of the lecture.


---
# Visualizations made easy
+ For a majority of assumption and diagnostic plots, we will make use of the `plot()` function.
  + If we give `plot()` a linear model object (e.g. `m1` or `m2`), we can automatically generate assumption plots.

+ We will also make use of some individual functions for specific visualizations.

+ Alternatively, we can also use `check_model()` from the `performance` package.
  + This provides `ggplot` figures as well as some notes to aid interpretation.
  + Caution that these plots are **not in a format to use directly in reports**

---
#  Linearity 
+ **Assumption**: The relationship between $y$ and $x$ is linear.
  + Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation


+ **Investigated with**:
  + Scatterplots with loess lines (single variables)
  + Component-residual plots (when we have multiple predictors)


---
# Linear vs non-linear

.pull-left[

```{r, echo=FALSE, message=FALSE}
df2 <- tibble(
  x = rnorm(1000, 10, 2),
  y = 5 + .8*x + rnorm(1000, 0,.5),
  y2 = 5 + .6*(x^3) + rnorm(1000, 0,10)
)

df2 %>%
  ggplot(., aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se=F) +
  labs(x= "X", y="Y", title = "Linear Relationship")

```

]

.pull-right[

```{r, message=FALSE, echo=FALSE}
df2 %>%
  ggplot(., aes(x=x, y=y2)) +
  geom_point() +
  geom_smooth(method = "lm", se=F) +
  labs(x= "X", y="Y", title = "Non-linear Relationship")
```

]

---
#  What is a loess line?

+ Method for helping visualize the shape of relationships:

+ Stands for...
  + **LO**cally
  + **E**stimated
  + **S**catterplot
  + **S**moothing

+ Essentially produces a line with follows the data.

+ Useful for single predictors.

---
# Visualization

.pull-left[
```{r, warning=FALSE}
lin_m1 <- salary2 %>%
  ggplot(., aes(x=serv, y=perf)) +
  geom_point()+
  geom_smooth(method = "lm", se=F) + # <<
  geom_smooth(method = "loess", se=F, #<<
              col = "red") +
  labs(x= "Years of Service", y="Performance", 
       title = "Scatterplot with linear (blue) 
       and loess (red) lines")
```
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
lin_m1
```

]

---
#  Non-linearity

+ With multiple predictors, we need to know whether the relations are linear between each predictor and outcome, controlling for the other predictors

+ This can be done using **component-residual plots**
  + Also known as partial-residual plots
		
+ Component-residual plots have the $x$ values on the X-axis and partial residuals on the Y-axis

+ *Partial residuals* for each X variable are:

$$\epsilon_i + B_jX_{ij}$$

+ Where :
	+ $\epsilon_i$ is the residual from the linear model including all the predictors
	+ $B_jX_{ij}$ is the partial (linear) relation between $x_j$ and $y$

---
#  `crPlots()` 

+ Component-residual plots can be obtained using the `crPlots()` function from `car` package

```{r, eval = F, warning=FALSE}
m1 <- lm(salary ~ perf + serv, data = salary2)
crPlots(m1)
```

+ The plots for continuous predictors show a linear (dashed) and loess (solid) line

+ The loess line should follow the linear line closely, with deviations suggesting non-linearity

---
#  `crPlots()`
```{r, echo= F, warning=FALSE, fig.height=3.8}
m1 <- lm(salary ~ perf + serv, data = salary2)
crPlots(m1)
```


???
+ Here the relations look pretty good.

+ Deviations of the line are minor

---
# Normally distributed errors 
+ **Assumption**: The errors ( $\epsilon_i$ ) are normally distributed around each predicted value.

+ **Investigated with**:
  + QQ-plots
  +	Histograms

	
---
# Visualizations 
+ **Histograms**: Plot the frequency distribution of the residuals.

```{r, eval=FALSE}
hist(m1$residuals)
```

--

+ **Q-Q Plots**: Quantile comparison plots.
	+ Plot the standardized residuals from the model against their theoretically expected values.
	+ If the residuals are normally distributed, the points should fall neatly on the diagonal of the plot.
	+ Non-normally distributed residuals cause deviations of points from the diagonal.
		+ The specific shape of these deviations are characteristic of the distribution of the residuals.

```{r, eval=FALSE}
plot(m1, which = 2) #<<
```


---
# Visualizations

.pull-left[

```{r, echo=FALSE}
hist(m1$residuals)
```

]


.pull-right[

```{r, echo=FALSE}
plot(m1, which = 2) #<<
```

]


---
#  Equal variance (Homoscedasticity) 

+ **Assumption**: The equal variances assumption is constant across values of the predictors $x_1$, ... $x_k$, and across values of the fitted values $\hat{y}$
	+ Heteroscedasticity refers to when this assumption is violated (non-constant variance)

+ **Investigated with**:
  + Plot residual values against the predicted values ( $\hat{y}$ ).

---
#  Residual-vs-predicted values plot 
+ In R, we can plot the residuals vs predicted values using `residualPlot()` function in the `car` package.
  + Categorical predictors should show a similar spread of residual values across their levels
  + The plots for continuous predictors should look like a random array of dots
	  + The solid line should follow the dashed line closely

```{r, eval=FALSE}
residualPlot(m1)
```

+ We can also get this plot using:

```{r, eval=FALSE}
plot(m1, which = 1)
```

---
#  Residual-vs-predicted values plot 

.pull-left[
```{r, echo=FALSE}
residualPlot(m1)
```

]

.pull-right[
```{r, echo=FALSE}
plot(m1, which = 1)
```
]



---
#  Independence of errors 
+ **Assumption**: The errors are not correlated with one another

+ Difficult to test unless we know the potential source of correlation between cases.

+ Essentially, if a design is between person, we will assume the errors to be independent.


---
#  Multi-collinearity 
+ This is **not an assumption of linear model**, but it is something we need to consider. 
  + It sits between assumptions and case diagnostics.
  
+ Multi-collinearity refers to the correlation between predictors
  + We saw this in the formula for the standard error of model slopes for an `lm` with multiple predictors.

+ When there are large correlations between predictors, the standard errors are increased
	+ Therefore, we don't want our predictors to be too correlated

---
#  Variance Inflation Factor 
+ The **Variance Inflation Factor** or VIF quantifies the extent to which standard errors are increased by predictor inter-correlations

+ It can be obtained in R using the `vif()` function:

```{r}
vif(m1)
```

+ The function gives a VIF value for each predictor

+ Ideally, we want values to be close to 1

+ VIFs> 10 indicate a problem

---
#  What to do about multi-collinearity 

+ In practice, multi-collinearity is not often a major problem

+ When issues arise, consider:
	+ Combining highly correlated predictors into a single composite
		  + E.g. create a sum or average of the two predictors
	+ Dropping an IV that is obviously statistically and conceptually redundant with another from the model


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Key model assumptions </h2>
<h2>Part 2: Basic model diagnostics  </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: What we have not covered? </h2>

---
#  Three important features

+ Model outliers
	+ Cases for which there is a large discrepancy between their predicted value ( $\hat{y_i}$ ) and their observed value ( $y_i$ )

--

+ High leverage cases
	+ Cases with an unusual value of the predictor ( $x_i$ )

--

+ High influence cases
	+ Cases who are having a large impact on the estimation of model

---
#  Influence

+ High leverage cases, when they are also linear model outliers, will have high **influence**

+ Cases with high influence, have a strong effect on the coefficients

+ If we deleted such a case, the linear model coefficients would change substantially


---
# Influence

+ If a handful of influential cases are responsible for the linear model results, the conclusions might not generalise very well

+ Multiple ways to consider influence.
  + Here we will discuss Cook's distance.
  
+ Cook's Distance of a data point $i$ (can be written many ways):


$$D_i = \frac{(\text{StandardizedResidual}_i)^2}{k+1} \times \frac{h_i}{1-h_i}$$

---
#  Cooks Distance 
$$\frac{(\text{StandardizedResidual}_i)^2}{k+1} = \text{Outlyingness}$$


$$\frac{h_i}{1-h_i} = \text{Leverage}$$

+ So $D_i = \text{Outlyingness} \times \text{Leverage}$


+ Cook's distance refers to **the average distance the $\hat{y}$ values will move if a given case is removed.**
  + If removing one case changes the predicted values a lot (moves the regression line), then that case is influencing our results.

---
#  Cooks Distance 

+ Many different suggestions for cut-off's:
  + $D_i > 1$ 
  + $D_i > \frac{4}{n-k-1}$
  + Or size relative all values in data set

---
#  Cook's distance in R

.pull-left[
```{r}
salary2 %>%
  mutate(
    cook = cooks.distance(m1) #<<
  ) %>%
  filter(., cook > 4/(100-3-1)) %>% #<<
  kable(.)  %>%
  kable_styling(., full_width = F)
```

]

.pull-right[
```{r, echo=F}
plot(m1, which = 4)
```

]


---
# Influence of coefficients
+ Cook's distance is a single value summarizing the total influence of a case

+ In the context of a lm with 2+ predictors, we may want to look in a little more detail.

+ **DFFit**: The difference between the predicted outcome value for a case with versus without a case included

+ **DFbeta**: The difference between the value for a coefficient with and without a case included

+ **DFbetas**: A standardised version of DFbeta
  + Obtained by dividing by an estimate of the standard error of the regression coefficient with the case removed

---
#  COVRATIO 
+ Influence on standard errors can be measured using the **COVRATIO** statistic
	+ COVRATIO value <1 show that precision is decreased (SE increased)  by a case
	+ COVRATIO value >1 show that precision is increased (SE decreased) by a case

+ Cases with COVRATIOS $> 1+[3(k +1)/n]$ or $< 1-[3( k +1)/ n ]$ can be considered to have a strong influence on the standard errors

---
#  COVRATIO in R 
+ COVRATIO values can be extracted using the `covratio()` function:
+ We can extract these measures using the `influence.measures()` function


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Key model assumptions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Basic model diagnostics </h2>
<h2>Part 3: What we have not covered? </h2>

---
# In short, quite a lot!

+ Coding schemes for categorical data
+ Coding specific comparisons
+ Interactions with categorical variables and 2+ levels
+ Detailed probing of interactions
+ Statistical tests for assumptions
+ Assumption corrections for assumption violations
+ Bootstrapping
+ Extended model diagnostics
+ Non-continuous outcome variables
+ ....

---
# Material links

+ All our lecture materials can be found [here](https://uoepsy.github.io/)

---
class: center, middle
# Thanks all!