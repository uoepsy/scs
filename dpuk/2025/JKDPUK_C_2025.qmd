---
title: "<b> Introduction to the Linear Model </b>"
subtitle: "DPUK Spring Academy<br><br> "
author: "Josiah King, Umberto Noe, (and credits to Tom Booth)"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "April 2025"
format:
  revealjs:
    theme: [default, _theme/mystyle.scss]
    css: img/css/wizard.css
    width: 1294
    height: 1000
    transition: fade
    slide-number: true
    preview-links: auto
    include-after-body: _theme/clean_title_page.html
    callout-appearance: simple
    code-line-numbers: false
    code-overflow: wrap
    code-annotations: hover
    code-copy: hover
    classoption: fleqn
    chalkboard: true
revealjs-plugins:
  - attribution
filters: 
  - "_theme/inverse_part.lua"
  - line-highlight
highlight-style: breeze
title-slide-attributes:
  data-background-color: "#95A5A6"
  data-background-image: img/logo.svg
  data-background-size: 25%
  data-background-position: 2% 2%
execute:
  warning: false
  echo: false
  message: false
  fig-format: svg
include-in-header: _theme/highlighter.js
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')
library(kableExtra)
library(car)

test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
res <- lm(score ~ hours, data = test)
```


# Overview

- Day 2: What is a linear model?  
- Day 3: But I have more variables, what now?  
- Day 4: Interactions  
- Day 5: Is my model any good?  


# Day 3

**But I have more variables, now what?**

- Part 1 - Quick Recap
- Part 2 - Explaining variance
- Part 3 - More predictors
- Part 4 - More variance explained? (model comparison)
- Part 5 - Coefficients

# Quick recap


## Models

::::{.columns}
:::{.column width="50%"}
__deterministic__  

given the same input, deterministic functions return *exactly* the same output

- area of sphere = $4\pi r^2$  

- height of fall = $1/2 g t^2$

    <!-- - $g =$ gravitational constant, $9.8m/s^2$ -->
    <!-- - $t =$ time (in seconds) of fall -->

:::

:::{.column width="50%" .fragment}
__statistical__  

$$ 
\color{red}{\textrm{outcome}} \color{black}{=} \color{blue}{(\textrm{model})} + \color{black}{\textrm{error}}
$$


- handspan = height + randomness  

- cognitive test score = age + premorbid IQ + ... + randomness
:::
::::



## The Linear Model

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error} \\
\qquad \\
\qquad \\
\color{red}{y_i} & = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \varepsilon_i \\
\qquad \\
& \text{where } \varepsilon_i \sim N(0, \sigma) \text{ independently} \\
\end{align}
$$


## The Linear Model

::::{.columns}

:::{.column width="50%"}
Our proposed model of the world:

$\color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \varepsilon_i$  

:::

:::{.column width="50%"}

```{r bb}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1,colour="blue") +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),colour="blue", lty="dotted") +
  geom_segment(aes(x=0,xend=1,y=f(0),yend=f(0)),colour="blue",linetype="dotted") +
  geom_segment(aes(x=1,y=f(0),xend=1,yend=f(1)),colour="blue",linetype="dotted") +
  geom_point(x=0,y=f(0),col="blue",size=3)+
  annotate("text",x=0.5,y=3,hjust=0,label=expression(paste(b[0], " (intercept)")),
           size=8,parse=TRUE,colour="blue") +
  geom_segment(x=.5,xend=0.01,y=3,yend=4.9,arrow = arrow(length=unit(0.30,"cm")),col="blue")+
  geom_segment(x=1.02,xend=1.02,y=5,yend=7,col="blue")+
  geom_segment(x=c(1,1),xend=c(1.02,1.02),y=c(5,7),yend=c(5,7),col="blue")+
  annotate("text",x=1,y=6,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue") +
    ggtitle(expression(paste(b[0]," = 5, ",b[1]," = 2")))+
  scale_y_continuous(breaks=0:13)+
  scale_x_continuous(limits = c(-0.3, 4), breaks=0:4)
p1 +
  ggtitle("")+
  geom_segment(x=2.02,xend=2.02,y=7,yend=9,col="blue",alpha=.4)+
  geom_segment(x=c(2,2),xend=c(2.02,2.02),y=c(7,9),yend=c(7,9),col="blue",alpha=.4)+
  annotate("text",x=2,y=8,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue",alpha=.4) +
  
  geom_segment(x=3.02,xend=3.02,y=9,yend=11,col="blue",alpha=.1)+
  geom_segment(x=c(3,3),xend=c(3.02,3.02),y=c(9,11),yend=c(9,11),col="blue",alpha=.1)+
  annotate("text",x=3,y=10,hjust=-.1,label=expression(paste(b[1], " (slope)")),
           size=8,parse=TRUE,colour="blue",alpha=.1) +
  
  scale_y_continuous("y",labels=NULL)+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))+
  ggtitle(expression(paste("y = ",b[0] %.% 1 + b[1] %.% x)))
  
```

:::
::::


## The Linear Model


::::{.columns}
:::{.column width="50%"}
Our model $\hat{\textrm{f}}\textrm{itted}$ to some data:  

$\hat{y}_i = \color{blue}{\hat b_0 \cdot{} 1 + \hat b_1 \cdot{} x_i}$  
  
For the $i^{th}$ observation:  

  - $\color{red}{y_i}$ is the value we observe for $x_i$   
  - $\hat{y}_i$ is the value the model _predicts_ for $x_i$   
  - $\color{red}{y_i} = \hat{y}_i + \hat\varepsilon_i$  

:::

:::{.column width="50%"}

```{r}
#| label: fitplot
xX <-1.2
yY <- 9.9
p1 + 
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",1.1,8.6,hjust=1,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=8)+
  geom_point(aes(x=xX,y=f(xX)),col="black",size=3,shape=1)+
  ggtitle(expression(paste("y = ",b[0] %.% 1 + b[1] %.% x)))+
  scale_y_continuous("y")+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))+
  theme(axis.text.y=element_text(colour="white"))+
  NULL
```
:::
::::



## An example

::::{.columns}

:::{.column width="50%"}
Our model $\hat{\textrm{f}}\textrm{itted}$ to some data:  

$\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \hat\varepsilon_i$  
  
:::{.fragment}
For the observation $x_i = 1.2, \; y_i = 9.9$:  

$$
\begin{align}
\color{red}{9.9} & = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \hat\varepsilon_i \\
& = 7.4 + \hat\varepsilon_i \\
& = 7.4 + 2.5 \\
\end{align}
$$
:::
:::
:::{.column width="50%"}
```{r}
#| label: errplot
xX <-1.2
yY <- 9.9
p1 + #ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",1.1,8.6,hjust=1,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=8)+
  ggtitle(expression(paste("y = ", 5 %.% 1 + 2 %.% x)))
```

:::
::::

::: {.notes}
minimizing residuals
:::

## Categorical Predictors

::::{.columns}
:::{.column width="50%"}

```{r}
#| label: catpred
set.seed(993)
tibble(
  x = sample(c("Category0","Category1"), size = 30, replace = T),
  y = 5 + 2*(x == "Category1") + rnorm(30,0,1) %>% round(2)
) %>% select(y,x) -> df
df %>% sample_n(6) %>% rbind(., c("...","...")) %>% kableExtra::kbl()
```

:::
:::{.column width="50%"}

```{r}
#| label: catpredplot
ggplot(df,aes(x=as.numeric(x=="Category1"),y=y))+
  geom_point(size=3,alpha=.5)+
  stat_summary(geom="point",shape=4,size=6)+
  stat_summary(geom="path", aes(group=1))+
  scale_x_continuous(name="isCategory1",breaks=c(0,1),
                     labels=c("0\n(FALSE)","1\n(TRUE)"))+
  geom_segment(x=0,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category0"]),
               lty="dashed",col="blue")+
  geom_segment(x=1,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category1"]),
               lty="dashed",col="blue")+
  annotate("text",x=1.05,y=6,
           label=expression(paste(b[1], " (slope)")),
           angle=90,
           size=8,parse=TRUE,colour="blue")+
  labs(title="y ~ x    (x is categorical)")
```
:::
::::


## Null Hypothesis Testing

```{r}
set.seed(2394)
samplemeans <- replicate(2000, mean(rnorm(n=100, mean=0, sd=5)))
g <- ggplot(data=tibble(samplemeans),aes(x=samplemeans))+
  #geom_histogram(alpha=.3)+
  stat_function(geom="line",fun=~dnorm(.x, mean=0,sd=sd(samplemeans))*270,lwd=1)

ld <- layer_data(g) %>% filter(x <= sd(samplemeans) & x >= (-sd(samplemeans)))
ld2 <- layer_data(g) %>% filter(x <= 2*sd(samplemeans) & x >= (-2*sd(samplemeans)))
ld3 <- layer_data(g) %>% filter(x >= 1.2)
ld4 <- layer_data(g) %>% filter(x <= -1.2)

g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_area(data=ld3,aes(x=x,y=y),fill="tomato1",alpha=.3) +
  geom_area(data=ld4,aes(x=x,y=y),fill="tomato1",alpha=.3) +
  geom_segment(aes(x=0,xend=0,y=0,yend=dnorm(0,0,sd=sd(samplemeans))*280), lty="dashed")+
  geom_segment(aes(x=1.2,xend=1.2,y=0,yend=180), lty="dashed", col="tomato1")+ 
  # geom_vline(aes(xintercept=1.2),lty="dashed",col="tomato1")+
  labs(x = "statistic")+
  scale_y_continuous(NULL, breaks=NULL)+
  scale_x_continuous(NULL, breaks=NULL, limits=c(-1.5,2))+
  annotate("text",x=-.5, y=250, label="population parameter\nunder null hypothesis", col="grey30")+
  annotate("text",x=1, y=210, label="statistics we would expect from\nsamples of size n if the\nnull hypothesis is true", col="grey30")+
  annotate("text",x=1.65, y=100, label="statistic we observed\nin our sample", col="tomato1")+
  annotate("text",x=1.5, y=40, label="p-value", col="tomato1")+
  geom_curve(aes(x=-.5, xend=0, y=240, yend=220), col="grey30", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1, xend=0.5, y=190, yend=150), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1.6, xend=1.2, y=90, yend=40), col="tomato1", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc"))) +
  geom_curve(aes(x=1.5, xend=1.3, y=35, yend=2.5), col="tomato1", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))
```

## Null Hypothesis Testing

```{r}

ld <- layer_data(g) %>% filter(x <= sd(samplemeans) & x >= (-sd(samplemeans)))
ld2 <- layer_data(g) %>% filter(x <= 2*sd(samplemeans) & x >= (-2*sd(samplemeans)))
ld3 <- layer_data(g) %>% filter(x >= qnorm(.975,0,sd(samplemeans)))
ld4 <- layer_data(g) %>% filter(x <= qnorm(.025,0,sd(samplemeans)))


g + geom_area(data=ld,aes(x=x,y=y),fill="grey30",alpha=.3) + 
  geom_area(data=ld2,aes(x=x,y=y),fill="grey30",alpha=.1) +
  geom_area(data=ld3,aes(x=x,y=y),fill="blue",alpha=.3) +
  geom_area(data=ld4,aes(x=x,y=y),fill="blue",alpha=.3) +
  geom_segment(aes(x=0,xend=0,y=0,yend=dnorm(0,0,sd=sd(samplemeans))*280), lty="dashed")+
  scale_y_continuous(NULL, breaks=NULL)+
  scale_x_continuous(NULL, breaks=NULL, limits=c(-1.5,2))+
  annotate("text",x=-.5, y=250, label="population parameter\nunder null hypothesis", col="grey30")+
  annotate("text",x=1, y=210, label="statistics we would expect from\nsamples of size n if the\nnull hypothesis is true", col="grey30")+
  annotate("text",x=1.5, y=40, label="critical regions", col="blue")+
  geom_curve(aes(x=1.5, xend=1.2, y=35, yend=5), col="blue", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=-.5, xend=0, y=240, yend=220), col="grey30", size=0.5, curvature = 0, arrow = arrow(length = unit(0.03, "npc")))+
  geom_curve(aes(x=1, xend=0.5, y=190, yend=150), col="grey30", size=0.5, curvature = -0.2, arrow = arrow(length = unit(0.03, "npc")))
```


## test of individual parameters

![](img_sandbox/sum1.png)

<!-- :::aside -->
<!-- the model "parameters" are the things being estimated - the intercept and all the slopes (and the residual standard deviation, but we're less interested in that).  -->
<!-- ::: -->

## test of individual parameters (2)

![](img_sandbox/sum2.png)

## test of individual parameters (3)

![](img_sandbox/sum3.png)

## test of individual parameters (4)

![](img_sandbox/sum4.png)


# The linear model as variance explained

## Sums of Squares

::::{.columns}
:::{.column width="50%"}

Rather than focussing on slope coefficients, we can also think of our model in terms of sums of squares (SS).  

- $SS_{total} = \sum^{n}_{i=1}(y_i - \bar y)^2$  

- $SS_{model} = \sum^{n}_{i=1}(\hat y_i - \bar y)^2$  
  
- $SS_{residual} = \sum^{n}_{i=1}(y_i - \hat y_i)^2$  

:::
:::{.column width="50%" .fragment}

```{r}
set.seed(993)
df <- 
  tibble(
    x1 = rnorm(100)+3,
    y = x1*2 + rnorm(100)
  )

# SST, SSM and SSR
plt_sst = 
  ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=mean(df$y)), lty="dashed",col="red")+
  geom_text(x=1,y=8,label="bar(y)",parse=T, size=6)+
  geom_curve(x=1.1,xend=2,y=8,yend=mean(df$y),curvature=-.2)+
  labs(title="SS Total")

plt_ssr = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=fitted(lm(y~x1,df))), lty="dashed",col="red")+
  labs(title="SS Residual")

plt_ssm = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,yend=mean(df$y),y=fitted(lm(y~x1,df))), lty="dashed",col="blue")+
  labs(title="SS Model")

plt_sst / (plt_ssm + plt_ssr) & scale_x_continuous("x",breaks=NULL) & scale_y_continuous("y",breaks=NULL)

```

:::
::::

## $R^2$

::::{.columns}
:::{.column width="50%"}
$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$
<br><br>
```{r echo=FALSE}
x = rnorm(100)
z = rnorm(100)
y = 1.4*x + .3*z + rnorm(100)
```
```{r}
#| echo: true
#| eval: false
mdl <- lm(y ~ 1 + x, data)
summary(mdl)
```
```
...

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)   ...       ...      ...    ...
x             ...       ...      ...    ...
...

...
Multiple R-squared:  0.66,	Adjusted R-squared:  0.657
...
```
:::
:::{.column width="50%"}

:::
::::


## F-ratio

::::{.columns}
:::{.column width="60%"}
$$
\begin{align}
& F_{df_{model},df_{residual}} = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{Residual}} \\
& \quad \\
& \text{Where:} \\
& df_{model} = k \\
& df_{residual} = n-k-1 \\
& n = \text{sample size} \\
& k  = \text{number of explanatory variables} \\
\end{align}
$$
```{r}
#| echo: true
#| eval: false
mdl <- lm(y ~ 1 + x, data)
summary(mdl)
```
```
...

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)   ...       ...      ...    ...
x             ...       ...      ...    ...
...

...
Multiple R-squared:  0.66,	Adjusted R-squared:  0.657
F-statistic:  190 on 1 and 98 DF,  p-value: <2e-16
```
:::

:::{.column width="40%" .incremental}

+ $F$-ratio tests the null hypothesis that all the regression slopes in a model are zero

+ a ratio of the explained to unexplained variance

+ Mean squares are sums of squares calculations divided by the associated degrees of freedom.
+ The degrees of freedom are defined by the number of "independent" values associated with the different calculations.
+ Bigger $F$-ratios indicate better models.
  + It means the model variance is big compared to the residual variance.

:::
::::

## F-test

+ testing the F-ratio

+ The null hypothesis for the model says that the best guess of any individuals $y$ value is the mean of $y$ plus error.  
	+ Or, that the $x$ variables carry no information collectively about $y$.

+ $F$-ratio will be close to 1 when the null hypothesis is true
  + If there is equivalent residual to model variation, $F$=1
	+ If there is more model than residual $F$ > 1

+ $F$-ratio is then evaluated against an $F$-distribution with $df_{Model}$ and $df_{Residual}$ and a pre-defined $\alpha$


## how does this all help?  

::: {.incremental}

+ The aim of a linear model is to explain variance in an outcome.

+ In simple linear models, we have a single predictor, but the model can accommodate (in principle) any number of predictors. 

+ However, when we include multiple predictors, those predictors are likely to correlate

+ Thus, a linear model with multiple predictors finds the optimal prediction of the outcome from several predictors, **taking into account their redundancy with one another**

:::

## how does this all help?  


+ how does $R^2$ change with the addition of our predictor of interest *beyond the variance explained by other variables*  

+ does the inclusion of our predictor of interest significantly improve model fit *beyond the variance explained by other variables*  



# More predictors

##  Uses of multiple regression  

::: {.incremental}

+ **For prediction:** multiple predictors may lead to improved prediction. 

+ **For theory testing:** often our theories suggest that multiple variables together contribute to variation in an outcome

+ **For covariate control:** we might want to assess the effect of a specific predictor, controlling for the influence of others.
	+ E.g., effects of personality on health after removing the effects of age and sex

:::


## ![](img_sandbox/tangent.png){width=35px} Third variables 

::::{.columns}
:::{.column width="50%"}

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide1.PNG)
:::
::::


::: {.notes}
TANGENT.  
why useful?  

visual is for learning only - intution building.
:::

## ![](img_sandbox/tangent.png){width=35px} Third variables 

::::{.columns}
:::{.column width="50%"}

- X and Y are 'orthogonal' (perfectly uncorrelated)  

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide2.PNG)

:::
::::


## ![](img_sandbox/tangent.png){width=35px} Third variables 

::::{.columns}
:::{.column width="50%"}

- X and Y are correlated.  
    - **a** = portion of Y's variance shared with X
    - **e** = portion of Y's variance unrelated to X

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide3.PNG)


:::
::::

:::{.notes}
- SIGNAL / NOISE  
- a/e


:::

## ![](img_sandbox/tangent.png){width=35px} Third variables 


::::{.columns}
:::{.column width="50%"}

- X and Y are correlated.  
    - **a** = portion of Y's variance shared with X
    - **e** = portion of Y's variance unrelated to X
- Z is also related to Y (**c**)
- Z is orthogonal to X (no overlap)

:::{.fragment}

- relation between X and Y is unaffected (**a**)
- unexplained variance in Y (**e**) is reduced, so **a**:**e** ratio is greater.

Design is so important! If possible, we could design it so that X and Z are orthogonal (in the long run) by e.g., randomisation.  
:::
:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide4.PNG)
:::
::::

:::{.notes}
- third variable  
- might be completely unrelated to X
- in which case, what does it do? 
    - makes noise smaller!
    - easier to detect signal!

- randomised experiments
- bp ~ drug | age 
:::


## ![](img_sandbox/tangent.png){width=35px} Third variables 

::::{.columns}
:::{.column width="50%"}

- X and Y are correlated.  

<br>

- Z is also related to Y (**c + b**)  
- Z *is* related to X (**b + d**)

:::{.fragment}
Association between X and Y is changed if we adjust for Z (**a** is smaller than previous slide), because there is a bit (**b**) that could be attributed to Z instead.  

- multiple regression coefficients for X and Z are like areas **a** and **c** (scaled to be in terms of 'per unit change in the predictor')
- total variance explained by _both_ X and Z is **a+b+c**

:::
:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide5.PNG)
:::
::::

::: {.notes}
- observational studies are much more difficult
- lots of possible variables that overlap X
  - e.g. older people take drug more
  - effect might just be age
  
- we want: difference BP between drug and no drug, OF SAME AGE
- coefficients are A and C - unique bits of X and Z

- NOTE - aim is not to control for EVERYTHING.
    - some Zs we might want to remove this bias, some we might not.
    - BP ~ drug | cholesterol

:::


## I have control issues.. 
### ..and so should you

::::{.columns}
:::{.column width="50%"}
what do we mean by "control"?  

- often quite a vague/nebulous idea

relationship between x and y ... 

- controlling for z
- accounting for z
- conditional upon z
- holding constant z
- **adjusting** for z

:::

:::{.column width="50%"}

![](img_sandbox/simp.jpg)

:::
::::

## I have control issues.. 
### ..and so should you

- linear regression models provide "linear adjustment" of Z 

    - sort of like stratification across groups, but Z can now be continuous

    - assumes effect of X on Y is constant across Z
        - (but doesn't have to, as we'll see tomorrow)

## I have control issues.. 
### ..and so should you

In order to estimate "the effect" of X on Y, deciding on what to control for **MUST** rely on a theoretical model of causes - i.e. a theory about the underlying data generating process  


## thus far - lm(y~x)

![](img_sandbox/caus/Slide1.PNG)

## what do i do?  

![](img_sandbox/caus/Slide13.PNG)

## Z is a confounder

![](img_sandbox/caus/Slide2.PNG)

## Z is a mediator

![](img_sandbox/caus/Slide3.PNG)

## Z is a collider

![](img_sandbox/caus/Slide4.PNG)

## example

![](img_sandbox/caus/Slide5.PNG)

## example

![](img_sandbox/caus/Slide6.PNG)

## example

![](img_sandbox/caus/Slide7.PNG)


## example

![](img_sandbox/caus/Slide8.PNG)

## example 2

![](img_sandbox/caus/Slide9.PNG)

- to play around with this, see  [https://colliderbias.herokuapp.com/](https://colliderbias.herokuapp.com/){target="_blank"}

## example 2

![](img_sandbox/caus/Slide10.PNG)

- to play around with this, see  [https://colliderbias.herokuapp.com/](https://colliderbias.herokuapp.com/){target="_blank"}

## example 2

![](img_sandbox/caus/Slide11.PNG) 

- to play around with this, see  [https://colliderbias.herokuapp.com/](https://colliderbias.herokuapp.com/){target="_blank"}

## general heuristics (NOT rules)

- control for a confounder  
- don't control for a mediator  
- don't control for a collider 

- draw your theoretical model **before** you get your data, so that you know what variables you (ideally) need.  

:::{.aside}

For more on this, see:  

- Wysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221095823.
- Rohrer, J. M. (2024). Causal inference for psychologists who think that causal inference is not for them. Social and Personality Psychology Compass, 18(3), e12948.
- Cinelli, C., Forney, A., & Pearl, J. (2024). A crash course in good and bad controls. Sociological Methods & Research, 53(3), 1071-1104.

:::


# More variance explained?


## More than one predictor?   

::::{.columns}
:::{.column width="50%"}

$\color{red}{y} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 + \, ... \, + b_k \cdot x_k} + \varepsilon$


```{r}
#| eval: false
#| echo: true
mymodel <- lm(y ~ 1 + x1 + ... + xk, data = mydata)
```


:::

:::{.column width="50%" .fragment}
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```
:::
::::


```{r}
#| eval: false
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)

```

::: {.notes}
minimizing residuals
:::

## Model comparison  

- does `x2` explain more variability in `y` than we would expect by a random variable, **over and above** variability explained by `x1`?  

    - once we know x1, does knowing about x2 tell us anything new about y? 

```{r}
#| echo: false
set.seed(124)
mydata = tibble(
  x1 = rnorm(100,0,1),
  x2 = rnorm(100, .2*x1, 2),
  y = rnorm(100, .5*x2+.4*x1, 1.7)
) |> mutate(y=scale(y)*11.9+20)
```

```{r}
#| echo: true

m1 <- lm(y ~ x1, data = mydata)
m2 <- lm(y ~ x1 + x2, data = mydata)

anova(m1, m2)
```



## tests of multiple parameters

```{r echo=F}
set.seed(2345)
tibble(
  x=rnorm(100),
  z=rnorm(100),
  z2=rnorm(100),
  species=rep(c("cat","dog","parrot","horse"),e=25),
  y=10+x-2*z+2*z2+(species=="cat")*4 + rnorm(100)
) -> df
```

Model comparisons:


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
m1 <- lm(y ~ 1 + x, data = df)
```
![](img_sandbox/ssvenn/Slide8.PNG)
:::

:::{.column width="50%"}
```{r}
#| echo: true
m2 <- lm(y ~ 1 + z + z2 + x, data = df)
```
![](img_sandbox/ssvenn/Slide7.PNG)
:::
::::


## tests of multiple parameters (2)

::::{.columns}
:::{.column width="50%"}
isolate the improvement in model fit due to inclusion of additional parameters

```{r}
#| echo: true
m1 <- lm(y ~ 1 + x, data = df)
m2 <- lm(y ~ 1 + z + z2 + x, data = df)
anova(m1, m2)
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7a.PNG)
:::
::::

:::aside
For linear models, model comparisons are typically testing reduction in residual sums of squares (via an $F$ test). For models with other error distributions, we often test differences in log-likelihood (via $\chi^2$ test).  
:::


## tests of multiple parameters (3)

::::{.columns}
:::{.column width="50%"}
Test everything in the model all at once by comparing it to a 'null model' with no predictors:  

```{r}
#| echo: true
m0 <- lm(y ~ 1, data = df)
m2 <- lm(y ~ 1 + z2 + z + x, data = df)

anova(m0, m2)
```

:::

:::{.column width="50%"}
![](img_sandbox/ssvenn/Slide7b.PNG)
:::
::::


# Coefficients

## multiple regression model

::::{.columns}
:::{.column width="50%"}

$\color{red}{y} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_1 + \, ... \, + b_k \cdot x_k} + \varepsilon$


```{r}
#| eval: false
#| echo: true
mymodel <- lm(y ~ 1 + x1 + ... + xk, data = mydata)
```


:::

:::{.column width="50%"}
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```
:::
::::


```{r}
#| eval: false
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)

```

::: {.notes}
minimizing residuals
:::



## multiple regression coefficients

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
mod1 <- lm(y ~ x1 + x2, data = mydata)
summary(mod1)
```
:::

:::{.column width="50%"}

<br><br><br>

```{r}
broom::tidy(mod1) |>
  transmute(
    term,
    est=round(estimate,2),
    interpretation=c(
      "estimated y when all predictors are zero/reference level",
      "estimated change in y when x1 increases by 1, and all other predictors are held constant",
      "estimated change in y when x2 increases by 1, and all other predictors are held constant"
    )
  ) |> gt::gt()
```

:::
::::

::: {.notes}
- coefficients now interpreted with "holding constant"

:::


## change in Y for a 1 unit change in X

```{r}
set.seed(887)
df = tibble(
  age = c(18,19,20,33,67,24,37,55,41,31),
  exercise = round(runif(10,0,7)),
  therapy = c(0,0,1,0,1,1,0,0,1,1),
  stress = age*.3 + -.7*exercise + -3*therapy  -.1*age*therapy+ rnorm(10)
)
```


```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age, data = df)
.pp(summary(mod), l = list(9:18))
```

## unit level predictions

```{r}
#| echo: true
mod = lm(stress ~ age, data = df)
coef(mod)
```

::::{.columns}
:::{.column width="30%"}
__Data__  
```{r}
#| echo: false
df
```

:::
:::{.column width="20%"}
__Predictions__  
```{r}
#| echo: false
library(marginaleffects)
predictions(mod) |> as_tibble() |> transmute(prediction=estimate)
```

:::
:::{.column width="50%"}

:::
::::


## unit level counterfactuals

```{r}
#| echo: true
mod = lm(stress ~ age, data = df)
coef(mod)
```

::::{.columns}
:::{.column width="30%"}
__Data__  
```{r}
#| echo: false
df
```

:::
:::{.column width="20%"}
__Predictions__  
```{r}
#| echo: false
library(marginaleffects)
predictions(mod) |> as_tibble() |> transmute(prediction=estimate)
```

:::
:::{.column width="50%"}
__Counterfactuals__  
```{r}
#| echo: false
library(marginaleffects)
cdf = df |> mutate(age = age+1)
newdata = tibble(
  counterfactual_age = cdf$age,
  prediction = predict(mod, newdata=cdf),
  diff = coef(mod)[2])
newdata
```

:::
::::


## unit level counterfactuals

```{r}
#| echo: true
mod = lm(stress ~ therapy, data = df)
coef(mod)
```

::::{.columns}
:::{.column width="30%"}
__Data__  
```{r}
#| echo: false
df
```

:::
:::{.column width="20%"}
__Predictions__  
```{r}
#| echo: false
library(marginaleffects)
predictions(mod) |> as_tibble() |> transmute(prediction=estimate)
```

:::
:::{.column width="50%"}
__Counterfactuals__  
```{r}
#| echo: false
library(marginaleffects)
cdf = df |> mutate(therapy = ifelse(therapy==1,0,1))
newdata = tibble(
  counterfactual_therapy = cdf$therapy,
  prediction = predict(mod, newdata=cdf),
  diff = prediction - predict(mod))
newdata
```

:::
::::

## holding constant

```{r}
#| echo: true
mod = lm(stress ~ age + exercise + therapy, data = df)
coef(mod)
```

::::{.columns}
:::{.column width="30%"}
__Data__  
```{r}
#| echo: false
df
```

:::
:::{.column width="20%"}
__Predictions__  
```{r}
#| echo: false
library(marginaleffects)
predictions(mod) |> as_tibble() |> transmute(prediction=estimate)
```

:::
:::{.column width="50%"}
__Counterfactuals__  
```{r}
#| echo: false
library(marginaleffects)
cdf = df |> mutate(therapy = ifelse(therapy==1,0,1))
newdata = tibble(
  age=cdf$age,exercise=cdf$exercise,
  counterfactual_therapy = cdf$therapy,
  prediction = predict(mod, newdata=cdf),
  diff = prediction - predict(mod))
newdata

```

:::
::::

## Think in hypotheticals


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age, data = df)
.pp(summary(mod), l = list(9:18))
```

:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide1.PNG)
:::
::::

## Think in hypotheticals


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age, data = df)
.pp(summary(mod), l = list(9:18))
```

:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide2.PNG)
:::
::::

## Think in hypotheticals


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age, data = df)
.pp(summary(mod), l = list(9:18))
```

:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide3.PNG)
:::
::::


## Think in hypotheticals


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ therapy, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ therapy, data = df)
.pp(summary(mod), l = list(9:18))
```

:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide4.PNG)
:::
::::



## Think in hypotheticals


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age + exercise + therapy, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age + exercise + therapy, data = df)
.pp(summary(mod), l = list(9:20))
```


:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide5.PNG)
:::
::::


## Think in hypotheticals

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age + exercise + therapy, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age + exercise + therapy, data = df)
.pp(summary(mod), l = list(9:20))
```
:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide6.PNG)
:::
::::

## Think in hypotheticals

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
#| eval: false
mod = lm(stress ~ age + exercise + therapy, data = df)
summary(mod)
```
```{r}
#| echo: false
mod = lm(stress ~ age + exercise + therapy, data = df)
.pp(summary(mod), l = list(9:20))
```
:::

:::{.column width="50%"}

![](img_sandbox/interactions/Slide7.PNG)
:::
::::


# categorical predictors

## example - simple regression


::::{.columns}
:::{.column width="50%"}

```{r}
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")

braindata <- braindata %>% mutate(
  isMonkey = ifelse(species != "Human", "YES", "NO")
)
```

```{r}
#| echo: true
head(braindata)
```

:::

:::{.column width="50%"}
```{r}
#| eval: false
#| echo: true
monkmod <- lm(mass_brain~isMonkey, data = braindata)
summary(monkmod)
```
```{r}
#| echo: false
monkmod <- lm(mass_brain~isMonkey, data = braindata)
.pp(summary(monkmod),l=list(9:12))
```

- `(Intercept)`: the estimated brain mass of Humans (when the isMonkeyYES variable is zero)
- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1). 
:::
::::


## example - simple regression

::::{.columns}
:::{.column width="50%"}

```{r}
braindata <- read_csv("https://uoepsy.github.io/data/usmr_braindata.csv")

braindata <- braindata %>% mutate(
  isMonkey = ifelse(species != "Human", "YES", "NO")
)
```

```{r}
#| echo: true
head(braindata)
```

:::

:::{.column width="50%"}
```{r}
#| label: fig-binpredplot
#| fig-cap: "A binary categorical predictor"
#| echo: false
fit <- lm(mass_brain~isMonkey, braindata)
ggplot(braindata, aes(x=isMonkey,y=mass_brain))+
  geom_jitter(width=.05, alpha=.1,size=3)+
  stat_summary(geom="pointrange", size = 1, aes(col=isMonkey)) + 
  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col="blue",lwd=1, lty="solid") +
  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col="blue",lwd=1, lty="dotted") +
  geom_segment(aes(x=2,xend=1,y=sum(coef(fit)[1]),yend=sum(coef(fit)[1])),col="blue",lwd=1, lty="dotted") +
  guides(col="none") +
  scale_x_discrete(labels=c("0\n'NO'","1\n'YES'")) +
  labs(title = "lm(mass_brain ~ isMonkey)") +
  annotate(geom="text",x=1,y=coef(fit)[1]+.03,hjust=-0.1, label="Intercept", hjust=1.1)+
  annotate(geom="text",x=2.1,y=coef(fit)[1]-.2, label="isMonkeyYES\n coefficient", hjust=0, angle=90)
```
:::
::::

## example - in multiple regression

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
head(braindata)
```
:::

:::{.column width="50%"}
```{r}
#| eval: false
#| echo: true
monkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)
summary(monkmod2)
```
```{r}
#| echo: false
monkmod2 <- lm(mass_brain~age + isMonkey, data = braindata)
.pp(summary(monkmod2),l=list(9:13))
```

- `(Intercept)`: the estimated brain mass of *new-born* Humans (when both age is zero and isMonkeyYES is zero)
- `age`: the estimated change in brain mass for every 1 year increase in age, holding isMonkey constant.
- `isMonkeyYES`: the estimated change in brain mass from Humans to Monkeys (change in brain mass when moving from isMonkeyYES = 0 to isMonkeyYES = 1), holding age constant.  
:::
::::


## example - in multiple regression

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
head(braindata)
```
:::

:::{.column width="50%"}
```{r}
#| echo: false
#| label: fig-binpredmult
#| fig.cap: "a binary predictor is just another dimension, but data can only exist at either 0 or 1. If a variable were continuous, then observations can take any value along a line"  
par(mfrow=c(1,2))
df <- braindata |> 
  transmute(x1 = age,x2=(isMonkey=="YES")*1,y=mass_brain)

fit<-lm(y~x1+x2,df)
steps=20
x1 <- with(df, seq(min(x1),max(x1),length=steps))
x2 <- with(df, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)
p <- persp(x1,x2,y, theta = 55,phi=10, col = NA,
           border=NA,
           xlab="age",ylab="isMonkey",zlab="mass_brain",
           main="mass_brain~age+isMonkey",zlim=c(0,1))
obs <- with(df, trans3d(x1 ,x2, y, p))
df1 = data.frame(x1=2:34,x2=0)
df1$y = predict(fit,df1)
df2 = data.frame(x1=2:34,x2=1)
df2$y = predict(fit,df2)
pred1 <- with(df1, trans3d(x1 ,x2, y, p))
pred2 <- with(df2, trans3d(x1 ,x2, y, p))
points(pred1, col = "red", pch = 16,type="l",lwd=3)
points(pred2, col = "red", pch = 16,type="l",lwd=3)
points(obs, col = "red", pch = 16)

fit<-lm(y~x1+x2,df)
steps=20
x1 <- with(df, seq(min(x1),max(x1),length=steps))
x2 <- with(df, seq(min(x2),max(x2),length=steps))
newdat <- expand.grid(x1=x1, x2=x2)
y <- matrix(predict(fit, newdat), steps, steps)
p <- persp(x1,x2,y, theta = 55,phi=10, col = NA,
           #border=NA,
           xlab="age",
           ylab="'monkey-ness continuum'", 
           zlab="mass_brain",zlim=c(0,1))
obs <- with(df, trans3d(x1 ,x2, y, p))
df1 = data.frame(x1=2:34,x2=0)
df1$y = predict(fit,df1)
df2 = data.frame(x1=2:34,x2=1)
df2$y = predict(fit,df2)
pred1 <- with(df1, trans3d(x1 ,x2, y, p))
pred2 <- with(df2, trans3d(x1 ,x2, y, p))
points(pred1, col = "red", pch = 16,type="l",lwd=3)
points(pred2, col = "red", pch = 16,type="l",lwd=3)
points(obs, col = "red", pch = 16)

par(mfrow=c(1,1))

```
:::
::::


## example - multiple levels


```{r}
#| echo: true
braindata <- braindata %>% mutate(
  speciesPotar = ifelse(species == "Potar monkey", 1, 0),
  speciesRhesus = ifelse(species == "Rhesus monkey", 1, 0),
)
braindata %>% 
  select(mass_brain, species, speciesPotar, speciesRhesus) %>%
  head()
```

- For a human, both `speciesPotar == 0` and `speciesRhesus == 0`
- For a Potar monkey, `speciesPotar == 1` and `speciesRhesus == 0`
- For a Rhesus monkey, `speciesPotar == 0` and `speciesRhesus == 1`

## example - multiple levels


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
lm(mass_brain ~ species, data = braindata) |> 
  summary()
```
:::

:::{.column width="50%"}
```{r}
#| echo: true
lm(mass_brain ~ speciesPotar + speciesRhesus, data = braindata) |> 
  summary()
```
:::
::::

## example - multiple levels
### more levels = more dimensions

```{r}
#| echo: false
#| out-width: "70%"
fit<-lm(mass_brain ~ species,braindata)
braindata2 <- as.data.frame(model.matrix(fit)[,2:3]) 
braindata2$mass_brain = braindata$mass_brain


library(scatterplot3d)
plt <- with(braindata2,scatterplot3d(`speciesRhesus monkey`,`speciesPotar monkey`,mass_brain, scale.y=1,angle=30,ylab="",
                                     y.ticklabs = c(0,NA,NA,NA,NA,1),
                                     x.ticklabs = c(0,NA,NA,NA,NA,1),
                                     main = "mass_brain ~ species(human/potar monkey/rhesus monkey)"))
text(x = 6.75, y = 0.45, "speciesPotar monkey", srt =15)


pp1 <- tibble(
  `speciesRhesus monkey`=0,
  `speciesPotar monkey`=seq(0,1,.1),
  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[2],length.out=11)
)
pp2 <- tibble(
  `speciesRhesus monkey`=seq(0,1,.1),
  `speciesPotar monkey`=0,
  y = seq(coef(fit)[1], coef(fit)[1]+coef(fit)[3],length.out=11)
)

plt$points(pp1$`speciesRhesus monkey`,pp1$`speciesPotar monkey`,pp1$y,type="l",col="blue")
plt$points(pp2$`speciesRhesus monkey`,pp2$`speciesPotar monkey`,pp2$y,type="l",col="blue")
```

## example - multiple levels

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
lm(mass_brain ~ species, data = braindata) |> 
  summary()
```
:::

:::{.column width="50%"}
```{r}
#| label: fig-kpredplot
#| fig-cap: "A categorical predictor with 3 levels"
#| echo: false
fit <- lm(mass_brain~species, braindata)
ggplot(braindata, aes(x=species,y=mass_brain))+
  geom_jitter(width=.05,alpha=.1,size=3)+
  stat_summary(geom="pointrange", size = 1, aes(col=species)) + 
  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1])),col="blue",lwd=1,lty="dotted") +
  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1)])),col="blue",lwd=1,lty="dotted") +
  geom_segment(aes(x=2,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col="blue",lwd=1,lty="dotted") +
  geom_segment(aes(x=3,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col="blue",lwd=1,lty="dotted") +
  geom_segment(aes(x=1,xend=2,y=coef(fit)[1],yend=sum(coef(fit)[1:2])),col="blue",lwd=.5) +
  geom_segment(aes(x=1,xend=3,y=coef(fit)[1],yend=sum(coef(fit)[c(1,3)])),col="blue",lwd=.5) +
  annotate(geom="text",x=1,y=coef(fit)[1]+.03,hjust=-0.1,label="Intercept") + 
  annotate(geom="text",x=2.1,y=coef(fit)[1]-.3,hjust=0,label="speciesPotar\n coefficient", angle=90) + 
  annotate(geom="text",x=3.1,y=coef(fit)[1]-.15,hjust=0,label="speciesRhesus\n coefficient", angle=90) + 
  guides(col="none") +
  labs(title = "lm(mass_brain ~ species)")+
  scale_x_discrete(labels=c(
    "speciesPotar = 0\nspeciesRhesus = 0\nHuman",
    "speciesPotar = 1\nspeciesRhesus = 0\nPotar monkey",
    "speciesPotar = 0\nspeciesRhesus = 1\nRhesus monkey"
  ))
```
:::
::::






# Thanks all!

