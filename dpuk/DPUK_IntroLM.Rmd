---
title: "<b> Introduction to the Linear Model </b>"
subtitle: "DPUK Spring Academy<br><br> "
author: "Tom Booth, Josiah King, Umberto Noe"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "March 2022"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #  base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
    base_color = "#95A5A6", #intro
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)

library(tidyverse)
library(kableExtra)
library(car)

```


# Overview

- Day 1: What is a linear model?
- Day 2: But I have more variables, what now?
- Day 3: Interactions
- Day 4: Is my model any good?


---
class: center, middle
# Day 1
**What is a linear model?**

---
class: inverse, center, middle

<h2>Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>


---
# Linear model
+ What we will focus on for the majority of the course is how we move from the idea of an association, to estimating a model for the relationship.

+ This model is the **linear model**

+ When using a linear model, we are typically trying to explain variation in an **outcome** (Y, dependent, response) variable, using one or more **predictor** (x, independent, explanatory) variable(s).


---
# Example

.pull-left[

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)

kable(test)
```

]

.pull-right[

**Simple data**

+ `student` = ID variable unique to each respondent

+ `hours` = the number of hours spent studying. This will be our predictor ( $x$ )

+ `score` = test score ( $y$ )

**Question: Do students who study more get higher scores on the test?**
]

---
# Scatterplot of our data

.pull-left[
```{r, echo=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  ylab("Test Score") +
  xlab("Hours Studied")

```
]

.pull-right[

{{content}}

]

--

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  stat_smooth(method="lm", se=FALSE, col="red") +
  ylab("Test Score") +
  xlab("Hours Studied")

```

{{content}}

???
+ we can visualize our data. We can see points moving bottom left to top right
+ so association looks positive
+ Now let's add a line that represents the best model

---
# Definition of the line
+ The line can be described by two values:

+ **Intercept**: the point where the line crosses $y$, and $x$ = 0

+ **Slope**: the gradient of the line, or rate of change

???
+ In our example, intercept = for someone who doesn't study, what score will they get?
+ Slope = for every hour of study, how much will my score change

---
# Intercept and slope

```{r, echo=FALSE, message=FALSE, warning=FALSE}

intercept <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 3, slope = .3) +
  geom_abline(intercept = 4, slope = .3) + 
  geom_abline(intercept = 5, slope = .3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Different intercepts, same slopes")

slope <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 4, slope = .3) +
  geom_abline(intercept = 4, slope = 0) + 
  geom_abline(intercept = 4, slope = -.3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Same intercepts, different slopes")

```

.pull-left[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
intercept

```

]

.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
slope

```

]

---
# How to find a line?
+ The line represents a model of our data.
    + In our example, the model that best characterizes the relationship between hours of study and test score.

+ In the scatterplot, the data is represented by points.

+ So a good line, is a line that is "close" to all points.


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$


+ $y_i$ = the outcome variable (e.g. `score`) 

+ $x_i$ = the predictor variable, (e.g. `hours`)

+ $\beta_0$ = intercept

+ $\beta_1$ = slope

+ $\epsilon_i$ = residual (we will come to this shortly)

where $\epsilon_i \sim N(0, \sigma)$ independently.
  + $\sigma$ = standard deviation (spread) of the errors
  + The standard deviation of the errors, $\sigma$, is constant


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ **Why do we have $i$ in some places and not others?**


--

+ $i$ is a subscript to indicate that each participant has their own value.

+ So each participant has their own: 
    + score on the test ( $y_i$ )
    + number of hours studied ( $x_i$ ) and
    + residual term ( $\epsilon_i$ )

--
+ **What does it mean that the intercept ( $\beta_0$ ) and slope ( $\beta_1$ ) do not have the subscript $i$?**

--

+ It means there is one value for all observations.
    + Remember the model is for **all of our data**

---
# What is $\epsilon_i$?

.pull-left[
+ $\epsilon_i$, or the residual, is a measure of how well the model fits each data point.

+ It is the distance between the model line (on $y$-axis) and a data point.

+ $\epsilon_i$ is positive if the point is above the line (red in plot)

+ $\epsilon_i$ is negative if the point is below the line (blue in plot)

]


.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")


```

]

???
+ comment red = positive and bigger (longer arrow) model is worse
+ blue is negative, and smaller (shorter arrow) model is better
+ key point to link here is the importance of residuals for knowing how good the model is
+ Link to last lecture in that they are the variability 
+ that is the link into least squares


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2>Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# Principle of least squares

+ The numbers $\beta_0$ and $\beta_1$ are typically **unknown** and need to be estimated in order to fit a line through the point cloud.

+ We denote the "best" values as $\hat \beta_0$ and $\hat \beta_1$

+ The best fitting line is found using **least squares**
    + Minimizes the distances between the actual values of $y$ and the model-predicted values of $\hat y$
    + Specifically minimizes the sum of the *squared* deviations

---
# Principle of least squares

+ Actual value = $y_i$

+ Model-predicted value = $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$

+ Deviation or residual = $y_i - \hat y_i$

+ Minimize the **residual sum of squares**, $SS_{Residual}$, which is

$$SS_{Residual} = \sum_{i=1}^{n} [y_i - (\hat \beta_0 + \hat \beta_1 x_{i})]^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

---
# Principle of least squares

+ **Why do you think we square the deviations? **

    + HINT: Look back to the "What is $\epsilon_i$?" slide
    
--

+ We have positive and negative residual terms

+ If we simply added them, they would cancel out.

---
# Data, predicted values and residuals

+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$ = the y-value on the line at specific values of $x$
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example, given you study for 4 hrs, what test score does our model predict you will get.

+ Residual = Difference between $y_i$ and $\hat{y}_i$. So;

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

???
+ these are important distinctions for understanding linear models
+ return to them a lot.

---
# Fitting the line
+ Calculations for slope:

$$\hat \beta_1 = \frac{SP_{xy}}{SS_x}$$


+ $SP_{xy}$ = sum of cross-products:


$$SP_{xy} = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$


+ $SS_x$ = sums of squared deviations of $x$:


$$SS_x = \sum_{i=1}^{n}(x_i - \bar{x})^2$$


<!-- --- -->
<!-- # Equivalent formula -->

<!-- $$\hat \beta_1 =  -->
<!-- \frac{SP_{xy}}{SS_x} =  -->
<!-- r \frac{s_y}{s_x}$$ -->

<!-- where -->

<!-- - $r = \frac{SP_{xy}}{\sqrt{SS_x \times SS_y}}$ -->

<!-- - $s_y = \sqrt{ \frac{SS_y}{n - 1} } = \sqrt{ \frac{\sum_{i=1}^{n}(y_i - \bar{y})^2}{n - 1} }$ -->

<!-- - $s_x = \sqrt{ \frac{SS_x}{n - 1} } = \sqrt{ \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2}{n - 1} }$ -->


---
# Fitting the line
+ Calculations for intercept:

$$\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$$

+ $\hat \beta_1$ = slope estimate

+ $\bar{y}$ = mean of $y$

+ $\bar{x}$ = mean of $x$



---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2>Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# `lm` in R
+ We do not generally calculate by hand like this, so we also briefly introduced the `lm()` function.

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
```

```{r}
lm(score ~ hours, data = test)
```

---
# Interpretation

+ **Slope is the number of units by which Y increases, on average, for a unit increase in X.**

--
    + Unit of Y = 1 point on the test
    + Unit of X = 1 hour of study
    
--

+ So, for every hour of study, test score increases on average by 1.055 points.

--

+ **Intercept is the expected value of Y when X is 0.**

--

    + X = 0 is a student who does not study.

--

+ So, a student who does no study would be expected to score 0.40 on the test.

???
+ So we know in a general sense what the intercept and slope are, but what do they mean with respect to our data and question?

---
# Note of caution on intercepts
+ In our example, 0 has a meaning.
    + It is a student who has studied for 0 hours.
    
+ But it is not always the case that 0 is meaningful.

+ Suppose our predictor variable was not hours of study, but age.

+ **Look back at the interpretation of the intercept, and instead of hours of study, insert age. Read this aloud a couple of times.**

--

+ This is the first instance of a very general lesson about interpreting statistical tests. 
    + The interpretation is always in the context of the constructs and how we have measured them.

---
# Unstandardized vs standardized coefficients
- So far we have calculated unstandardized $\hat \beta_1$.

+ We interpreted the slope as the change in $y$ units for a unit change in $x$
  + Where the unit is determined by how we have measured our variables.

+ In our running example:
  + A unit of study time is 1 hour.
  + A unit of test score is 1 point.
  
+ However, sometimes we may want to represent our results in standard units.

---
# Standardized units
+ Why might standard units be useful?

--

+ **If the scales of our variables are arbitrary.**
  + Example: A sum score of questionnaire items answered on a Likert scale.
  + A unit here would equal moving from a 2 to 3 on one item.
  + This is not especially meaningful (and actually has A LOT of associated assumptions)

--

+ **If we want to compare the effects of variables on different scales**
  + If we want to say something like, the effect of $x_1$ is stronger than the effect of $x_2$, we need a common scale.


---
# Standardizing the coefficients
+ After calculating a $\hat \beta_1$, it can be standardized by:


$$\hat{\beta_1^*} = \hat \beta_1 \frac{s_x}{s_y}$$

+ where;
  + $\hat{\beta_1^*}$ = standardized beta coefficient
  + $\hat \beta_1$ = unstandardized beta coefficient
  + $s_x$ = standard deviation of $x$
  + $s_y$ = standard deviation of $y$

---
# Standardizing the variables

+ Alternatively, for continuous variables, transforming both the IV and DV to $z$-scores (mean=0, SD=1) prior to fitting the model yields standardised betas.

+ $z$-score for $x$:

$$z_{x_i} = \frac{x_i - \bar{x}}{s_x}$$

+ and the $z$-score for $y$:

$$z_{y_i} = \frac{y_i - \bar{y}}{s_y}$$

+ That is, we divide the individual deviations from the mean by the standard deviation
  
---
# Two approaches in action


```{r}
res <- lm(score ~ hours, data = test)
summary(res)$coefficients
```

```{r}
1.055 * (sd(test$hours)/sd(test$score)) #<<
```

---
# Two approaches in action

```{r}
test <- test %>%
  mutate(
    z_score = scale(score, center = T, scale = T),
    z_hours = scale(hours, center = T, scale = T)
  )

res_z <- lm(z_score ~ z_hours, data = test) #<<
summary(res_z)$coefficients
```


---
#  Interpreting standardized regression coefficients  
+ $R^2$ , $F$ and $t$-test remain the same for the standardized coefficients as for unstandardised coefficients.

+ $b_0$ (intercept) = zero when all variables are standardized:
$$
\bar{y} - \hat \beta_1 \bar{x} = 0 - \hat \beta_1  0 = 0
$$

+ The interpretation of the coefficients becomes the increase in $y$ in standard deviation units for every standard deviation increase in $x$

+ So, in our example:

>**For every standard deviation increase in hours of study, test score increases by 0.72 standard deviations**

---
# Relationship to r
+ Standardized slope ( $\hat \beta_1^*$ ) = correlation coefficient ( $r$ ) for a linear model with a single continuous predictor.

+ In our example, $\hat \beta_{hours}^*$ = 0.72

```{r}
cor(test$hours, test$score)
```

+ $r$ is a standardized measure of linear association

+ $\hat \beta_1^*$ is a standardized measure of the linear slope.

---
# Which should we use? 
+ Unstandardized regression coefficients are often more useful when the variables are on  meaningful scales
	+ E.g. X additional hours of exercise per week adds Y years of healthy life

+ Sometimes it's useful to obtain standardized regression coefficients
	+ When the scales of variables are arbitrary
	+ When there is a desire to compare the effects of variables measured on different scales	

+ Cautions
	+ Just because you can put regression coefficients on a common metric doesn't mean they can be meaningfully compared.
	+ The SD is a poor measure of spread for skewed distributions, therefore, be cautious of their use with skewed variables

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2>Part 4: Single binary predictor = t-test</h2>

---
# Recap: Binary variable
+ Binary variable is a categorical variable with two levels.

+ Traditionally coded with a 0 and 1
  + Referred to as dummy coding
  + We will come back to this for categorical variables with 2+ levels

--

+ Why 0 and 1?
  + Quick version: It has some nice properties when it comes to interpretation.


---
# Extending our example

.pull-left[
+ Our in class example so far has used test scores and revision time for 10 students.

+ Let's say we collect this data on 150 students.

+ We also collected data on who they studied with;
  + 0 = alone
  + 1 = with others
  
+ So our variable `study` is a binary
]

.pull-right[

+ This data set is available on LEARN

```{r, warning=FALSE, message=FALSE}
df <- read_csv("./dapr2_lec07.csv") #<<
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(df)%>%
  kable_styling(., full_width = F) %>%
  scroll_box(height = "300px")
```

]

---
#  LM with binary predictors 
+ In previous lectures we asked the question:

  + **Do students who study more get higher scores on the test?**

+ And we specified a linear model:

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ Or

$$score_i = \beta_0 + \beta_1 hours_{i} + \epsilon_i$$

--

+ And nothing changes with our binary variable. We can ask the question:

  + **Do students who study with others score better than students who study alone?**

$$score_i = \beta_0 + \beta_1 study_{i} + \epsilon_i$$

---
# In `R`

```{r}
res2 <- lm(score ~ study, data = df)
summary(res2)
```


---
# Interpretation

.pull-left[
+ As before, the intercept $\hat \beta_0$ is the expected value of $y$ when $x=0$

+ What is $x=0$ here?
  + It is the students who study alone.

+ So what about $\hat \beta_1$?

+ **Look at the output on the right hand side.** 
  + What do you notice about the difference in averages?

]

.pull-right[
```{r warning=FALSE, message=FALSE}
df %>%
  group_by(., study) %>% #<<
  summarise(
    Average = round(mean(score),4) #<<
  )
```


]


---
# Interpretation
+ $\hat \beta_0$ = predicted expected value of $y$ when $x = 0$
  + Or, the mean of group coded 0 (those who study alone)
  
+ $\hat \beta_1$ = predicted difference between the means of the two groups.
  + Group 1 - Group 0 (Mean `score` for those who study with others - mean `score` of those who study alone)
  
+ Notice how this maps to our question. 
  + Do students who study with others score better than students who study alone?
  

---
class: center, middle
# Time for a break

**Have a go at the binary variable quiz**

---
class: center, middle
# Welcome Back!

**Where we left off... **

Let's think about the interpretation by group a little more.

---
#  Equations for each group 
+ What would our linear model look like if we added the values for $x$.


$$\widehat{score} = \hat \beta_0 + \hat \beta_1 study$$


+ For those who study alone ( $study = 0$ ):


$$\widehat{score}_{alone} = \hat \beta_0 + \hat \beta_1 \times 0$$


+ So;


$$\widehat{score}_{alone} = \hat \beta_0$$



---
#  Equations for each group 
+ For those who study with others ( $study = 1$ ):


$$\widehat{score}_{others} = \hat \beta_0 + \hat \beta_1 \times 1$$


+ So;


$$\widehat{score}_{others} = \hat \beta_0 + \hat \beta_1$$


+ And if we re-arrange;


$$\hat \beta_1 = \widehat{score}_{others} - \hat \beta_0$$


+ Remembering that $\widehat{score}_{alone} = \hat \beta_0$, we finally obtain:

$$\hat \beta_1 = \widehat{score}_{others} - \widehat{score}_{alone}$$


---
#  Visualize the model

```{r, echo=FALSE}
bin <- df %>%
  ggplot(., aes(x=factor(study), y=score, colour = study)) +
  geom_point(alpha=0.4) +
  labs(x = "\n Study", y = "Test Score \n") +
  ylim(0,10) +
  scale_x_discrete(labels = c("alone", "others")) +
  theme(legend.position = "none")

bin
```


---
#  Visualize the model
```{r, echo=FALSE}
bin +
  geom_jitter(width = .1, height = 0, alpha=0.4)

```


---
#  Visualize the model
```{r, echo=FALSE}
gpM <- df %>%
  group_by(study) %>%
  summarise(
    score = mean(score)
  )

bin +
  geom_jitter(width = .1, height = 0, alpha=0.4) +
  geom_errorbar(data = gpM, width=0.6,aes(ymax=..y..,ymin=..y..), size=1)

```



---
#  Visualize the model

```{r, echo=FALSE}
library(latex2exp)

bin +
  geom_jitter(width = .1, height = 0, alpha=0.4) +
  geom_errorbar(data = gpM, width=0.6, aes(ymax=..y..,ymin=..y..), size=1)+
  geom_segment(x=1.5, y=gpM[[1,2]], xend=1.5, yend=gpM[[2,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[1,2]], xend=1.52, yend=gpM[[1,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[2,2]], xend=1.52, yend=gpM[[2,2]], size =1, col="red") +
  geom_text(x=1.55, y = 5.5, label = TeX('$\\hat{\\beta}_1$'), size=5, col = "red") +
  geom_text(x=0.65, y = gpM[[1,2]] , label = TeX('$\\hat{\\beta}_0$'), size=5, col = "red")

```


---
#  Evaluation of model and significance of $\beta_1$

+ $R^2$ and $F$-ratio interpretation are identical to their interpretation in models with only continuous predictors.

+ And we assess the significance of predictors in the same way

+ We use the standard error of the coefficient to construct:
  + We calculate the $\hat \beta_1$ = difference between groups
	+ $t$-value and associated $p$-value for the coefficient
	+ Or a confidence interval around the coefficient


---
# Hold on... it's a t-test

```{r}
df %>%
  t.test(score ~ study, .)
```

???
Yup!


---
# Standardizing $\hat \beta_1$

+ When discussing continuous predictors we discussed standardized $b_1$ and unstandardized $\hat \beta_1$

+ Recall, when we calculated $b_1$ we used the SD of x, $s_x$.

+ For a binary categorical variable, the SD is not appropriate.

+ 1 unit also has meaning - it is the membership of a different group.

+ As such, we do not standardize


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 2
**But I have more variables, now what?**

---
class: inverse, center, middle

<h2>Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>

---
#  Multiple regression 
+ The aim of a linear model is to explain variance in an outcome

+ In simple linear models, we have a single predictor, but the model can accommodate (in principle) any number of predictors. 

+ However, when we include multiple predictors, those predictors are likely to correlate

+ Thus, a linear model with multiple predictors finds the optimal prediction of the outcome from several predictors, **taking into account their redundancy with one another**


---
#  Uses of multiple regression 
+ **For prediction:** multiple predictors may lead to improved prediction. 

+ **For theory testing:** often our theories suggest that multiple variables together contribute to variation in an outcome

+ **For covariate control:** we might want to assess the effect of a specific predictor, controlling for the influence of others.
	+ E.g., effects of personality on health after removing the effects of age and sex


---
#  Extending the regression model 

+ Our model for a single predictor:

$$y_i = \beta_0 + \beta_1 x_{1i} + \epsilon_i$$ 

+ is extended to include additional $x$'s:

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i$$  

+ For each $x$, we have an additional $b$
  + $\beta_1$ is the coefficient for the 1st predictor
  + $\beta_2$ for the second etc.


---
#  Interpreting coefficients in multiple regression 

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_j x_{ji} + \epsilon_i$$

+ Given that we have additional variables, our interpretation of the regression coefficients changes a little

+ $\beta_0$ = the predicted value for $y$ **all** $x$ are 0.
	
+ Each $\beta_j$ is now a **partial regression coefficient**
	+ It captures the change in $y$ for a one unit change in , $x$ **when all other x's are held constant**

+ What does holding constant mean? 
  + Refers to finding the effect of the predictor when the values of the other predictors are fixed
		+ It may also be expressed as the effect of **controlling for**, or **partialling out**, or **residualizing for** the other $x$'s

+ With multiple predictors `lm` isolates the effects and estimates the unique contributions of predictors. 

---
# Example with interpretations 


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2>Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>

---
#  Significance of individual effects 
+ A general way to ask this question would be to state: 

> **Is our model model informative about the relationship between X and Y?**

--

+ In the context of our example from last lecture, we could ask, 

> **Is study time a useful predictor of test score?**

--

+ The above is a research question/hypothesis. As we have done before, we need to turn this into a testable statistical hypothesis.

---
#  Evaluating individual predictors 
+ Steps in hypothesis testing:

--
  + Research questions
    
--
  
  + Statistical hypothesis
    
--
  
  + Define the null
    
--
  
  + Calculate an estimate of effect of interest.
  
--
  
  + Calculate an appropriate test statistic.
    
--
  
  + Evaluate the test statistic against the null.
    

---
# Research question and hypotheses

+ **Research questions** are statements of what we intend to study. 

+ A good question defines:

--

  + Constructs under study
  + the relationship being tested
  + A direction of relationship
  + target populations etc.

> **Does increased study time improve test scores in school age children?**

--

+ **Statistical hypotheses** are testable mathematical statements.

--

  + In typical testing in Psychology, we define have a **null ( $H_0$ )** and an **alternative ( $H_1$ )** hypothesis.
  + $H_0$ is precise, and states a specific value for the effect of interest.
  + $H_1$ is not specific, and simply says "something else other than the null is more likely"

???
Flag here that if these comments are completely alien to them, they should go back and recap the hypothesis testing material from dapR1-lectures 12 to 15 (20-21) or 12-14 (19-20).

---
# Defining null

+ Conceptually:
	+ If $x$ yields no information on $y$, then $\beta_1 = 0$
	
+ **Why would this be the case?**

--
	+ $\beta$ gives the predicted change in $y$ for a unit change in $x$.
	+ If $x$ and $y$ are unrelated, then a change in $x$ will not result in any change to the predicted value of $y$
	+ So for a unit change in $x$, there is no (=0) change in $y$.
	
+ We can state this formally as a null and alternative:

$$H_0: \beta_1 = 0$$
$$H_1: \beta_1 \neq 0$$

???
+ For the null to be testable, we need to formally define it. 
+ Point out here the difference in the specificity of the hypotheses. $H_0$ is that the $b_1$ takes a specific value. $H_1$ is that $b_1$ has some value that is not this specific value. i..e one is directly testable, the other is not.


---
class: center, middle
# Time for a break

**Quiz time**

We are about to look at hypothesis tests for coefficients.

So our quiz is on the concept of the standard error


---
class: center, middle
# Welcome Back!

**Where we left off... **

We have defined a null, now let's look at constructing the test


---
# Point estimate and test statistic

+ We have already seen how we calculate $\hat \beta_1$.

+ The associated test statistic to for $\beta$ coefficients is a $t$-statistic

$$t = \frac{\hat \beta}{SE(\hat \beta)}$$

+ where

  + $\hat \beta$ = any beta coefficient we have calculated
  + $SE(\hat \beta)$ = standard error of $\beta$ 

--

+ **Recall** that the standard error describes the spread of the sampling distribution.
  + The standard error (SE) provides a measure of sampling variability
  + Smaller SE's suggest more precise estimate (=good)
  
???
+ brief reminders on test statistics
  + every quantity we wish to calculate a significance test for needs an test statistic.
  + the test statistic is a value that has a known sampling distribution
+ If sampling distribution is unfamiliar, again, recap the hypothesis testing material


---
#  SE( $\hat \beta_1$ )
+ The formula for the standard error of the slope is:

$$SE(\hat \beta_1) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}}$$

+ Where:
	+ $SS_{Residual}$ is the residual sum of squares
	+ $n$ is the sample size
	+ $k$ is the number of predictors (= 1 for simple linear regression)

+ **Given the above, think about what things would make the SE smaller.**

--

+ From this formula, we can see that the SE's for $\beta$ will be smaller when:
	+ Residual variance ( $SS_{Residual}$ ) is smaller
	+ Sample size, $n$, is larger


---
# Back to the example

$$t = \frac{\hat \beta_1}{SE(\hat \beta_1)}$$

+ Let's calculate $t$ for our example.
+ We will use some values we have already calculated
  + $\hat \beta_1 = 1.055$ 

--

+ So we need $SE(\hat \beta_1)$

$$SE(\hat \beta_1) = \sqrt{\frac{SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}}$$

+ $n$ = 10
+ $k$ = 1
+ $\sum(x_i - \bar{x})^2$ = 20.625

---
# Back to the example
+ So all we have left is $SS_{Residual}$.

+ From earlier this week, we know:

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ $SS_{Residual}$ = residual sum of squares = sum of the squared residuals

```{r}
res <- lm(score ~ hours, data = test)
SSRes = sum(res$residuals^2)
SSRes
```

---
# Back to our example

+ So pull all this together:

$$SE(\hat \beta_1) = \sqrt{\frac{SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}} 
= \sqrt{\frac{21.16364 / (10-1-1)}{20.625}} = \sqrt{\frac{2.65455}{20.625}} = 0.35814$$

--

+ and finally

$$t = \frac{\hat \beta_1}{SE(\hat \beta_1)} = \frac{1.055}{0.35814} = 2.945773 = 2.95$$


---
# Sampling distribution for the null

+ Now we have our $t$-statistic, we need to evaluate it.

+ For that, we need sampling distribution for the null.

+ For $\beta$, this is a $t$-distribution with $n-k-1$ degrees of freedom.
	+ Where $k$ is the number of predictors, and the additional -1 represents the intercept.

--

+ So for linear models with 1 predictor this is $n-2$
  + In our case = 8

---
#  A decision about the null 
+ So we have a $t$-value associated with our $\beta$ coefficient.
	+ t = 2.95

+ And we know we will evaluate it against a $t$-distribution with 8 df.

+ As with all tests we need to set our $\alpha$.
	+ Let's take 0.05 two tailed.

--

+ Now we need a critical value to compare our observed $t$-value to.

---
# Visualize the null

.pull-left[
```{r, echo=FALSE}
ggplot() + 
  xlim(-8, 8) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=8)) +
  stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.025, 8), -8),
                alpha=.25,
                fill = "blue",
                args = list(df=8)) +
    stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.975, 8), 8),
                alpha=.25,
                fill = "blue",
                args = list(df=8)) +
  geom_vline(xintercept = 2.95, col="red") +
  xlab("\n t") +
  ylab("") +
  ggtitle("t-distribution (df=8); t-statistic (2.95; red line)")
```

]

.pull-right[

+ Critical value and $p$-value:
```{r}
tibble(
  LowerCrit = round(qt(0.025, 8), 3),
  UpperCrit = round(qt(0.975, 8), 3),
  Exactp = (1 - pt(2.95, 8)) * 2
)
```

]

???
+ discuss this plot.
+ remind them of 2-tailed
+ areas
+ % underneath each end
+ comment on how it would be different one tailed
+ remind about what X is, thus where the line is

---
# `lm` in R

.pull-left[
```{r}
summary(res)
```
]

.pull-right[
+ So in our example, we **reject the null**.
+ **Spend a little bit of time looking at this output, and comparing the various values to those things we calculated in our example.**
  + Some we haven't yet looked at. 
  + They are coming next.

]


---
#  Standard errors: Multiple predictors

.pull-left[

$$SE(\hat \beta_j) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_i - \bar{x})^2}}$$

]


.pull-right[

$$SE(\hat \beta_j) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_{ij} - \bar{x_{ij}})^2(1-R_{xj}^2)}}$$

+ $1-R_{xj}^2$ is capturing the correlation between $x_j$ and all other $x$'s

]

---
#  Standard errors: Multiple predictors

$$SE(\hat \beta_j) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_{ij} - \bar{x_{ij}})^2(1-R_{xj}^2)}}$$

+ Examining the above formula we can see that:
	+ $SE$ is smaller when residual variance ( $SS_{residual}$ ) is smaller
	+ $SE$ is smaller when sample size ( $N$ ) is larger
	+ $SE$ is larger when the number of predictors ( $k$ ) is larger
	+ $SE$ is larger when a predictor is strongly correlated with other predictors ( $R_{xj}^2$ )

???
+ Well return to this later when we discuss multi-collinearity issues



---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2>Part 3: Evaluating my model </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Comparing models </h2>

---
#  Quality of the overall model 
+ When we measure an outcome ( $y$ ) in some data, the scores will vary (we hope).
  + Variation in $y$ = total variation of interest.

--

+ The aim of our linear model is to build a model which describes $y$ as a function of $x$.
	+ That is we are trying to explain variation in $y$ using $x$.

--

+ But it won't explain it all.
  + What is left unexplained is called the residual variance.

--

+ So we can breakdown variation in our data based on sums of squares as;

$$SS_{Total} = SS_{Model} + SS_{Residual}$$

---
#  Coefficient of determination 
+ One way to consider how good our model is, would be to consider the proportion of total variance our model accounts for. 

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$$

+ $R^2$ = coefficient of determination

--

  + Quantifies the amount of variability in the outcome accounted for by the predictors.
  + More variance accounted for, the better.
  + Represents the extent to which the prediction of $y$ is improved when predictions are based on the linear relation between $x$ and $y$.

--

+ Let's see how it works.
  + To do so, we need to calculate the different sums of squares.


---
# Total Sum of Squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

+ Squared distance of each data point from the mean of $y$.

+ Mean is our baseline. 

+ Without any other information, our best guess at the value of $y$ for any person is the mean.

]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Total}')) +
  geom_hline(aes(yintercept = mean(score)),color="blue", size=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(rep(mean(score),10))), color = "red", lty =2)
```

]

---
# Calculations

.pull-left[
```{r}
ss_tab <- test %>%
    mutate(
        y_dev = score - mean(score), 
        y_dev2 = y_dev^2
    )
```

```{r}
ss_tab %>%
    summarize(
        ss_tot = sum(y_dev2)
    )
```

]

.pull-right[

```{r, echo=FALSE}
kable(ss_tab) %>%
  kable_styling(., full_width = F)
```

]

---
# Residual sum of squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ Which you may recognise.

+ Squared distance of each point from the predicted value.
]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Residual}')) +
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", size=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(res$fitted.values)), color = "red", lty =2)

```

]

---
# Calculations

.pull-left[
```{r}
ss_tab <- ss_tab %>%
  mutate(
    y_pred = round(res$fitted.values,2),
    pred_dev = round((score - y_pred),2),
    pred_dev2 = round(pred_dev^2,2)
  )
```

```{r}
ss_tab %>%
  summarize(
    ss_tot = sum(y_dev2),
    ss_resid = sum(pred_dev2) #<<
  )
```

]

.pull-right[

```{r, echo=FALSE}
kable(ss_tab[-c(4,5)]) %>%
  kable_styling(., full_width = F)
```

]


---
# Model sums of squares

.pull-left[
+ Sums of squares quantify difference sources of variation.

$$SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$

+ That is, it is the deviance of the predicted scores from the mean of $y$.

+ But it is easier to simply take:

$$SS_{Model} = SS_{Total} - SS_{Residual}$$

]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle(latex2exp::TeX('SS_{Model}')) +
  geom_hline(aes(yintercept = mean(score)),color="blue", size=1) + 
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", size=1) + 
  geom_segment(aes(x = hours, c(res$fitted.values), xend = hours, yend = c(rep(mean(score),10))), color = "red", lty =2)
```

]


---
# Calculations

.pull-left[

$$SS_{Model} = SS_{Total} - SS_{Residual}$$

```{r}
ss_tab %>%
  summarize(
    ss_tot = sum(y_dev2),
    ss_resid = sum(pred_dev2)
  ) %>%
  mutate( #<<
    ss_mod = ss_tot - ss_resid #<<
  )
```

]

.pull-right[

```{r, echo=FALSE}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color="red", size = 2) +
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0))) +
  theme(axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0))) +
  xlab("Hours of Study") +
  ylab("Test Score") +
  ggtitle("SS Model") +
  geom_hline(aes(yintercept = mean(score)),color="blue", size=1) + 
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", size=1) + 
  geom_segment(aes(x = hours, c(res$fitted.values), xend = hours, yend = c(rep(mean(score),10))), color = "red", lty =2)
```

]


---
#  Coefficient of determination 
+ Now we can finally come back to $R^2$.

$$R^2 = 1 - \frac{SS_{Residual}}{SS_{Total}}$$

+ Or

$$R^2 = \frac{SS_{Model}}{SS_{Total}}$$

+ So in our example:

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = \frac{22.9}{44.1} = 0.519$$

+ ** $R^2$ = 0.519 means that 52% of the variation in test scores is accounted for by hours of revision.**

---
#  Our example 

```{r}
res <- lm(score ~ hours, data = test)
summary(res)
```

???
As at the end of last session, we can check this against the R-output:
Be sure to flag small amounts of rounding difference from working through "by hand" and so presenting to less decimal places.

---
#  Adjusted $R^2$ 
+ We can also compute an adjusted $R^2$ when our lm has 2+ predictors.
  + $R^2$ is an inflated estimate of the corresponding population value

+ Due to random sampling fluctuation, even when $R^2 = 0$ in the population, it's value in the sample may $\neq 0$ 

+ In **smaller samples** , the fluctuations from zero will be larger on average

+ With **more IVs** , there are more opportunities to add to the positive fluctuation


$$\hat R^2 = 1 - (1 - R^2)\frac{N-1}{N-k-1}$$

+ Adjusted $R^2$ adjusts for both sample size ( $N$ ) and number of predictors ( $k$ )



---
#  Significance of the overall model 
+ The test of the individual predictors (IVs, or $x$'s) does not tell us if the overall model is significant or not.
	+ Neither does R-square
	+ But both are indicative

+ To test the significance of the model as a whole, we conduct an $F$-test.

---
#  F-ratio
+ $F$-ratio tests the null hypothesis that all the regression slopes in a model are all zero
  + We are currently talking about a model with only one $x$, thus one slope.
  + But the $F$-ratio test will generalise.

--

+ $F$-ratio is a ratio of the explained to unexplained variance:

$$F = \frac{MS_{Model}}{MS_{Residual}}$$

+ Where MS = mean squares

--

+ **What are mean squares?**
  + Mean squares are sums of squares calculations divided by the associated degrees of freedom.
  + The degrees of freedom are defined by the number of "independent" values associated with the different calculations.

---
# F-table

```{r, echo=FALSE}
ftest <- tibble(
  SS = c("Model", "Residual", "Total"),
  df = c("k", "n-k-1", " "),
  MS = c("SS model/df model", "SS residual/df residual", " "),
  Fratio = c("MS model/ MS residual", " ", " "),
  pvalue = c("F(df model,df residual)", "", "")
)

kable(ftest)%>%
  kable_styling(., full_width = F)
```

---
# Our example: F-table

```{r, echo=FALSE}
tibble(
  Component = c("Model", "Residual", "Total"),
  SS = c(22.9,21.2,44.1),
  df = c(1, 8, " "),
  MS = c(22.9, 2.65, " "),
  Fratio = c(8.641509, " ", " "),
  pvalue = c("F(1,8)", "", "")
) %>%
  kable(.)%>%
  kable_styling(., full_width = F)
```


---
# F-ratio
+ Bigger $F$-ratios indicate better models.
  + It means the model variance is big compared to the residual variance.

--

+ The null hypothesis for the model says that the best guess of any individuals $y$ value is the mean of $y$ plus error.
	+ Or, that the $x$ variables carry no information collectively about $y$.

--

+ $F$-ratio will be close to 1 when the null hypothesis is true
  + If there is equivalent residual to model variation, $F$=1
	+ If there is more model than residual $F$ > 1

--

+ $F$-ratio is then evaluated against an $F$-distribution with $df_{Model}$ and $df_{Residual}$ and a pre-defined $\alpha$

--

+ Testing the $F$-ratio evaluates statistical significance of the overall model

---
# Visualize the test

.pull-left[

```{r, echo=FALSE}
ggplot() +
  xlim(-1,10) + 
  stat_function(fun=df,
                geom = "line",
                args = list(df1=1, df2=8)) +
  stat_function(fun = df, 
                geom = "area",
                xlim = c(qf(0.95, 1,8), 10),
                alpha=.25,
                fill = "blue",
                args = list(df1=1, df2=8)) +
  geom_vline(xintercept = 8.641509, col="red") +
  xlab("\n F") +
  ylab("") +
  ggtitle("F-distribution (1,8); f-ratio (8.64; red line)")
```
]

.pull-right[

+ Critical value and $p$-value:
```{r}
tibble(
  Crit = round(qf(0.95, 1, 8),3),
  Exactp = 1-pf(8.64, 1, 8)
)
```

+ From this we would **reject the null**. 

]

---
#  Our example 

```{r}
res <- lm(score ~ hours, data = test)
summary(res)
```

???
As at the end of last session, we can check this against the R-output:
Comment on the minor differences for rounding.



---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Adding more predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Evaluating predictors </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Evaluating my model </h2>
<h2>Part 4: Comparing models </h2>

---
# Why might we compare models?

+ Suppose we wanted to know the effect of a key predictor, after first having controlled for some covariates.
  + In many places described as hierarchical regression

+ We can't do this with the skills learned so far. 

+ We can using model comparison tools. 

---
# Recall the $F$-test
+ $F$-ratio is a ratio of the explained to unexplained variance:

$$F = \frac{MS_{Model}}{MS_{Residual}}$$

+ Where the mean squares (MS) are the sums of squares divided by the degrees of freedom. So we can also write:

$$F = \frac{SS_{Model}/df_{Model}}{SS_{Residual}/df_{residual}}$$

---
# F-ratio
+ Bigger $F$-ratios indicate better models.
  + It means the model variance is big compared to the residual variance.

+ The null hypothesis for the model says that the best guess of any individuals $y$ value is the mean of $y$ plus error.
	+ Or, that the $x$ variables carry no information collectively about $y$.
	+ Or, a test that all $\beta$ = 0

+ $F$-ratio will be close to 1 when the null hypothesis is true
  + If there is equivalent residual to model variation, $F$=1
	+ If there is more model than residual $F$ > 1

+ $F$-ratio is then evaluated against an $F$-distribution with $df_{Model}$ and $df_{Residual}$ and a pre-defined $\alpha$

+ Testing the $F$-ratio evaluates statistical significance of the overall model


---
# $F$-test as an incremental test

+ One important way we can think about the $F$-test and the $F$-ratio is as an incremental test against an "empty" or null model.

+ A null or empty model is a linear model with only the intercept.
  + In this model, our predicted value of the outcome for every case in our data set, is the mean of the outcome.
  + That is, with no predictors, we have no information that may help us predict the outcome.
  + So we will be "least wrong" by guessing the mean of the outcome.

+ An empty model is the same as saying all $\beta$ = 0.

+ So in this way, the $F$-test we have already seen **is comparing two models**.

+ We can extend this idea, and use the $F$-test to compare two models that contain different sets of predictors.
  + This is the **incremental $F$-test**

---
# Incremental $F$-test
.pull-left[
+ The incremental $F$-test evaluates the statistical significance of the improvement in variance explained in an outcome with the addition of further predictor(s)

+ It is based on the difference in $F$-values between two models.
  + We call the model with the additional predictor(s) model 1 or full model
  + We call the model without model 0 or restricted model

]

.pull-right[
$$F_{(df_R-df_F),df_F} = \frac{(SSR_R-SSR_F)/(df_R-df_F)}{SSR_F / df_F}$$



$$
\begin{align}
& \text{Where:} \\
& SSR_R = \text{residual sums of squares for the restricted model} \\
& SSR_F = \text{residual sums of squares for the full model} \\
& df_R = \text{residual degrees of freedom from the restricted model} \\
& df_F = \text{residual degrees of freedom from the full model} \\
\end{align}
$$
]


---
# Incremental $F$-test in R

+ In order to apply the $F$-test for model comparison in R, we use the `anova()` function.

+ `anova()` takes as its arguments models that we wish to compare
  + Here we will show examples with 2 models, but we can use more.

---
# Example


---
# Other uses for `anova()`


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 3
**Interactions (uh-oh)**

---
class: inverse, center, middle

<h2>Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>

---
#  Lecture notation 

+ For the next two lectures, we will work with the following equation and notation:

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ $y$ is a continuous outcome

+ $x$ will always be the continuous predictor

+ $z$ will be either the continuous or binary predictor
	+ Dependent on the type of interaction we are discussing.
	
+ $xz$ is their product or interaction predictor

---
#  General definition 

+ When the effects of one predictor on the outcome differ across levels of another predictor.

+ Note interactions are symmetrical. 

+ What does this mean?
  + We can talk about interaction of X with Z, or Z with X.
  + These are identical.

---
#  General definition 

+ Categorical*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the outcome is different across levels of a categorical predictor.

--

+ Continuous*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the outcome changes as the values of a second continuous predictor change.
	+ May have heard this referred to as moderation.

--

+ Categorical*categorical interaction:
	+ There is a difference in the differences between groups across levels of a second factor.
	+ We will discuss this in the context of linear models for experimental design


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2>Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>


---
#  Interpretation: Categorical*Continuous 


$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Where $z$ is a binary predictor

  + $\beta_0$ = Value of $y$ when $x$ and $z$ are 0

  + $\beta_1$ = Effect of $x$ (slope) when $z$ = 0 (reference group)

  + $\beta_2$ = Difference intercept between $z$ = 0 and $z$ = 1, when $x$ = 0.

  + $\beta_3$ = Difference in slope across levels of $z$

---
# Example LBC

- Calculate
- Show plot by group
- Show equation by group = simple slopes


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2>Part 3: Continuous*Continuous interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Categorical*categorical interactions </h2>

---
#  Lecture notation 

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Lecture notation:
  + $y$ is a continuous outcome
  
  + $x$ is a continuous predictor
  
  + $z$ is a continuous predictor
  
  + $xz$ is their product or interaction predictor

---
#  Interpretation: Continuous*Continuous 

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Lecture notation:
  
  + $\beta_0$ = Value of $y$ when $x$ and $z$ are 0
  
  + $\beta_1$ = Effect of $x$ (slope) when $z$ = 0
  
  + $\beta_2$ = Effect of $z$ (slope) when $x$ = 0
  
  +  $\beta_3$ = Change in slope of $x$ on $y$ across values of $z$ (and vice versa).
	    + Or how the effect of $x$ depends on $z$ (and vice versa)

---
#  Interpretation: Continuous*Continuous 

$$y_i = \beta_0 + \beta_1 x_{i} + \beta_2 z_{i} + \beta_3 xz_{i} + \epsilon_i$$

+ Note, $\beta_1$ and $\beta_2$ and are referred to as conditional effects, **not as main effects** .
	
	+ They are the effects at the value 0 of the interacting variable.
	
	+ Main effects are typically assumed to be constant.
	
	+ For any $\beta$ associated with a variable **not** included in the interaction, interpretation does not change.

---
# Example LBC

- Show example with interpretation
- Simple slopes (need to pick values)


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is an interaction and why are we talking about it? </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Continuous*binary interactions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Continuous*Continuous interactions </h2>
<h2>Part 4: Categorical*categorical interactions </h2>


---
class: center, middle
# Thanks all!

---
class: center, middle
# Day 4
**Is my model any good?**

---
class: inverse, center, middle

<h2>Part 1: Key model assumptions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Basic model diagnostics </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: What we have not covered? </h2>

---
# Linear model assumptions 
+ So far, we have discussed evaluating linear models with respect to:
	+ Overall model fit ( $F$ -ratio, $R^2$)
	+ Individual predictors

+ However, the linear model is also built on a set of assumptions.

+ If these assumptions are violated, the model will not be very accurate.

+ Thus, we also need to assess the extent to which these assumptions are met.


---
# Some data for today

.pull-left[
+ Let's look again at our data predicting salary from years or service and performance ratings (no interaction).

$$y_i = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon_i$$

+ $y$ = Salary (unit = thousands of pounds ).

+ $x_1$ = Years of service.

+ $x_2$ = Average performance ratings.
 
]

.pull-right[

```{r, echo=FALSE}
salary2 <- read_csv("./salary2.csv")
salary2 %>%
  slice(1:10) %>%
  kable(.) %>%
  kable_styling(full_width = F)
```


]

---
# Our model
```{r}
m1 <- lm(salary ~ perf + serv, data = salary2)
```

+ We will run all our assumptions based on the object `m1`

---
# Visualizations vs tests
+ There exist a variety of ways to assess assumptions, which broadly split into statistical tests and visualizations.

+ We will focus on visualization:
	+ Easier to see the nature and magnitude of the assumption violation
	+ There is also a very useful function for producing them all.

+ Statistical tests often suggest assumptions are violated when problem is small.
  + This is to do with the statistical power of the tests.
  + Give no information on what the actual problem is.
  + A summary table of tests will be given at the end of the lecture.


---
# Visualizations made easy
+ For a majority of assumption and diagnostic plots, we will make use of the `plot()` function.
  + If we give `plot()` a linear model object (e.g. `m1` or `m2`), we can automatically generate assumption plots.

+ We will also make use of some individual functions for specific visualizations.

+ Alternatively, we can also use `check_model()` from the `performance` package.
  + This provides `ggplot` figures as well as some notes to aid interpretation.
  + Caution that these plots are **not in a format to use directly in reports**

---
#  Linearity 
+ **Assumption**: The relationship between $y$ and $x$ is linear.
  + Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation


+ **Investigated with**:
  + Scatterplots with loess lines (single variables)
  + Component-residual plots (when we have multiple predictors)


---
# Linear vs non-linear

.pull-left[

```{r, echo=FALSE, message=FALSE}
df2 <- tibble(
  x = rnorm(1000, 10, 2),
  y = 5 + .8*x + rnorm(1000, 0,.5),
  y2 = 5 + .6*(x^3) + rnorm(1000, 0,10)
)

df2 %>%
  ggplot(., aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method = "lm", se=F) +
  labs(x= "X", y="Y", title = "Linear Relationship")

```

]

.pull-right[

```{r, message=FALSE, echo=FALSE}
df2 %>%
  ggplot(., aes(x=x, y=y2)) +
  geom_point() +
  geom_smooth(method = "lm", se=F) +
  labs(x= "X", y="Y", title = "Non-linear Relationship")
```

]

---
#  What is a loess line?

+ Method for helping visualize the shape of relationships:

+ Stands for...
  + **LO**cally
  + **E**stimated
  + **S**catterplot
  + **S**moothing

+ Essentially produces a line with follows the data.

+ Useful for single predictors.

---
# Visualization

.pull-left[
```{r, warning=FALSE}
lin_m1 <- salary2 %>%
  ggplot(., aes(x=serv, y=perf)) +
  geom_point()+
  geom_smooth(method = "lm", se=F) + # <<
  geom_smooth(method = "loess", se=F, #<<
              col = "red") +
  labs(x= "Years of Service", y="Performance", 
       title = "Scatterplot with linear (blue) 
       and loess (red) lines")
```
]

.pull-right[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
lin_m1
```

]

---
#  Non-linearity

+ With multiple predictors, we need to know whether the relations are linear between each predictor and outcome, controlling for the other predictors

+ This can be done using **component-residual plots**
  + Also known as partial-residual plots
		
+ Component-residual plots have the $x$ values on the X-axis and partial residuals on the Y-axis

+ *Partial residuals* for each X variable are:

$$\epsilon_i + B_jX_{ij}$$

+ Where :
	+ $\epsilon_i$ is the residual from the linear model including all the predictors
	+ $B_jX_{ij}$ is the partial (linear) relation between $x_j$ and $y$

---
#  `crPlots()` 

+ Component-residual plots can be obtained using the `crPlots()` function from `car` package

```{r, eval = F, warning=FALSE}
m1 <- lm(salary ~ perf + serv, data = salary2)
crPlots(m1)
```

+ The plots for continuous predictors show a linear (dashed) and loess (solid) line

+ The loess line should follow the linear line closely, with deviations suggesting non-linearity

---
#  `crPlots()`
```{r, echo= F, warning=FALSE, fig.height=3.8}
m1 <- lm(salary ~ perf + serv, data = salary2)
crPlots(m1)
```


???
+ Here the relations look pretty good.

+ Deviations of the line are minor

---
# Normally distributed errors 
+ **Assumption**: The errors ( $\epsilon_i$ ) are normally distributed around each predicted value.

+ **Investigated with**:
  + QQ-plots
  +	Histograms

	
---
# Visualizations 
+ **Histograms**: Plot the frequency distribution of the residuals.

```{r, eval=FALSE}
hist(m1$residuals)
```

--

+ **Q-Q Plots**: Quantile comparison plots.
	+ Plot the standardized residuals from the model against their theoretically expected values.
	+ If the residuals are normally distributed, the points should fall neatly on the diagonal of the plot.
	+ Non-normally distributed residuals cause deviations of points from the diagonal.
		+ The specific shape of these deviations are characteristic of the distribution of the residuals.

```{r, eval=FALSE}
plot(m1, which = 2) #<<
```


---
# Visualizations

.pull-left[

```{r, echo=FALSE}
hist(m1$residuals)
```

]


.pull-right[

```{r, echo=FALSE}
plot(m1, which = 2) #<<
```

]


---
#  Equal variance (Homoscedasticity) 

+ **Assumption**: The equal variances assumption is constant across values of the predictors $x_1$, ... $x_k$, and across values of the fitted values $\hat{y}$
	+ Heteroscedasticity refers to when this assumption is violated (non-constant variance)

+ **Investigated with**:
  + Plot residual values against the predicted values ( $\hat{y}$ ).

---
#  Residual-vs-predicted values plot 
+ In R, we can plot the residuals vs predicted values using `residualPlot()` function in the `car` package.
  + Categorical predictors should show a similar spread of residual values across their levels
  + The plots for continuous predictors should look like a random array of dots
	  + The solid line should follow the dashed line closely

```{r, eval=FALSE}
residualPlot(m1)
```

+ We can also get this plot using:

```{r, eval=FALSE}
plot(m1, which = 1)
```

---
#  Residual-vs-predicted values plot 

.pull-left[
```{r, echo=FALSE}
residualPlot(m1)
```

]

.pull-right[
```{r, echo=FALSE}
plot(m1, which = 1)
```
]



---
#  Independence of errors 
+ **Assumption**: The errors are not correlated with one another

+ Difficult to test unless we know the potential source of correlation between cases.
  + We will see more of this in year 3.

+ Essentially, for now, we will evaluate this based on study design.
  + If a design is between person, we will assume the errors to be independent.


---
#  Multi-collinearity 
+ This is **not an assumption of linear model**, but it is something we need to consider. 
  + It sits between assumptions and case diagnostics.
  
+ Multi-collinearity refers to the correlation between predictors
  + We saw this in the formula for the standard error of model slopes for an `lm` with multiple predictors.

+ When there are large correlations between predictors, the standard errors are increased
	+ Therefore, we don't want our predictors to be too correlated

---
#  Variance Inflation Factor 
+ The **Variance Inflation Factor** or VIF quantifies the extent to which standard errors are increased by predictor inter-correlations

+ It can be obtained in R using the `vif()` function:

```{r}
vif(m1)
```

+ The function gives a VIF value for each predictor

+ Ideally, we want values to be close to 1

+ VIFs> 10 indicate a problem

---
#  What to do about multi-collinearity 

+ In practice, multi-collinearity is not often a major problem

+ When issues arise, consider:
	+ Combining highly correlated predictors into a single composite
		  + E.g. create a sum or average of the two predictors
	+ Dropping an IV that is obviously statistically and conceptually redundant with another from the model


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Key model assumptions </h2>
<h2>Part 2: Basic model diagnostics  </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: What we have not covered? </h2>

---
#  Three important features

+ Model outliers
	+ Cases for which there is a large discrepancy between their predicted value ( $\hat{y_i}$ ) and their observed value ( $y_i$ )

--

+ High leverage cases
	+ Cases with an unusual value of the predictor ( $x_i$ )

--

+ High influence cases
	+ Cases who are having a large impact on the estimation of model

---
#  Influence

+ High leverage cases, when they are also linear model outliers, will have high **influence**

+ Cases with high influence, have a strong effect on the coefficients

+ If we deleted such a case, the linear model coefficients would change substantially


---
# Influence

+ If a handful of influential cases are responsible for the linear model results, the conclusions might not generalise very well

+ Multiple ways to consider influence.
  + Here we will discuss Cook's distance.
  + In a few weeks we will consider some additional measures
  
+ Cook's Distance of a data point $i$ (can be written many ways):


$$D_i = \frac{(\text{StandardizedResidual}_i)^2}{k+1} \times \frac{h_i}{1-h_i}$$

---
#  Cooks Distance 
$$\frac{(\text{StandardizedResidual}_i)^2}{k+1} = \text{Outlyingness}$$


$$\frac{h_i}{1-h_i} = \text{Leverage}$$

+ So $D_i = \text{Outlyingness} \times \text{Leverage}$


+ Cook's distance refers to **the average distance the $\hat{y}$ values will move if a given case is removed.**
  + If removing one case changes the predicted values a lot (moves the regression line), then that case is influencing our results.

---
#  Cooks Distance 

+ Many different suggestions for cut-off's:
  + $D_i > 1$ 
  + $D_i > \frac{4}{n-k-1}$
  + Or size relative all values in data set

---
#  Cook's distance in R

.pull-left[
```{r}
#df %>%
#  mutate(
#    cook = cooks.distance(m1) #<<
#  ) %>%
#  filter(., cook > 4/(150-1-1)) %>% #<<
#  kable(.)  %>%
#  kable_styling(., full_width = F)
```

]

.pull-right[
```{r, echo=F}
plot(m1, which = 4)
```

]


---
# Influence of coefficients
+ Cook's distance is a single value summarizing the total influence of a case

+ In the context of a lm with 2+ predictors, we may want to look in a little more detail.

+ **DFFit**: The difference between the predicted outcome value for a case with versus without a case included

+ **DFbeta**: The difference between the value for a coefficient with and without a case included

+ **DFbetas**: A standardised version of DFbeta
  + Obtained by dividing by an estimate of the standard error of the regression coefficient with the case removed

---
#  COVRATIO 
+ Influence on standard errors can be measured using the **COVRATIO** statistic
	+ COVRATIO value <1 show that precision is decreased (SE increased)  by a case
	+ COVRATIO value >1 show that precision is increased (SE decreased) by a case

+ Cases with COVRATIOS $> 1+[3(k +1)/n]$ or $< 1-[3( k +1)/ n ]$ can be considered to have a strong influence on the standard errors

---
#  COVRATIO in R 
+ COVRATIO values can be extracted using the `covratio()` function:
+ We can extract these measures using the `influence.measures()` function


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Key model assumptions </h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Basic model diagnostics </h2>
<h2>Part 3: What we have not covered? </h2>

---
# In short, quite a lot!

+ Categorical predictors with 2+ levels
+ Coding schemes for categorical data
+ Coding specific comparisons
+ Interactions with categorical variables and 2+ levels
+ Statistical tests for assumptions
+ Assumption violations and corrections
+ Extended model diagnostics
+ Non-continuous outcome variables
+ Confidence intervals
+ Bootstrapping


---
class: center, middle
# Thanks all!