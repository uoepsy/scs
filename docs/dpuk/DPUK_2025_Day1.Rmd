---
title: "<b> Introduction to the Linear Model </b>"
subtitle: "DPUK Spring Academy<br><br> "
author: "Josiah King, Umberto Noe, (and credits to Tom Booth)"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "April 2025"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    self_contained: true
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.height = 6)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #  base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
    base_color = "#95A5A6", #intro
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)

library(tidyverse)
library(kableExtra)
library(car)

theme_set(theme_gray(base_size = 15))
```


# Overview

- Day 1: What is a linear model?
- Day 2: But I have more variables, what now?
- Day 3: Interactions
- Day 4: Is my model any good?


---
class: center, middle
# Day 1
**What is a linear model?**

---
class: inverse, center, middle

<h2>Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# What is a model?
+ Pretty much all statistics is about models.

+ A model is an idea about the way the world is.
  + A formal representation of a system or relationships

+ Typically we represent models as functions.
  + We input data
  + Specify a set of relationships
  + We output a prediction

---
# An Example
+ To think through these relations, we can use a simple example.

+ Suppose I have a model for growth of babies.<sup>1</sup>

$$
Length = 55 + 4 * Month
$$

.footnote[
[1] Length is measured in cm.
]

---
# Visualizing a model

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
fun1 <- function(x) 55 + (4*x)
m1 <- ggplot(data = data.frame(x=0), aes(x=x)) +
  stat_function(fun = fun1) +
  xlim(0,24) +
  ylim(0,150) +
  ylab("Length (cm)") +
  xlab("Age (months)") #+
  #geom_point(colour = "red", size = 3) #+
 # geom_segment(aes(x = x, y = fx, xend = x, yend = 0), 
  #             arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  #geom_segment(aes(x = x, y = fx, xend = 0, yend = fx), 
   #            arrow=arrow(type = "closed", length = unit(0.25, "cm")))
m1
```
]

.pull-right[

{{content}}
]

--
+ The black line represents our model
{{content}}

--
+ The x-axis shows `Age` $(x)$
{{content}}

--
+ The y-axis values for `Length` our model predicts
{{content}}



---
# Models as "a state of the world"
+ Let's suppose my model is true.
  + That is, it is a perfect representation of how babies grow.
  
+ My models creates predictions.

+ **IF** my model is a true representation of the world, **THEN** data from the world should closely match my predictions.


---
# Predictions and data

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
m1+
  geom_segment(aes(x = 11, y = 99, xend = 11, yend = 0),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  geom_segment(aes(x = 11, y = 99, xend = 0, yend = 99),
               col = "red", lty = 2,
               arrow=arrow(type = "closed", length = unit(0.25, "cm")))
```
]

.pull-right[

```{r, echo=FALSE}
tibble(
  Age = seq(10, 12, 0.25)
) %>%
  mutate(
    Prediction = 55 + (Age*4)
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

???
+ Our predictions are points which fall on our line (representing the model, as a function)
+ Here the arrows are showing how we can use the model to find a predicted value.
+ we find the value of the input on the x-axis (here 11), read up to the line, then across to the y-axis




---
# Predictions and data

.pull-left[

+ Consider the predictions when the children get a lot older...

{{content}}

]


.pull-right[
```{r, echo=FALSE}
tibble(
  Age = seq(216,300, 12)
) %>%
  mutate(
    Year = Age/12,
    Prediction = 55 + (Age*4),
    Prediction_M = Prediction/100
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

--
+ What do you think this would mean for our actual data?
{{content}}

--
+ Will the data fall on the line?
{{content}}


---
# How good is my model?
+ How might we judge how good our model is?

  1. Model is represented as a function
  
  2. We see that as a line (or surface if we have more things to consider)
  
  3. That yields predictions (or values we expect if our model is true)
  
  4. We can collect data
  
  5. If the predictions do not match the data (points deviate from our line), that says something about our model.



---
# Linear model
+ The linear model is the workhorse of statistics.

+ When using a linear model, we are typically trying to explain variation in an **outcome** (Y, dependent, response) variable, using one or more **predictor** (x, independent, explanatory) variable(s).


---
# Example

.pull-left[

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)

kable(test)
```

]

.pull-right[

**Simple data**

+ `student` = ID variable unique to each respondent

+ `hours` = the number of hours spent studying. This will be our predictor ( $x$ )

+ `score` = test score ( $y$ )

**Question: Do students who study more get higher scores on the test?**
]

---
# Scatterplot of our data

.pull-left[
```{r, echo=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  ylab("Test Score") +
  xlab("Hours Studied")

```
]

.pull-right[

{{content}}

]

--

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  stat_smooth(method="lm", se=FALSE, col="red") +
  ylab("Test Score") +
  xlab("Hours Studied")

```

{{content}}

???
+ we can visualize our data. We can see points moving bottom left to top right
+ so association looks positive
+ Now let's add a line that represents the best model

---
# Definition of the line
+ The line can be described by two values:

+ **Intercept**: the point where the line crosses $y$, and $x$ = 0

+ **Slope**: the gradient of the line, or rate of change

???
+ In our example, intercept = for someone who doesn't study, what score will they get?
+ Slope = for every hour of study, how much will my score change

---
# Intercept and slope

```{r, echo=FALSE, message=FALSE, warning=FALSE}

intercept <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 3, slope = .3) +
  geom_abline(intercept = 4, slope = .3) + 
  geom_abline(intercept = 5, slope = .3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Different intercepts, same slopes")

slope <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 4, slope = .3) +
  geom_abline(intercept = 4, slope = 0) + 
  geom_abline(intercept = 4, slope = -.3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Same intercepts, different slopes")

```

.pull-left[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
intercept

```

]

.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
slope

```

]

---
# How to find a line?
+ The line represents a model of our data.
  + In our example, the model that best characterizes the relationship between hours of study and test score.

+ In the scatterplot, the data is represented by points.

+ So a good line, is a line that is "close" to all points.


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$


+ $y_i$ = the outcome variable (e.g. `score`) 

+ $x_i$ = the predictor variable, (e.g. `hours`)

+ $\beta_0$ = intercept

+ $\beta_1$ = slope

+ $\epsilon_i$ = residual (we will come to this shortly)

where $\epsilon_i \sim N(0, \sigma)$ independently.
  + $\sigma$ = standard deviation (spread) of the errors
  + The standard deviation of the errors, $\sigma$, is constant


---
# Linear Model

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ **Why do we have $i$ in some places and not others?**


--

+ $i$ is a subscript to indicate that each participant has their own value.

+ So each participant has their own: 
    + score on the test ( $y_i$ )
    + number of hours studied ( $x_i$ ) and
    + residual term ( $\epsilon_i$ )

--
+ **What does it mean that the intercept ( $\beta_0$ ) and slope ( $\beta_1$ ) do not have the subscript $i$?**

--

+ It means there is one value for all observations.
    + Remember the model is for **all of our data**

---
# What is $\epsilon_i$?

.pull-left[
+ $\epsilon_i$, or the residual, is a measure of how well the model fits each data point.

+ It is the distance between the model line (on $y$-axis) and a data point.

+ $\epsilon_i$ is positive if the point is above the line (red in plot)

+ $\epsilon_i$ is negative if the point is below the line (blue in plot)

]


.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")


```

]

???
+ comment red = positive and bigger (longer arrow) model is worse
+ blue is negative, and smaller (shorter arrow) model is better
+ key point to link here is the importance of residuals for knowing how good the model is
+ Link to last lecture in that they are the variability 
+ that is the link into least squares


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2>Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# Principle of least squares

+ The numbers $\beta_0$ and $\beta_1$ are typically **unknown** and need to be estimated in order to fit a line through the point cloud.

+ We denote the "best" values as $\hat \beta_0$ and $\hat \beta_1$

+ The best fitting line is found using **least squares**
    + Minimizes the distances between the actual values of $y$ and the model-predicted values of $\hat y$
    + Specifically minimizes the sum of the *squared* deviations

---
# Principle of least squares

+ Actual value = $y_i$

+ Model-predicted value = $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$

+ Deviation or residual = $y_i - \hat y_i$

+ Minimize the **residual sum of squares**, $SS_{Residual}$, which is

$$SS_{Residual} = \sum_{i=1}^{n} [y_i - (\hat \beta_0 + \hat \beta_1 x_{i})]^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

---
# Data, predicted values and residuals

+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$ = the y-value on the line at specific values of $x$
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example, given you study for 4 hrs, what test score does our model predict you will get.

+ Residual = Difference between $y_i$ and $\hat{y}_i$. So;

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

???
+ these are important distinctions for understanding linear models
+ return to them a lot.


---
# Data, predicted values and residuals

.pull-left[

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ Squared distance of each point from the predicted value.
]

.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")

```

]



---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2>Part 3: Single continuous predictor = correlation</h2>
<h2 style="text-align: left;opacity:0.3;">Part 4: Single binary predictor = t-test</h2>

---
# `lm` in R
```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
```

```{r}
res <- lm(score ~ hours, data = test)
summary(res)
```

---
# Interpretation

+ **Slope is the number of units by which Y increases, on average, for a unit increase in X.**

--
    + Unit of Y = 1 point on the test
    + Unit of X = 1 hour of study
    
--

+ So, for every hour of study, test score increases on average by 1.055 points.

--

+ **Intercept is the expected value of Y when X is 0.**

--

    + X = 0 is a student who does not study.

--

+ So, a student who does no study would be expected to score 0.40 on the test.

???
+ So we know in a general sense what the intercept and slope are, but what do they mean with respect to our data and question?

---
# Note of caution on intercepts
+ In our example, 0 has a meaning.
    + It is a student who has studied for 0 hours.
  + But it is not always the case that 0 is meaningful.

+ Suppose our predictor variable was not hours of study, but age.

+ **A person of 0 age has a test score of 0.40.**

---
# Unstandardized vs standardized coefficients
- In this example, we have unstandardized $\hat \beta_1$.

+ We interpreted the slope as the change in $y$ units for a unit change in $x$
  + Where the unit is determined by how we have measured our variables.

+ However, sometimes we may want to represent our results in standard units.
  + If the scales of our variables are arbitrary.
  + If we want to compare the effects of variables on different scales.


---
# Standardized results
+ We can either...

+ Standardized coefficients: 

$$\hat{\beta_1^*} = \hat \beta_1 \frac{s_x}{s_y}$$

+ where;
  + $\hat{\beta_1^*}$ = standardized beta coefficient
  + $\hat \beta_1$ = unstandardized beta coefficient
  + $s_x$ = standard deviation of $x$
  + $s_y$ = standard deviation of $y$

---
# Standardizing the variables

+ Alternatively, for continuous variables, transforming both the IV and DV to $z$-scores (mean=0, SD=1) prior to fitting the model yields standardised betas.

+ $z$-score for $x$:

$$z_{x_i} = \frac{x_i - \bar{x}}{s_x}$$

+ and the $z$-score for $y$:

$$z_{y_i} = \frac{y_i - \bar{y}}{s_y}$$

+ That is, we divide the individual deviations from the mean by the standard deviation

---
# `lm()` using z-scores

```{r}
test <- test %>%
  mutate(
    z_score = scale(score, center = T, scale = T),
    z_hours = scale(hours, center = T, scale = T)
  )

res_z <- lm(z_score ~ z_hours, data = test)
round(summary(res_z)$coefficients, 3)
```


---
#  Interpreting standardized regression coefficients  

+ $b_0$ (intercept) = zero when all variables are standardized:

+ The interpretation of the coefficients becomes the increase in $y$ in standard deviation units for every standard deviation increase in $x$

+ So, in our example:

>**For every standard deviation increase in hours of study, test score increases by 0.72 standard deviations**

---
# Relationship to r
+ Standardized slope ( $\hat \beta_1^*$ ) = correlation coefficient ( $r$ ) for a linear model with a single continuous predictor.

+ In our example, $\hat \beta_{hours}^*$ = 0.72

```{r}
round(cor(test$hours, test$score),3)
```

+ $r$ is a standardized measure of linear association

+ $\hat \beta_1^*$ is a standardized measure of the linear slope.


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: What is the linear model?</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Best line </h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Single continuous predictor = correlation</h2>
<h2>Part 4: Single binary predictor = t-test</h2>

---
# Binary variable
+ Binary variable is a categorical variable with two levels.

+ Traditionally coded with a 0 and 1
  + Referred to as dummy coding
  + We will come back to this for categorical variables with 2+ levels

--

+ Why 0 and 1?
  + Quick version: It has some nice properties when it comes to interpretation.


---
# Extending our example

.pull-left[
+ Our in class example so far has used test scores and revision time for 10 students.

+ Let's say we collect this data on 150 students.

+ We also collected data on who they studied with;
  + 0 = alone
  + 1 = with others
  
+ So our variable `study` is a binary
]

.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}
df <- read_csv("./dapr2_lec07.csv") 
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
slice(df, 1:10)
```

]

---
#  LM with binary predictors 
+ Now we can ask the question:

  + **Do students who study with others score better than students who study alone?**

$$score_i = \beta_0 + \beta_1 study_{i} + \epsilon_i$$

---
# In `R`

```{r}
res2 <- lm(score ~ study, data = df)
summary(res2)
```


---
# Interpretation

.pull-left[
+ As before, the intercept $\hat \beta_0$ is the expected value of $y$ when $x=0$

+ What is $x=0$ here?
  + It is the students who study alone.

+ So what about $\hat \beta_1$?

+ **Look at the output on the right hand side.** 
  + What do you notice about the difference in averages?

]

.pull-right[
```{r warning=FALSE, message=FALSE}
df %>%
  group_by(., study) %>% #<<
  summarise(
    Average = round(mean(score),4) #<<
  )
```


]


---
# Interpretation
+ $\hat \beta_0$ = predicted expected value of $y$ when $x = 0$
  + Or, the mean of group coded 0 (those who study alone)
  
+ $\hat \beta_1$ = predicted difference between the means of the two groups.
  + Group 1 - Group 0 (Mean `score` for those who study with others - mean `score` of those who study alone)
  
+ Notice how this maps to our question. 
  + Do students who study with others score better than students who study alone?
  

---
#  Visualize the model

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(latex2exp)
gpM <- df %>%
  group_by(study) %>%
  summarise(
    score = mean(score)
  )

df %>%
  ggplot(., aes(x=factor(study), y=score, colour = study)) +
  geom_point(alpha=0.4) +
  labs(x = "\n Study", y = "Test Score \n") +
  ylim(0,10) +
  scale_x_discrete(labels = c("alone", "others")) +
  theme(legend.position = "none") +
  geom_jitter(width = .1, height = 0, alpha=0.4) +
  geom_errorbar(data = gpM, width=0.6, aes(ymax=..y..,ymin=..y..), size=1)+
  geom_segment(x=1.5, y=gpM[[1,2]], xend=1.5, yend=gpM[[2,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[1,2]], xend=1.52, yend=gpM[[1,2]], size =1, col="red") +
  geom_segment(x=1.48, y=gpM[[2,2]], xend=1.52, yend=gpM[[2,2]], size =1, col="red") +
  geom_text(x=1.55, y = 5.5, label = TeX('$\\hat{\\beta}_1$'), size=5, col = "red") +
  geom_text(x=0.65, y = gpM[[1,2]] , label = TeX('$\\hat{\\beta}_0$'), size=5, col = "red")

```


---
# Hold on... it's a t-test

```{r}
df %>%
  t.test(score ~ study, .)
```

???
Yup!


---
class: center, middle
# Thanks all!

