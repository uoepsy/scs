[
  {
    "objectID": "00-getting-started.html",
    "href": "00-getting-started.html",
    "title": "0: Getting started",
    "section": "",
    "text": "Suppose you have data, lots of data. Perhaps they are about penguins, such these three different species of penguins:\n\n\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\nIf you’re here it’s because you want to learn how to go from this:\n\n\n\n\n  \n\n\n\n\nto this:\n\n\n\n\n\n\nSummary of bill and flipper lengths by species\n  \n\nSpecies\n      Count\n      \n        Bill length (mm)\n      \n      \n        Flipper length (mm)\n      \n    \n\nM\n      SD\n      M\n      SD\n    \n\n\n\nAdelie\n152\n38.79\n2.66\n189.95\n6.54\n\n\nChinstrap\n68\n48.83\n3.34\n195.82\n7.13\n\n\nGentoo\n124\n47.50\n3.08\n217.19\n6.48\n\n\n\n\n\n\n\nor this:\n\n\n\n\n\n\n\n\n\nCollecting a large amount of data and looking at it in Excel or Numbers is not helpful for humans, it does not give us any insights or knowledge.\nKnowledge is obtained by creating suitable summaries and visual displays from the data."
  },
  {
    "objectID": "00-getting-started.html#prelude",
    "href": "00-getting-started.html#prelude",
    "title": "0: Getting started",
    "section": "",
    "text": "Suppose you have data, lots of data. Perhaps they are about penguins, such these three different species of penguins:\n\n\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\nIf you’re here it’s because you want to learn how to go from this:\n\n\n\n\n  \n\n\n\n\nto this:\n\n\n\n\n\n\nSummary of bill and flipper lengths by species\n  \n\nSpecies\n      Count\n      \n        Bill length (mm)\n      \n      \n        Flipper length (mm)\n      \n    \n\nM\n      SD\n      M\n      SD\n    \n\n\n\nAdelie\n152\n38.79\n2.66\n189.95\n6.54\n\n\nChinstrap\n68\n48.83\n3.34\n195.82\n7.13\n\n\nGentoo\n124\n47.50\n3.08\n217.19\n6.48\n\n\n\n\n\n\n\nor this:\n\n\n\n\n\n\n\n\n\nCollecting a large amount of data and looking at it in Excel or Numbers is not helpful for humans, it does not give us any insights or knowledge.\nKnowledge is obtained by creating suitable summaries and visual displays from the data."
  },
  {
    "objectID": "00-getting-started.html#what-you-need",
    "href": "00-getting-started.html#what-you-need",
    "title": "0: Getting started",
    "section": "What you need",
    "text": "What you need\nTo succeed in this bootcamp you will only need:\n\n\na laptop\n\nIf you are using a Chromebook, please contact us via email.\n\n\n\nactive learning\n\nJust reading the material won’t be enough, you need to type along the code and get familiar with errors.\n\n\n\nwillingness to learn\n\nIf you approach the material with an inquisitive attitude, it will be easier to learn."
  },
  {
    "objectID": "00-getting-started.html#r",
    "href": "00-getting-started.html#r",
    "title": "0: Getting started",
    "section": "R",
    "text": "R\nWhat is R?\nR is a programming language: an actual language that a computer can understand. The purpose of a programming language is to instruct the computer to do some boring and long computations on our behalf.\nWhen you learn to program you are in fact learning a new language, just like English, Italian, and so on. The only difference is that, since we will be communicating with a machine, the language itself needs to be unambiguous, concise, and hence very limited in its grammar and scope. Basically, a programming language follows a very strict set of rules. The computer will do exactly what you type. It will not try to understand what you want it to do and, if you make a language error, the computer will not fix it, but it will just execute exactly what you said.\nIf you commit an error, there are two possible outcomes:\n\nThe computation goes ahead without any sign of errors or messages. This is the most worrying type of error as it’s hard to catch. You will get a result for your computation, but it may make no sense.\nThe computer will tell you that what you’re asking to do doesn’t make sense. Easier to fix!\n\nThe programming language that you will learn is called R. It also has a very fancy logo:\n\n\n\n\n\n\n\n\nThe code you type using the R programming language then will need to be converted to lower level instructions for the computer, such as “store this number into memory location with a specific address”. This is done by the interpreter which, is also called R. So R is both the programming language and the interpreter telling the computer what to do with your commands.\nHow does R look? Exactly as the picture below. It comes into a window called the Console, which is where any R code you type there will be executed.\n\n\n\n\n\n\n\n\nInstalling/updating R\nClick the section that applies to your specific PC.\n\n\nInstalling on Windows\nInstalling on macOS\nChromebooks\n\n\n\n\nIf you are updating R, uninstall all previous R or RTools programs you have installed in your PC.\nInstall RTools\n\nInstall R\n\n\n\n\n\nIf you are updating R, uninstall all previous R installations you have by moving the R icon from the Applications folder to the Bin.\nInstall XQuartz\nInstall R. Click the release that has title R-Number.Number.Number.pkg (for example R-4.3.0.pkg but this will change in the future).\n\n\n\nPlease contact us via email."
  },
  {
    "objectID": "00-getting-started.html#rstudio",
    "href": "00-getting-started.html#rstudio",
    "title": "0: Getting started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\n\n\nWhat is RStudio?\nRStudio is a nicer interface to R. It is simply a wrapper around R that combines the R Console, a text editor, a file explorer, a help panel, and a graphics panel to see all your pictures.\nIn summary:\n\nR is the engine\nRStudio is the dashboard\n\n\nSource: www.moderndive.com\n\n\nLet’s see how RStudio looks:\n\n\n\n\n\n\n\n\nIt has four panels or panes, described below. You can customise the appearance of the panes by clicking in the menu View -&gt; Panes -&gt; Pane Layout.\n\nR Console. The R console (bottom left) is where the code gets executed. If you type a command there and press enter, it will generate results/output. Writing all the code here is not handy however if you need to do lots of computations, and the code you write will not be saved and you will not be able to re-run the same steps on another day.\nCode Editor. It is better to type code in a special file, an R script, where you can save the code so that you can continue your work on another day. R scripts are opened in the code editor (top left). This is where you write your files with R code. Then, to run each line of code you wrote, you place your cursor at the end of each line and press Control+Enter (Windows) or Command+Enter (macOS). This way, the code get then sent to the R console to actually do the computation and display the result.Note. If you cannot see the editor, you have to open a new file. From the menu click “File” -&gt; “New file” -&gt; “R Script”\n\n\nEnvironment. The environment shows the things you have created, for example data.\nPlots and Files. The plots and files panel displays any plots you create and, if you click the files tab, it has a file explorer for you to find files and data stored in excel or similar. There is also a Help tab here, which is where you get help for R code.\nInstalling/updating RStudio\n\nIf you have a previous version of RStudio already installed, uninstall it (if on Windows), or move it to the bin (if on a macOS).\nAt this link, click the button under “2: Install RStudio”.\nOpen RStudio, type the following in the console, and press Enter after each line\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "00-getting-started.html#update-regularly",
    "href": "00-getting-started.html#update-regularly",
    "title": "0: Getting started",
    "section": "Update regularly",
    "text": "Update regularly\nIt is important that you keep your R and RStudio installations up-to-date. If you don’t you will encounter many errors at some point.\n\nTry to update R at least twice a year.\nTry to update RStudio at least once a year."
  },
  {
    "objectID": "00-getting-started.html#postlude",
    "href": "00-getting-started.html#postlude",
    "title": "0: Getting started",
    "section": "Postlude",
    "text": "Postlude\nWhenever we say “open R” or “using R”, what we really mean is “open RStudio” or “using RStudio”.\nYou should always using RStudio to write code. So, even if you will have two applications in your computer: R and RStudio, you will only need to open RStudio for your day-to-day work."
  },
  {
    "objectID": "00-getting-started.html#recommended-activities",
    "href": "00-getting-started.html#recommended-activities",
    "title": "0: Getting started",
    "section": "Recommended activities",
    "text": "Recommended activities\n\nInstructions: Installing R and RStudio"
  },
  {
    "objectID": "00-getting-started.html#recommended-readings",
    "href": "00-getting-started.html#recommended-readings",
    "title": "0: Getting started",
    "section": "Recommended readings",
    "text": "Recommended readings\nFor further information, check to the following:\n\nModerndive, Chapter 1\nStat 545, Chapter 1"
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "1: R basics",
    "section": "",
    "text": "Be organised!\nWhen writing code, it is fundamental to be organised. Create a different folder for each course or project that can be considered a self-contained box.\nIn that folder you will store:\n\ncode files (called R scripts)\nsubfolders with other items within them; for example a subfolder called “data” containing the data to analyse\n\nIn your computer, create a folder called “r-bootcamp”, in which you will do all the work for this course. Go inside the r-bootcamp folder, and create a subfolder called data.\nNow go ahead and open RStudio!"
  },
  {
    "objectID": "01-basics.html#prelude",
    "href": "01-basics.html#prelude",
    "title": "1: R basics",
    "section": "",
    "text": "Be organised!\nWhen writing code, it is fundamental to be organised. Create a different folder for each course or project that can be considered a self-contained box.\nIn that folder you will store:\n\ncode files (called R scripts)\nsubfolders with other items within them; for example a subfolder called “data” containing the data to analyse\n\nIn your computer, create a folder called “r-bootcamp”, in which you will do all the work for this course. Go inside the r-bootcamp folder, and create a subfolder called data.\nNow go ahead and open RStudio!"
  },
  {
    "objectID": "01-basics.html#r-scripts",
    "href": "01-basics.html#r-scripts",
    "title": "1: R basics",
    "section": "R scripts",
    "text": "R scripts\nTyping code in the console is handy because it is interactive and returns a result immediately, but writing in the console is only recommended for very quick calculations. Longer code should be written in special code files, so that you can save all your work and get back to it another day. If you typed all your code in the console, you would lose it and you won’t be able to build on top of what you did on previous days of work. You would have to always start from scratch!\nWe write code in R scripts. These are simply text files with a file name ending with extension .R\nIn an R script you will write your code and, when you’re happy with it, you can send it to the console for execution by either highlighting it and clicking Run on the top right of the editor, or placing your cursor at the end of each line and pressing Control+Enter (on Windows) or Command+Enter (on macOS).\nOnce you opened RStudio, you can create a new R script file using any of these methods.\n1. RStudio menu\nFrom the RStudio menu, click File -&gt; New File -&gt; R script\n2. Editor menu\nOn the Editor panel, click the first icon on the top-left, then select script.\n\nGive a useful name and save your R script!\nYou should give a useful name to your R script, otherwise if in a month time you will try to find some code you wrote in the past, you won’t find it.\nIn summary, an R script\n\nis a text file in which you will type your R code before sending it to the console (which will run it).\nhas a file name ending with .R such as lesson1.R"
  },
  {
    "objectID": "01-basics.html#r-is-a-calculator",
    "href": "01-basics.html#r-is-a-calculator",
    "title": "1: R basics",
    "section": "R is a calculator",
    "text": "R is a calculator\nIt’s time to throw away that table calculator you perhaps had in high school. R is smarter and lets you do bigger calculations much faster!\nIn the R script lesson1.R type the following.\n\n2 + 2\n3 * 4\n6 / 2\n2^3\n\nThen, place your cursor at the end of the first line and press Ctrl + Enter (on Windows) or Cmd + Enter (on macOS) to send the code to the console to be executed. Each time you press Ctrl/Cmd + Enter, the cursor will move to the next line for you!\nYou should see the following output in the console:\n\n2 + 2\n\n[1] 4\n\n3 * 4\n\n[1] 12\n\n6 / 2\n\n[1] 3\n\n2^3\n\n[1] 8\n\n\nWe can also add comments to explain what code does. Comments are helpful when the computation is complex and hard to understand, so in the following example it’s not really needed, but it’s just meant to show you how to add comments.\nIn an R script a comment is provided by the hash symbol, #. Anything written after an hash symbol will not be run by the R console as it will be interpreted as a reminder/explanatory message for the reader.\n\n# Power\n2^3\n\n[1] 8\n\n\n\nsqrt(9) # anything after the hash gets ignored by R\n\n[1] 3\n\n\nR ignores spaces, and the following will all return the same result:\n\n2 + 2\n2+2\n2 +2\n2+ 2\n\nHowever, we added a space before and after arithmetic operators such as +, -, * to make it easier to read. Typically, we do not add spaces around powers.\n\n\n\n\n\n\n\nCheckpoint\n\n\n\nYour R script should look something like this by now:\n\n\n\n\nLet’s consider again this operation:\n\n3 * 4\n\n[1] 12\n\n\nIn the above example, [1] indicates that the output is the first element of a vector of length 1 (we will discuss vectors later in this course).\nIt is important for you to try the extra examples that follow. Remember that to successfully learn programming you need to actually type the code.\n\n5 * (4 - 2)\n\n[1] 10\n\nsqrt(4)\n\n[1] 2\n\nsqrt(9)\n\n[1] 3\n\nabs(-5)\n\n[1] 5\n\nabs(3)\n\n[1] 3\n\n\nIn the examples above, sqrt() and abs() are examples of so-called functions. Functions take some input (or multiple inputs), do some computation with those inputs, and return a result or output.\nHere, the function sqrt() takes a nonnegative number and returns its square root. The abs() function takes a number as input and, if the input is nonnegative, it returns the same number; while if the input is negative, it makes it positive.\nR has many other functions, and users can even create their own functions. We will see more on this later on."
  },
  {
    "objectID": "01-basics.html#objects",
    "href": "01-basics.html#objects",
    "title": "1: R basics",
    "section": "Objects",
    "text": "Objects\nR can store information by creating an object with a name of our choice.\nWe use the assignment operator &lt;- to assign some value to an object:\nname &lt;- computation\nTypically, it is recommended to use a name that makes sense for the value that is being stored.\nThe name of an object must follow some rules:\n\nIt cannot begin with a number (but it can contain numbers)\nIt cannot contain spaces\nIt cannot use special characters such as $, %, +, -, *, /, ^ which have a special meaning in R.\n\nThe Environment panel (top-right of RStudio) will display all the objects that you have created in the current R session. Once you have create an object, you can simply refer to it by name.\nTo see the actual value, simply call the name of the object:\n\nx &lt;- log(2^3)\nx\n\n[1] 2.079442\n\n\nThe arrow means: take what’s on the right and assign it to the name on the left.\nNote: If you do not wish to view the result, as sometimes it may print too much stuff, simply do not call the name of the variable.\n\nx &lt;- log(2^3)\n\nYou can then use the object to do subsequent computations:\n\nx * 3\n\n[1] 6.238325\n\n\nNote that if you assign a different value to the same object name, then the value of the object will be changed. So be careful to not overwrite objects that you plan to use later!\n\nx &lt;- log(2^3)\nx\n\n[1] 2.079442\n\nx &lt;- 1000 - 500\nx\n\n[1] 500\n\n\nAnother thing to pay attention to is that R is case sensitive. Hello is not the same as either hello or HELLO. As a consequence, we receive an error in the R console when we type X rather than x, which is defined above.\n\nX\n\nError in eval(expr, envir, enclos): object 'X' not found\n\n\nGetting familiar with programming errors or bugs is part of learning to program. Do not panic when you get an error, it doesn’t mean the PC is exploding. You can make as many errors as you want and fix them iteratively.\nIn programming you actually learn from your errors. The tricky part is figuring out how to fix them, and you will learn this with time. Here, the error message tells us that the object X does not exist. We can see the list of existing objects in the Environment tab in the upper-right window, where we will find that the correct object is x.\nIt is also possible to view the list of created objects in the console by using the list objects function ls():\n\nls()\n\n[1] \"has_annotations\" \"x\"              \n\n\nLet’s create more objects!\n\nobj1 &lt;- 50\nobj2 &lt;- 10\nobj3 &lt;- obj1 / obj2\nobj3\n\n[1] 5\n\n\nLook at the objects created:\n\nls()\n\n[1] \"has_annotations\" \"obj1\"            \"obj2\"            \"obj3\"           \n[5] \"x\"              \n\n\nRemove one or more object:\n\nrm(obj1, obj2)\nls()\n\n[1] \"has_annotations\" \"obj3\"            \"x\"              \n\n\nOr, to remove the entire list of created objects:\n\nrm(list = ls())\n\nUntil now we have only assigned numbers to an object, but R can also store other types of values as objects. For example, you can store a string of characters by enclosing it by double quotation marks or single quotation marks. Don’t mix and match!\n\nedinburgh &lt;- \"university\"\nedinburgh\n\n[1] \"university\"\n\n\nIn character strings spacing is allowed:\n\nedinburgh &lt;- \"department of psychology\"\nedinburgh\n\n[1] \"department of psychology\"\n\n\nAlso note that R treats numbers a characters if you tell it to do so:\n\nresult &lt;- \"8\"\nresult\n\n[1] \"8\"\n\n\nHowever, you cannot perform arithmetic operations on characters as those are defined for numbers only. For example, trying to do addition or taking the square root of a character string will return an error:\n\nresult + 2\n\nError in result + 2: non-numeric argument to binary operator\n\nsqrt(result)\n\nError in sqrt(result): non-numeric argument to mathematical function\n\n\nR knows what kind of operations make sense on each object by assigning each object to a specific class. For example, numbers such as 2, 8, 1000, 3.2 are of class numeric, while character strings such as the words \"aja\", \"martin\" are of class character. The function class() will tell you what class is your object.\n\nresult\n\n[1] \"8\"\n\nclass(result)\n\n[1] \"character\"\n\nclass(3.6)\n\n[1] \"numeric\"\n\nx &lt;- 5\nclass(x)\n\n[1] \"numeric\"\n\nclass(\"josiah\")\n\n[1] \"character\"\n\nclass(\"5\")\n\n[1] \"character\"\n\nclass(sqrt)\n\n[1] \"function\"\n\n\nThere are many other classes in R, and you will see more as you proceed through the course.\n\n\n\n\n\n\n\nYou can also use = for assignment\n\n\n\nTo assign a value to an object, you can use either the arrow &lt;- or the equal sign =. However, the R community prefers the use of the former.\n\ny &lt;- sqrt(25)\ny\n\n[1] 5\n\nz = sqrt(25)\nz\n\n[1] 5"
  },
  {
    "objectID": "01-basics.html#conditions",
    "href": "01-basics.html#conditions",
    "title": "1: R basics",
    "section": "Conditions",
    "text": "Conditions\nHow do we check whether or not some condition holds? For example, is 2^3 equal to 8? The answer can be either TRUE or FALSE. That is what R will print.\nTo check whether a something equals something else, we use double equal signs == (to distinguish with the single equal used to assign a value to a variable):\n\n2^3 == 8\n\n[1] TRUE\n\n2^3 == 12\n\n[1] FALSE\n\n\nTo check whether a value is smaller than another:\n\n3 &lt; 8\n\n[1] TRUE\n\n3 &gt; 8\n\n[1] FALSE\n\n\n\n\nExercise\nAnswer\n\n\n\nWhat does each of the following check for?\n\n4 &lt;= 4\n4 &lt;= 5\n4 &gt;= 4\n4 &gt;= 5\n\n\n\nWe use &lt;= to test whether the LHS is less than or equal to the RHS:\n\n4 &lt;= 4\n\n[1] TRUE\n\n4 &lt;= 5\n\n[1] TRUE\n\n\nWe use &gt;= to test whether the LHS is greater than or equal to the RHS:\n\n4 &gt;= 4\n\n[1] TRUE\n\n4 &gt;= 5\n\n[1] FALSE\n\n\n\n\n\n\n\nExercise\nAnswer\n\n\n\nLet:\n\nx &lt;- 18\ny &lt;- 10\n\n\nIs x &lt; y?\nIs log(y) &lt; x?\n\n\n\n\nIs x &lt; y?\n\n\nx &lt; y\n\n[1] FALSE\n\n\n\nIs log(y) &lt; x?\n\n\nlog(x) &lt; y\n\n[1] TRUE\n\n\n\n\n\nYou can assign conditions to an object, say\n\nout &lt;- 3 &gt; 2\nout\n\n[1] TRUE\n\n\nThe out object is an object of class logical, or a logical object in short — which is exactly what the values TRUE and FALSE are, logical values.\n\nclass(out)\n\n[1] \"logical\"\n\nclass(TRUE)\n\n[1] \"logical\"\n\nclass(FALSE)\n\n[1] \"logical\"\n\n\nIf you try to perform arithmetic operations on logicals, TRUE is converted to 1 and FALSE to 0.\n\nTRUE + 2\n\n[1] 3\n\nFALSE + 2\n\n[1] 2\n\n\nIn general, to convert logical to numeric you can either multiply the values by 1, or use the as.numeric() function\n\n(5 &gt; 6) * 1\n\n[1] 0\n\n(5 &lt; 6) * 1\n\n[1] 1\n\nas.numeric(TRUE)\n\n[1] 1\n\n\nTo convert instead 1/0 to logical TRUE/FALSE, you use the as.logical() function:\n\nas.logical(1)\n\n[1] TRUE\n\nas.logical(0)\n\n[1] FALSE"
  },
  {
    "objectID": "01-basics.html#parentheses",
    "href": "01-basics.html#parentheses",
    "title": "1: R basics",
    "section": "Parentheses",
    "text": "Parentheses\nSometimes we want to do some computations before others, and to give order we use the parentheses.\nIf you do not use parentheses, the order of precedence is\n\n^\n* or /, whatever appears first\n+ or -, whatever appears first\n\n\n(5 + 3) * 2\n\n[1] 16\n\n5 + 3 * 2\n\n[1] 11\n\n5 + (3 * 2)\n\n[1] 11\n\n\n\n(5 + 6) / 2\n\n[1] 5.5\n\n5 + 6 / 2\n\n[1] 8\n\n5 + (6 / 2)\n\n[1] 8"
  },
  {
    "objectID": "01-basics.html#vectors",
    "href": "01-basics.html#vectors",
    "title": "1: R basics",
    "section": "Vectors",
    "text": "Vectors\nWhat if we wanted to store multiple values? For example, you may want to do a computation involving multiple data values.\nThis section will present the simplest and most inefficient way of entering data into R, which involves creating a vector.\nA vector is an ordered listing of entries. If all entries are numbers, we have a numeric vector. If all entries are character strings, we have a character vector.\nThe following table reports the world population estimates (in thousands) for the past few decades.\n\n\n\n\n\n\nWorld Population Estimates.\n  \n\n\n\n\nYear\n      \nWorld population(thousands)\n    \n\n\n1950\n2536431\n\n\n1960\n3034950\n\n\n1970\n3700437\n\n\n1980\n4458003\n\n\n1990\n5327231\n\n\n2000\n6143494\n\n\n2010\n6956824\n\n\n2020\n7794799\n\n\n\n\nSource: United Nations, Department of Economic and Social Affairs, Population Division (2019). World Population Prospects 2019, Online Edition. Rev. 1.\n\n    \n\n\n\n\nWe can now enter the world population estimates into R by creating a numeric vector object. To do so, we use the combine function c() which combines together different entries. The entries should be provided to the c() function separated by commas:\n\nworld_pop &lt;- c(2536431, 3034950, 3700437, 4458003, \n               5327231, 6143494, 6956824, 7794799)\nworld_pop\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\n\nNote that the combine function can also be used to concatenate two separate vectors:\n\npop_first_half &lt;- c(2536431, 3034950, 3700437, 4458003)\npop_second_half &lt;- c(5327231, 6143494, 6956824, 7794799)\npop_all &lt;- c(pop_first_half, pop_second_half)\npop_all\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\n\nWe can check the number of items in the world_pop vector with the length() function:\n\nlength(world_pop)\n\n[1] 8\n\n\nWe can compute the average world population for the period 1950-2020 with the function:\n\nmean(world_pop)\n\n[1] 4994021\n\n\nClearly, you could have done it by hand, summing all grades up and dividing the sum by 8. However, do you have the patience to do this for thousands of numbers? No, this is why we use R.\nTo access specific entries of a vector we use square brackets and inside those we specify which entry we want. This is called indexing.\n\nworld_pop[1]  # first entry, i.e. for year 1950\n\n[1] 2536431\n\nworld_pop[3]\n\n[1] 3700437\n\nworld_pop[8]  # last entry, i.e. for year 2020\n\n[1] 7794799\n\n\nMultiple elements can be extracted via a vector of indices within square brackets. If an index is negative, the corresponding entry is removed from the vector. Note that none of these operations change the original vector, because we have not overwritten the object via an assignment.\n\nworld_pop[c(2, 5)]\n\n[1] 3034950 5327231\n\nworld_pop[c(5, 2)]\n\n[1] 5327231 3034950\n\nworld_pop[-2]\n\n[1] 2536431 3700437 4458003 5327231 6143494 6956824 7794799\n\n\nSince the vector is a numeric vector, we can perform arithmetic operations on it. For example, we may wish to express the world population in millions rather than thousands:\n\npop_millions &lt;- world_pop / 1000\npop_millions\n\n[1] 2536.431 3034.950 3700.437 4458.003 5327.231 6143.494 6956.824 7794.799\n\n\nAlternatively, we could be interested in the rate of increase relative to the year 1950 (which is the first entry in the vector):\n\npop_rate &lt;- world_pop / world_pop[1]\npop_rate\n\n[1] 1.000000 1.196543 1.458915 1.757589 2.100286 2.422102 2.742761 3.073137\n\n\nYou can also perform calculations using multiple vectors. In such case the operation is performed element by element.\nSay you were interested in the population increase for each decade. This corresponds to the increase in a decade divided by the population at the start of the decade.\nFor example is the population was 100 thousands in a year, and became 120 thousands after 10 years, the percentage increase would be 20%.\nTo compute the percentage increase we must obtain a vector without the first decade, and another vector without the last decade. Then we must subtract the second vector from the first, such that the first entry would then be the difference in population between the years 1960 and 1950.\n\nworld_pop[-1]\n\n[1] 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\nworld_pop[-8]\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824\n\npop_increase &lt;- (world_pop[-1] - world_pop[-8])\nperc_increase &lt;- 100 * (pop_increase / world_pop[-8])\nperc_increase\n\n[1] 19.65435 21.92745 20.47234 19.49815 15.32246 13.23888 12.04537\n\n\nFinally, you can replace values in a vector by combining indexing and assignment. Let’s replace the first three entries with their rounded values:\n\nperc_increase[c(1, 2, 3)] &lt;- c(20, 22, 20)\nperc_increase\n\n[1] 20.00000 22.00000 20.00000 19.49815 15.32246 13.23888 12.04537"
  },
  {
    "objectID": "01-basics.html#logical-indexing",
    "href": "01-basics.html#logical-indexing",
    "title": "1: R basics",
    "section": "Logical indexing",
    "text": "Logical indexing\nHow would you keep all population estimates greater than the average?\n\nworld_pop\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\nmean_pop &lt;- mean(world_pop)\nmean_pop\n\n[1] 4994021\n\n\nFirst, construct a logical vector by applying a condition to each entry of the vector:\n\nworld_pop &gt; mean_pop\n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nThen, simply provide that logical vector as indexing. Only the elements with a TRUE index will be returned\n\nworld_pop[world_pop &gt; mean_pop]\n\n[1] 5327231 6143494 6956824 7794799\n\n\nYou can also create an indexing object and use that:\n\nidx &lt;- world_pop &gt; mean_pop\nworld_pop[idx]\n\n[1] 5327231 6143494 6956824 7794799"
  },
  {
    "objectID": "01-basics.html#not-avaible-i.e.-missing-values",
    "href": "01-basics.html#not-avaible-i.e.-missing-values",
    "title": "1: R basics",
    "section": "Not avaible (i.e. missing) values",
    "text": "Not avaible (i.e. missing) values\nR can store missing values too. These are simply not available data, or NA in short, and they are special. As you don’t have the value, the result of a computation involving NA values will return NA.\n\nNA\n\n[1] NA\n\nNA + 2\n\n[1] NA\n\n10 * NA\n\n[1] NA\n\nx &lt;- c(NA, 2, 10)\nx + 5\n\n[1] NA  7 15\n\n\nImagine we wanted to store the world population up to year 2030. We don’t know the last value yet, so we would have\n\nyear_new &lt;- seq(1950, 2030, by = 10)\nyear_new\n\n[1] 1950 1960 1970 1980 1990 2000 2010 2020 2030\n\n\nThe function seq(from, to, by) creates a sequence of values starting at from, ending at to, in steps specified by by.\n\nworld_pop_new &lt;- c(2536431, 3034950, 3700437, 4458003, 5327231, 6143494, 6956824, 7794799, NA)\nworld_pop_new\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799      NA\n\n\nIf we want to divide the vector by 1000 to express the population in millions, rather than thousands, you will see that the last entry of the vector will stay as NA.\n\nworld_pop_new / 1000\n\n[1] 2536.431 3034.950 3700.437 4458.003 5327.231 6143.494 6956.824 7794.799\n[9]       NA\n\n\nWe can find which entries are NA using the function is.na()\n\nis.na(world_pop_new)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nAnd use this vector for logical indexing. Remember the values returned are only those for which the index is TRUE:\n\nworld_pop_new[is.na(world_pop_new)]\n\n[1] NA\n\n\nNegation, i.e. the opposite is obtained with the exclamation mark !, which means “not”\n\nworld_pop_new[!is.na(world_pop_new)]\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\n\nIn general, the opposite of TRUE is FALSE, and vice versa:\n\n!FALSE\n\n[1] TRUE\n\nx &lt;- TRUE\nx\n\n[1] TRUE\n\n!x\n\n[1] FALSE\n\n(2 == 3)\n\n[1] FALSE\n\n!(2 == 3)\n\n[1] TRUE"
  },
  {
    "objectID": "01-basics.html#functions",
    "href": "01-basics.html#functions",
    "title": "1: R basics",
    "section": "Functions",
    "text": "Functions\nFunctions are pieces of code that take one or more inputs (also called arguments), do some computation on those, and return an output. In the code sqrt(25), the function is sqrt and the input (or argument) is 25.\nWe have already seen some functions: sqrt(), mean(), class(), and c().\nSome useful functions for summarising numeric vectors include length() for the length of the vector, min() for the minimum value, max() for the maximum value, range() shows the range of data, while mean() and sum() show the mean and sum of the data respectively.\n\nlength(world_pop)\n\n[1] 8\n\nmin(world_pop)\n\n[1] 2536431\n\nmax(world_pop)\n\n[1] 7794799\n\nrange(world_pop)\n\n[1] 2536431 7794799\n\nmean(world_pop)\n\n[1] 4994021\n\n\nWe can verify that the average grade is the same as the sum of the numbers divided by the number of entries:\n\nsum(world_pop) / length(world_pop)\n\n[1] 4994021\n\n\nAs you saw above, the sum() function takes a numeric vector and sums up all its values.\nUntil now we have only seen functions that took a single input. However, a function can take more than one input. The syntax then becomes\nfunction_name(input1, input2)\nAn example of this is the seq(A, B, C) function which, as the name says, creates a sequence of values from A to B in steps of C.\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1, 10, 3)\n\n[1]  1  4  7 10\n\n\nAs you can see above, the first input tells you the “from”, the second the “to”, the last give you the “step”. If you don’t provide “step”, this is 1 by default.\nYou can also see that by switching the order of the inputs, you get a different result\n\nseq(10, 1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nNote. A shortcut for a sequence from A to B in steps of 1 is A:B\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nHence, when a function takes multiple inputs, it is typical to provide the names of the inputs, resulting in the same output:\n\nseq(from = 1950, to = 2020, by = 10)\n\n[1] 1950 1960 1970 1980 1990 2000 2010 2020\n\nseq(to = 2020, by = 10, from = 1950)\n\n[1] 1950 1960 1970 1980 1990 2000 2010 2020\n\n\nWe can write our own function that computes the mean:\n\nmy_mean &lt;- function(data) {\n  m &lt;- sum(data) / length(data)\n  return(m)\n}\n\nHere, we have created a function called my_mean, which takes one input, data, and does some computation on it. It creates an object m equal to the sum of the data divided by the length, and returns it.\nWe can then call our function on the world population data\n\nmy_mean(world_pop)\n\n[1] 4994021\n\n# is it equal to the built-in function?\nmean(world_pop)\n\n[1] 4994021\n\n\nWhat type of object is my_mean?\n\nclass(my_mean)\n\n[1] \"function\"\n\n\nClearly, it’s an object of class function."
  },
  {
    "objectID": "01-basics.html#factors",
    "href": "01-basics.html#factors",
    "title": "1: R basics",
    "section": "Factors",
    "text": "Factors\nSo far we have seen different classes of objects:\n\nnumeric\ncharacter\nfunction\n\nHowever, there is another important class: factors.\nA factor object stores data representing fixed and pre-specified categories. Such data arise for example in clinical trials, where participants are given either a placebo, a treatment already in use, or a new alternative treatment — in short “Placebo”, “DrugA”, or “DrugB”. Another example would be in questionnaires, where an user is asked to rate their experience as “excellent”, “very good”, “good”, “neutral”, “poor”, “very poor”.\nClearly, in the clinical trial it would be an error if one of the data said “DrugE” as this is not identified in the problem. Similarly, in the questionnaire example, it would be an error if a data value said “peppapig”.\nTo protect ourselves from wrong entries and to tell R that some data can only take a limited set of values, we use factors.\n\nclinical &lt;- factor(c('A', 'B', 'A', 'Placebo','A', 'Placebo', 'B', 'Placebo'),\n                   levels = c('Placebo', 'A', 'B'))\nclinical\n\n[1] A       B       A       Placebo A       Placebo B       Placebo\nLevels: Placebo A B\n\n\nThe distinct categories that the data can take are called levels of the factors. You can inspect the factor levels with the levels() function:\n\nlevels(clinical)\n\n[1] \"Placebo\" \"A\"       \"B\"      \n\n\n\nlength(clinical) # we have 8 elements in the vector\n\n[1] 8\n\nclass(clinical) # factor vector\n\n[1] \"factor\"\n\n\nWhat if we made an error when inputting the data and the 3rd participants had actually taken drug B? We can fix it as follows:\n\nclinical[3] &lt;- 'B'\nclinical\n\n[1] A       B       B       Placebo A       Placebo B       Placebo\nLevels: Placebo A B\n\n\nNow, say you made a typing error and wrote lowercase b (note that R distinguishes between uppercase and lowercase, they are not the same thing!), you would get a  (= not available) value because “b” is not among the pre-specified values. This safeguards you from potential errors, for example if you tried to write anything else, like drug C (which didn’t exist) or anything else, e.g. “peppapig”:\n\nclinical[3] &lt;- 'b'\n\nWarning in `[&lt;-.factor`(`*tmp*`, 3, value = \"b\"): invalid factor level, NA\ngenerated\n\nclinical\n\n[1] A       B       &lt;NA&gt;    Placebo A       Placebo B       Placebo\nLevels: Placebo A B\n\nclinical[3] &lt;- 'peppapig'\n\nWarning in `[&lt;-.factor`(`*tmp*`, 3, value = \"peppapig\"): invalid factor level,\nNA generated\n\nclinical\n\n[1] A       B       &lt;NA&gt;    Placebo A       Placebo B       Placebo\nLevels: Placebo A B\n\n\nRenaming a level of a factor\n\n\nExercise\nAnswer\n\n\n\nRename the values “Placebo” to “P”.\nHint: look at the help for the factor() function. The argument labels might be helpful.\n\nclinical &lt;- factor(c('A', 'B', 'A', 'Placebo','A', 'Placebo', 'B', 'Placebo'),\n                   levels = c('Placebo', 'A', 'B'),\n                   labels = ...)\nclinical\n\n\n\n\nclinical &lt;- factor(c('A', 'B', 'A', 'Placebo','A', 'Placebo', 'B', 'Placebo'),\n                   levels = c('Placebo', 'A', 'B'),\n                   labels = c('P', 'A', 'B'))\nclinical\n\n[1] A B A P A P B P\nLevels: P A B\n\n\n\n\n\nCollapsing two levels into a single one\n\n\nExercise\nAnswer\n\n\n\nRelabel drug A and B to just be “Treatment”. In other words, the factor should just say whether the participant had a placebo or was treated.\n\nclinical &lt;- factor(c('A', 'B', 'A', 'Placebo','A', 'Placebo', 'B', 'Placebo'),\n                   levels = c('Placebo', 'A', 'B'),\n                   labels = ...)\nclinical\n\n\n\n\nclinical &lt;- factor(c('A', 'B', 'A', 'Placebo','A', 'Placebo', 'B', 'Placebo'),\n                   levels = c('Placebo', 'A', 'B'),\n                   labels = c('Placebo', 'Treatment', 'Treatment'))\nclinical\n\n[1] Treatment Treatment Treatment Placebo   Treatment Placebo   Treatment\n[8] Placebo  \nLevels: Placebo Treatment"
  },
  {
    "objectID": "01-basics.html#recommended-activities",
    "href": "01-basics.html#recommended-activities",
    "title": "1: R basics",
    "section": "Recommended activities",
    "text": "Recommended activities\n\n1.1 Programming basics\n1.2 Data visualisation basics"
  },
  {
    "objectID": "01-basics.html#recommended-readings",
    "href": "01-basics.html#recommended-readings",
    "title": "1: R basics",
    "section": "Recommended readings",
    "text": "Recommended readings\nFor further information, check to the following:\n\nStat 545, Chapter 2"
  },
  {
    "objectID": "02-data.html",
    "href": "02-data.html",
    "title": "2: Working with data",
    "section": "",
    "text": "Your R installation comes pre-packed with lots of functionality. Just by opening R you are able to perform computations, create graphics (you’ll see this later on), use functions, and much more.\nHowever, sometimes you may wish to use additional functionality that does not come directly with the installed software. R is a free and open-source programming language with a vast user network. This has led in turn to a myriad of user-contributed additional functionality, each bundled in different packages (also called libraries).\nThese packages are stored in the Comprehensive R Archive Network (CRAN) and can be installed.\nYou only need to install a package once, to bring it from the web to your local computer.\nThen, to actually use the package in every R session, you simply need to load the package (and not install it every single time).\n\n\n\nYou install a package by going to the Files & Plots panel, click “Packages” in the tabs, then select Install, type the package names and click Install.\n\n\n\nRemember, you only need to do this once.\nAs we said, package are pre-shipped with your R installation. Examples are the base, stats, and graphics packages. Each of those can be considered as a “folder” providing different functionality. You will see the use of those functions later on in the course.\n\n\n\nActually, when you install R it comes with many more packages. Some are automatically loaded when you open R, making their functionality immediately available to you, while others are left in the background unopened. In the picture below, the loaded packages are shown in orange, and the ones not loaded are in gray.\nFor each new R session, in order to use the functionality from packages that are not automatically loaded, or that you have installed later, you must load the package with the function library(package_name) at the top of the R script."
  },
  {
    "objectID": "02-data.html#adding-functionality",
    "href": "02-data.html#adding-functionality",
    "title": "2: Working with data",
    "section": "",
    "text": "Your R installation comes pre-packed with lots of functionality. Just by opening R you are able to perform computations, create graphics (you’ll see this later on), use functions, and much more.\nHowever, sometimes you may wish to use additional functionality that does not come directly with the installed software. R is a free and open-source programming language with a vast user network. This has led in turn to a myriad of user-contributed additional functionality, each bundled in different packages (also called libraries).\nThese packages are stored in the Comprehensive R Archive Network (CRAN) and can be installed.\nYou only need to install a package once, to bring it from the web to your local computer.\nThen, to actually use the package in every R session, you simply need to load the package (and not install it every single time).\n\n\n\nYou install a package by going to the Files & Plots panel, click “Packages” in the tabs, then select Install, type the package names and click Install.\n\n\n\nRemember, you only need to do this once.\nAs we said, package are pre-shipped with your R installation. Examples are the base, stats, and graphics packages. Each of those can be considered as a “folder” providing different functionality. You will see the use of those functions later on in the course.\n\n\n\nActually, when you install R it comes with many more packages. Some are automatically loaded when you open R, making their functionality immediately available to you, while others are left in the background unopened. In the picture below, the loaded packages are shown in orange, and the ones not loaded are in gray.\nFor each new R session, in order to use the functionality from packages that are not automatically loaded, or that you have installed later, you must load the package with the function library(package_name) at the top of the R script."
  },
  {
    "objectID": "02-data.html#prelude",
    "href": "02-data.html#prelude",
    "title": "2: Working with data",
    "section": "Prelude",
    "text": "Prelude\nIn today’s lesson you will need the tidyverse package. We will use this package to read data files into R. Try loading it:\nlibrary(tidyverse)\nDid you get an error?\nIf yes, you don’t have the package installed. Install the package following the instructions above. Then try again running library(tidyverse).\nIf you don’t get an error, amazing! It should look like this:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nImportant. It is common practice to put all code loading packages at the top of the R script.\nThere are some messages returned, but none of those are errors. In fact, in R errors start with the word “Error”. If you don’t read the word “Error” in the message, it’s not an error.\nOpen RStudio, create a new R script called lesson2.R and save it into your r-bootcamp folder.\nIn the previous examples we only entered data manually. However, this isn’t very efficient if you are working with lots of data and it increases the chance of typing errors. This lesson will show you how to read into R data stored in files.\nBefore going ahead, please download the following data files and save them into the subfolder “data” of r-bootcamp\n\nUNpop.csv\nUNpop.txt\nUNpop.xlsx"
  },
  {
    "objectID": "02-data.html#data-files",
    "href": "02-data.html#data-files",
    "title": "2: Working with data",
    "section": "Data files",
    "text": "Data files\nA data file is any file storing data. The following types of data files are commonly used:\n\n.csv (Comma Separated Values)\n.tsv or .txt (Tab Separated Values)\n.xlsx (Excel data file)\n\nHow does each look?\n\n\ncsv files\ntsv or txt files\nExcel files"
  },
  {
    "objectID": "02-data.html#directories",
    "href": "02-data.html#directories",
    "title": "2: Working with data",
    "section": "Directories",
    "text": "Directories\nBefore importing the data into R we need to tell R in which folder we are working. This is because R will look for the data files in that folder only. If the files are stored somewhere else, R will not find them.\nThis step is called setting the working directory. You can set the working directory, i.e. tell R in which folder you are working, by going to the RStudio menu -&gt; Session -&gt; Set working directory -&gt; Choose directory -&gt; Navigate to r-bootcamp\nAlternatively, you can go to the Files and Plots panel, click the Files tab, navigate to the r-bootcamp folder, click More -&gt; Set as working directory.\nWithin the r-bootcamp folder, you should have a subfolder “data” with the data files saved inside."
  },
  {
    "objectID": "02-data.html#reading-data-into-r",
    "href": "02-data.html#reading-data-into-r",
    "title": "2: Working with data",
    "section": "Reading data into R",
    "text": "Reading data into R\nTo read a CSV file into R we use the following function:\n\nunpop &lt;- read_csv('data/UNpop.csv')\n\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): year, world_pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nunpop\n\n# A tibble: 8 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n4  1980   4458003\n5  1990   5327231\n6  2000   6143494\n7  2010   6956824\n8  2020   7794799\n\n\nTo read a file in which the values are separated by tabs, we use the read_tsv() function:\n\nunpop_txt &lt;- read_tsv('data/UNpop.txt')\n\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): year, world_pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nunpop_txt\n\n# A tibble: 8 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n4  1980   4458003\n5  1990   5327231\n6  2000   6143494\n7  2010   6956824\n8  2020   7794799\n\n\nTo read Excel files, you need to load a package called readxl. Edit the top of your R script to have also the library(reaxl) command. The top of your R script should look like this:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nNow, read the excel file into R:\n\nunpop_xlsx &lt;- read_excel('data/UNpop.xlsx')\nunpop_xlsx\n\n# A tibble: 8 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n4  1980   4458003\n5  1990   5327231\n6  2000   6143494\n7  2010   6956824\n8  2020   7794799\n\n\nAs you can see, they all produce the same data in R. Since they’re all the same, let’s look at the first one.\n\nunpop\n\n# A tibble: 8 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n4  1980   4458003\n5  1990   5327231\n6  2000   6143494\n7  2010   6956824\n8  2020   7794799\n\n\nThe data table is stored in an object of class tibble. If you inspect the class it looks quite complicated but it has “tbl” among the values.\n\nclass(unpop)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nHow many rows and columns are there?\n\nnrow(unpop)\n\n[1] 8\n\nncol(unpop)\n\n[1] 2\n\ndim(unpop)\n\n[1] 8 2\n\n\nThe data store the world population estimates for the last 8 decades. Hence, we have 2 variables (year and world_pop), and 8 rows (one for each decade).\nTo get the names of the recorded variables you could either use names() or colnames():\n\nnames(unpop)\n\n[1] \"year\"      \"world_pop\"\n\ncolnames(unpop)\n\n[1] \"year\"      \"world_pop\"\n\n\nThe summary() function produces quick summaries for the variables in the data table. However, this is just for a quick exploration, it is not appropriate copying and pasting such output for a journal publication, essay, or report. That would require better styling.\n\nsummary(unpop)\n\n      year        world_pop      \n Min.   :1950   Min.   :2536431  \n 1st Qu.:1968   1st Qu.:3534065  \n Median :1985   Median :4892617  \n Mean   :1985   Mean   :4994021  \n 3rd Qu.:2002   3rd Qu.:6346826  \n Max.   :2020   Max.   :7794799  \n\n\nWe can create a publication quality summary table by doing for example\n\nstats &lt;- unpop %&gt;%\n  summarise(Mean = mean(world_pop), \n            SD = sd(world_pop),\n            Min = min(world_pop),\n            Max = max(world_pop))\nstats\n\n# A tibble: 1 × 4\n      Mean       SD     Min     Max\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 4994021. 1888599. 2536431 7794799\n\n\nThe function %&gt;% is called pipe and it simply means “take what’s on the left, then do …”\nFor example these are equivalent:\n\nround(exp(sqrt(1:10)), 2)\n\n [1]  2.72  4.11  5.65  7.39  9.36 11.58 14.09 16.92 20.09 23.62\n\n1:10 %&gt;%\n  sqrt() %&gt;%\n  exp() %&gt;%\n  round(2)\n\n [1]  2.72  4.11  5.65  7.39  9.36 11.58 14.09 16.92 20.09 23.62\n\n\nTo format the tables for journals or reports, you can use one of the following options.\n\n\nOption 1: kable()\nOption 2: gt()\n\n\n\nThe kable() function from the knitr package. If you don’t have the package installed, make sure you run install.packages(\"knitr\") before!\n\nlibrary(knitr)\n\nkable(stats)\n\n\n\nMean\nSD\nMin\nMax\n\n\n4994021\n1888599\n2536431\n7794799\n\n\n\n\n\n\nThe gt() function from the gt package. If you don’t have the package installed, make sure you run install.packages(\"gt\") before!\n\nlibrary(gt)\n\ngt(stats)\n\n\n\n\n\n\nMean\n      SD\n      Min\n      Max\n    \n\n4994021\n1888599\n2536431\n7794799"
  },
  {
    "objectID": "02-data.html#indexing-data-tables",
    "href": "02-data.html#indexing-data-tables",
    "title": "2: Working with data",
    "section": "Indexing data tables",
    "text": "Indexing data tables\nWe can extract particular parts of a tibble (i.e. data table) as follows\n\nunpop[1:3, ]\n\n# A tibble: 3 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n\nunpop[c(2, 5, 6), ]\n\n# A tibble: 3 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1960   3034950\n2  1990   5327231\n3  2000   6143494\n\nunpop[2, ]\n\n# A tibble: 1 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1960   3034950\n\nunpop[, 2]\n\n# A tibble: 8 × 1\n  world_pop\n      &lt;dbl&gt;\n1   2536431\n2   3034950\n3   3700437\n4   4458003\n5   5327231\n6   6143494\n7   6956824\n8   7794799\n\nunpop$world_pop\n\n[1] 2536431 3034950 3700437 4458003 5327231 6143494 6956824 7794799\n\nunpop$world_pop[1:3]\n\n[1] 2536431 3034950 3700437\n\nunpop[1:3, 2]\n\n# A tibble: 3 × 1\n  world_pop\n      &lt;dbl&gt;\n1   2536431\n2   3034950\n3   3700437\n\nunpop[1:3, 'world_pop']\n\n# A tibble: 3 × 1\n  world_pop\n      &lt;dbl&gt;\n1   2536431\n2   3034950\n3   3700437\n\n\n\n\nExercise\nAnswer\n\n\n\nExtract the first 3 years from the data.\n\n\nThe following returns a tibble\n\nunpop[1:3, 'year']\n\n# A tibble: 3 × 1\n   year\n  &lt;dbl&gt;\n1  1950\n2  1960\n3  1970\n\n\nWhile this way returns a vector\n\nunpop$year[1:3]\n\n[1] 1950 1960 1970\n\n\n\n\n\nTo create a new variable, you simply use the $ symbol with a new name\n\nunpop$world_pop_millions &lt;- unpop$world_pop /  1000"
  },
  {
    "objectID": "02-data.html#the-tidyverse-way",
    "href": "02-data.html#the-tidyverse-way",
    "title": "2: Working with data",
    "section": "The tidyverse way",
    "text": "The tidyverse way\nThere is a simpler way to work with data. The tidyverse package, which you can load with\n\nlibrary(tidyverse)\n\nHas the following functions:\n\nslice() to keep only some rows\nfilter() to only keep the rows matching some condition\nselect() to only keep specific columns\nmutate() to create new columns via a computation\n\nDisplay all data:\n\nunpop\n\n# A tibble: 8 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1950   2536431              2536.\n2  1960   3034950              3035.\n3  1970   3700437              3700.\n4  1980   4458003              4458.\n5  1990   5327231              5327.\n6  2000   6143494              6143.\n7  2010   6956824              6957.\n8  2020   7794799              7795.\n\n\nCreate a new column with the population in millions\n\nunpop %&gt;%\n  mutate(world_pop_millions = world_pop / 1000)\n\n# A tibble: 8 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1950   2536431              2536.\n2  1960   3034950              3035.\n3  1970   3700437              3700.\n4  1980   4458003              4458.\n5  1990   5327231              5327.\n6  2000   6143494              6143.\n7  2010   6956824              6957.\n8  2020   7794799              7795.\n\n\nCheck if the column is still in the original data:\n\nunpop\n\n# A tibble: 8 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1950   2536431              2536.\n2  1960   3034950              3035.\n3  1970   3700437              3700.\n4  1980   4458003              4458.\n5  1990   5327231              5327.\n6  2000   6143494              6143.\n7  2010   6956824              6957.\n8  2020   7794799              7795.\n\n\nIt is not there! Why? Because we didn’t overwrite the unpop variable when we created the new column:\n\nunpop &lt;- unpop %&gt;%\n  mutate(world_pop_millions = world_pop / 1000)\n\nKeep first 3 rows only:\n\nunpop %&gt;%\n  slice(1:3)\n\n# A tibble: 3 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1950   2536431              2536.\n2  1960   3034950              3035.\n3  1970   3700437              3700.\n\n\nKeep only the world_pop column:\n\nunpop %&gt;%\n  select(world_pop)\n\n# A tibble: 8 × 1\n  world_pop\n      &lt;dbl&gt;\n1   2536431\n2   3034950\n3   3700437\n4   4458003\n5   5327231\n6   6143494\n7   6956824\n8   7794799\n\n\nKeep only the world_pop column and the first 3 rows only:\n\nunpop %&gt;%\n  select(world_pop) %&gt;%\n  slice(1:3)\n\n# A tibble: 3 × 1\n  world_pop\n      &lt;dbl&gt;\n1   2536431\n2   3034950\n3   3700437\n\n\nLet’s compute the average population over that time period:\n\navg_pop &lt;- mean(unpop$world_pop)\navg_pop\n\n[1] 4994021\n\n\nKeep only the rows with a world population larger than the mean:\n\nunpop %&gt;%\n  filter(world_pop &gt; avg_pop)\n\n# A tibble: 4 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1990   5327231              5327.\n2  2000   6143494              6143.\n3  2010   6956824              6957.\n4  2020   7794799              7795.\n\n\nIn all the code above, the symbol %&gt;% is called pipe and can be inserted with Control + Shift + M on Windows or Command + Shift + M on macOS.\nIts role is telling R to continue the computation. Basically it allows you to write nested expressions\nbop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)\nusing a more human-readable form1:\nfoo_foo %&gt;%\n  hop(through = forest) %&gt;%\n  scoop(up = field_mouse) %&gt;%\n  bop(on = head)\nThe idea is that if you have some function f(), you can do\ndf %&gt;%\n  f(y)\nWhich has the same meaning as:\nf(df, y)\nSo whatever you carry forward with the pipe gets passed as the first argument of the next function.\nIf you want to be specific, you can use a . inside:\n\n10 %&gt;%\n  seq(., to = 20, by = 1)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\n\n\n10 %&gt;%\n  seq(from = 1, to = ., by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "02-data.html#tibbles",
    "href": "02-data.html#tibbles",
    "title": "2: Working with data",
    "section": "Tibbles",
    "text": "Tibbles\nA data table is an object which stores data. Up to now, you have only read data tables in R from files stored on your PC.\nYou can also create it directly yourself by typing the values into a function called tibble(), also part of the tidyverse package.\nIn lesson 1, we stored the world population and the years in separate numeric vectors:\n\nyear &lt;- seq(1950, 2020, by = 10)\nworld_pop &lt;- c(2536431, 3034950, 3700437, 4458003, \n               5327231, 6143494, 6956824, 7794799)\n\nBut the first value in world_pop is linked to the first year, and so on. So it’s better to keep the values linked together and arranged into a data table.\nIn a tibble we write the column names, an equal sign, and the values\ntibble(\n  column_name = values,\n  another_column_name = other_values\n)\n\ntbl &lt;- tibble(\n  year = seq(1950, 2020, by = 10),\n  world_pop = c(2536431, 3034950, 3700437, 4458003, 5327231, 6143494, 6956824, 7794799)\n)\ntbl\n\n# A tibble: 8 × 2\n   year world_pop\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1950   2536431\n2  1960   3034950\n3  1970   3700437\n4  1980   4458003\n5  1990   5327231\n6  2000   6143494\n7  2010   6956824\n8  2020   7794799"
  },
  {
    "objectID": "02-data.html#missing-values",
    "href": "02-data.html#missing-values",
    "title": "2: Working with data",
    "section": "Missing values",
    "text": "Missing values\nWe do not have the estimates for the current year yet, and also we do not have the estimates for 1930 and 1940. Let’s add rows corresponding to those years in the data table, but for the world population estimate value we will provide NA = not available.\n\nextra &lt;- tibble(year = c(1930, 1940, 2021), \n                world_pop = c(NA, NA, NA))\nextra\n\n# A tibble: 3 × 2\n   year world_pop\n  &lt;dbl&gt; &lt;lgl&gt;    \n1  1930 NA       \n2  1940 NA       \n3  2021 NA       \n\nunpop &lt;- bind_rows(unpop, extra)\nunpop\n\n# A tibble: 11 × 3\n    year world_pop world_pop_millions\n   &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n 1  1950   2536431              2536.\n 2  1960   3034950              3035.\n 3  1970   3700437              3700.\n 4  1980   4458003              4458.\n 5  1990   5327231              5327.\n 6  2000   6143494              6143.\n 7  2010   6956824              6957.\n 8  2020   7794799              7795.\n 9  1930        NA                NA \n10  1940        NA                NA \n11  2021        NA                NA \n\n\nLet’s arrange the years in ascending order\n\nunpop &lt;- arrange(unpop, year)\nunpop\n\n# A tibble: 11 × 3\n    year world_pop world_pop_millions\n   &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n 1  1930        NA                NA \n 2  1940        NA                NA \n 3  1950   2536431              2536.\n 4  1960   3034950              3035.\n 5  1970   3700437              3700.\n 6  1980   4458003              4458.\n 7  1990   5327231              5327.\n 8  2000   6143494              6143.\n 9  2010   6956824              6957.\n10  2020   7794799              7795.\n11  2021        NA                NA \n\n\nLet’s try computing the summaries again:\n\nunpop %&gt;%\n  summarise(Mean = mean(world_pop), \n            SD = sd(world_pop),\n            Min = min(world_pop),\n            Max = max(world_pop)) %&gt;%\n  gt()\n\n\n\n\n\n\nMean\n      SD\n      Min\n      Max\n    \n\nNA\nNA\nNA\nNA\n\n\n\n\n\nIt’s not working anymore! Why is that?\nR cannot sum a number with something that isn’t available.\n\n2 + NA\n\n[1] NA\n\n\nHence, there are 2 possible solutions:\n\ntell the R functions to ignore the NA values by specifying the argument na.rm = TRUE\n\nsubset the data to only keep the rows without NAs\n\n\n\nOption 1\nOption 2\n\n\n\n\nunpop %&gt;%\n  summarise(Mean = mean(world_pop, na.rm = TRUE), \n            SD = sd(world_pop, na.rm = TRUE),\n            Min = min(world_pop, na.rm = TRUE),\n            Max = max(world_pop, na.rm = TRUE)) %&gt;%\n  gt()\n\n\n\n\n\n\nMean\n      SD\n      Min\n      Max\n    \n\n4994021\n1888599\n2536431\n7794799\n\n\n\n\n\n\n\n\nunpop_clean &lt;- na.omit(unpop)\nunpop_clean\n\n# A tibble: 8 × 3\n   year world_pop world_pop_millions\n  &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1  1950   2536431              2536.\n2  1960   3034950              3035.\n3  1970   3700437              3700.\n4  1980   4458003              4458.\n5  1990   5327231              5327.\n6  2000   6143494              6143.\n7  2010   6956824              6957.\n8  2020   7794799              7795.\n\nunpop_clean %&gt;%\n  summarise(Mean = mean(world_pop), \n            SD = sd(world_pop),\n            Min = min(world_pop),\n            Max = max(world_pop)) %&gt;%\n  gt()\n\n\n\n\n\n\nMean\n      SD\n      Min\n      Max\n    \n\n4994021\n1888599\n2536431\n7794799"
  },
  {
    "objectID": "02-data.html#graphics",
    "href": "02-data.html#graphics",
    "title": "2: Working with data",
    "section": "Graphics",
    "text": "Graphics\nYou may also want to display your data visually. We do that using the ggplot package, which is automatically loaded when you load the tidyverse package.\nThe idea of a ggplot is to build a plot layer by layer. You start by specifying the axes, then add the geometries you want shown (points, lines, columns/bars), and then add labels, and theme options.\n\n\nSource: Michela Cameletti\n\n\n# Specify axes only\nggplot(data = unpop_clean, aes(x = year, y = world_pop))\n\n\n\n\n\n\n# Add columns\nggplot(data = unpop_clean, aes(x = year, y = world_pop)) +\n  geom_col()\n\n\n\n\n\n\n# Change labels\nggplot(data = unpop_clean, aes(x = year, y = world_pop / 1000)) +\n  geom_col() + \n  labs(x = 'Year', y = 'World population estimate (in millions)')\n\n\n\n\n\n\n# Change theme\nggplot(data = unpop_clean, aes(x = year, y = world_pop / 1000)) +\n  geom_col() + \n  labs(x = 'Year', y = 'World population estimate (in millions)') +\n  theme_classic()\n\n\n\n\n\n\n\nAlternatively, you could plot the population for each decade as a dot and join the dots with lines:\n\nggplot(data = unpop_clean, aes(x = year, y = world_pop / 1000)) +\n  geom_point(size = 3) + \n  geom_line() +\n  labs(x = 'Year', y = 'World population estimate (in millions)') +\n  theme_classic()\n\n\n\n\n\n\n\nNote the key components of a ggplot:\n\ndata = is where we provide the name of the data table\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\nThen we add (using +) some geometry. These are the shapes (for instance, points or lines), which will be put in the correct place according to what we specified in aes().\n+ geom_... Adds different shapes (e.g., points) to the plot.\n\nYou can find great documentation on ggplot2 at https://www.statsandr.com/blog/graphics-in-r-with-ggplot2/."
  },
  {
    "objectID": "02-data.html#recommended-activities",
    "href": "02-data.html#recommended-activities",
    "title": "2: Working with data",
    "section": "Recommended activities",
    "text": "Recommended activities\n\nCheckpoint"
  },
  {
    "objectID": "02-data.html#recommended-readings",
    "href": "02-data.html#recommended-readings",
    "title": "2: Working with data",
    "section": "Recommended readings",
    "text": "Recommended readings\nFor further information, check to the following:\n\nModerndive\nStat 545"
  },
  {
    "objectID": "02-data.html#footnotes",
    "href": "02-data.html#footnotes",
    "title": "2: Working with data",
    "section": "Footnotes",
    "text": "Footnotes\n\nSource: https://github.com/hadley/r4ds/blob/master/pipes.Rmd↩︎"
  },
  {
    "objectID": "03-exploring-data.html",
    "href": "03-exploring-data.html",
    "title": "3: Exploring data",
    "section": "",
    "text": "In this lesson we will work with some of the data from the Gapminder project. This data are available via an R package, which you will need to install in R.\nFirst, install the gapminder package. Then load the gapminder package and the tidyverse package as we will need its functionality to work with data.\n\nlibrary(gapminder)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe gapminder package provides you with a data table having the same name as the package (unfortunately).\nLet’s inspect the contents of the data using the glimpse function from tidyverse:\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nThe output tells us that the Gapminder data contain 1704 rows and 6 columns.\nAn alternative to glimpse() is the str() function, showing the structure of the data\n\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ..."
  },
  {
    "objectID": "03-exploring-data.html#gapminder-data",
    "href": "03-exploring-data.html#gapminder-data",
    "title": "3: Exploring data",
    "section": "",
    "text": "In this lesson we will work with some of the data from the Gapminder project. This data are available via an R package, which you will need to install in R.\nFirst, install the gapminder package. Then load the gapminder package and the tidyverse package as we will need its functionality to work with data.\n\nlibrary(gapminder)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe gapminder package provides you with a data table having the same name as the package (unfortunately).\nLet’s inspect the contents of the data using the glimpse function from tidyverse:\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nThe output tells us that the Gapminder data contain 1704 rows and 6 columns.\nAn alternative to glimpse() is the str() function, showing the structure of the data\n\nstr(gapminder)\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ..."
  },
  {
    "objectID": "03-exploring-data.html#handling-data",
    "href": "03-exploring-data.html#handling-data",
    "title": "3: Exploring data",
    "section": "Handling data",
    "text": "Handling data\nIf you have not already done so, install the tidyverse package now. Then, load it via:\n\nlibrary(tidyverse)\n\nLook at the data:\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nLook at the head (= top 6 rows) or tail (= last 6 rows) of the data\n\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\ntail(gapminder)\n\n# A tibble: 6 × 6\n  country  continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Zimbabwe Africa     1982    60.4  7636524      789.\n2 Zimbabwe Africa     1987    62.4  9216418      706.\n3 Zimbabwe Africa     1992    60.4 10704340      693.\n4 Zimbabwe Africa     1997    46.8 11404948      792.\n5 Zimbabwe Africa     2002    40.0 11926563      672.\n6 Zimbabwe Africa     2007    43.5 12311143      470.\n\n\nInspect the variable names, the number of columns, the length of the data (= ncol), the dimension, and the number of rows:\n\nnames(gapminder)\n\n[1] \"country\"   \"continent\" \"year\"      \"lifeExp\"   \"pop\"       \"gdpPercap\"\n\nncol(gapminder)\n\n[1] 6\n\nlength(gapminder)\n\n[1] 6\n\ndim(gapminder)\n\n[1] 1704    6\n\nnrow(gapminder)\n\n[1] 1704\n\n\nYou can get a quick statistical description of each variable with\n\nsummary(gapminder)\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1"
  },
  {
    "objectID": "03-exploring-data.html#look-at-some-variables-individually",
    "href": "03-exploring-data.html#look-at-some-variables-individually",
    "title": "3: Exploring data",
    "section": "Look at some variables individually",
    "text": "Look at some variables individually\nLet’s start by looking at the year variable. Clearly, we are not running the following code otherwise we will get 1704 values printed out and it will be impossible to make any sense of that!\n\n# Don't run this, or you will get 1704 values printed!\ngapminder$year\n\nInstead we will use head() to see the first few values:\n\nhead(gapminder$year)\n\n[1] 1952 1957 1962 1967 1972 1977\n\n\nIf you wish to see more entries,\n\nhead(gapminder$year, n = 25)\n\n [1] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 1952 1957 1962\n[16] 1967 1972 1977 1982 1987 1992 1997 2002 2007 1952\n\ntail(gapminder$year, n = 25)\n\n [1] 2007 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 1952 1957\n[16] 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007\n\n\nYear is a numeric vector with entries being whole numbers (no decimals), in R these are also called integers.\n\nclass(gapminder$year)\n\n[1] \"integer\"\n\n\nWe can get a quick summary of the data with the summary() function:\n\nsummary(gapminder$year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1952    1966    1980    1980    1993    2007 \n\n\nNext, we can compute how many times each year appears in the data. This is also called a frequency table, as it shows the absolute frequency (or count) of how many times each year is present in the data:\n\ntable(gapminder$year)\n\n\n1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 2002 2007 \n 142  142  142  142  142  142  142  142  142  142  142  142 \n\n\nLet’s now focus on the continent variable:\n\nclass(gapminder$continent)\n\n[1] \"factor\"\n\nhead(gapminder$continent)\n\n[1] Asia Asia Asia Asia Asia Asia\nLevels: Africa Americas Asia Europe Oceania\n\n\nWe can see that continent is a factor object. The distinct categories it can take (= levels) and the number of levels are:\n\nlevels(gapminder$continent)\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n\nnlevels(gapminder$continent)\n\n[1] 5\n\n\nPerforming a summary() on an object of class factor will perform a frequency table\n\nsummary(gapminder$continent)\n\n  Africa Americas     Asia   Europe  Oceania \n     624      300      396      360       24 \n\n\nThat is, the same as\n\ntable(gapminder$continent)\n\n\n  Africa Americas     Asia   Europe  Oceania \n     624      300      396      360       24 \n\n\nFor small data tables, you can look at the data directly by typing\nView(gapminder)"
  },
  {
    "objectID": "03-exploring-data.html#plotting",
    "href": "03-exploring-data.html#plotting",
    "title": "3: Exploring data",
    "section": "Plotting",
    "text": "Plotting\nWe can create a barplot showing how many times each continent appears in the data\n\nggplot(data = gapminder, aes(x = continent)) +\n    geom_bar()\n\n\n\n\nLet’s focus on Europe:\n\ngapminder_eu &lt;- gapminder %&gt;%\n    filter(continent == 'Europe')\n\nhead(gapminder_eu)\n\n# A tibble: 6 × 6\n  country continent  year lifeExp     pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n1 Albania Europe     1952    55.2 1282697     1601.\n2 Albania Europe     1957    59.3 1476505     1942.\n3 Albania Europe     1962    64.8 1728137     2313.\n4 Albania Europe     1967    66.2 1984060     2760.\n5 Albania Europe     1972    67.7 2263554     3313.\n6 Albania Europe     1977    68.9 2509048     3533.\n\n\nAverage life expectancy over european countries for each year:\n\ngapminder_avg &lt;- gapminder_eu %&gt;%\n    group_by(year) %&gt;%\n    summarise(mean_life_exp = mean(lifeExp))\n\ngapminder_avg\n\n# A tibble: 12 × 2\n    year mean_life_exp\n   &lt;int&gt;         &lt;dbl&gt;\n 1  1952          64.4\n 2  1957          66.7\n 3  1962          68.5\n 4  1967          69.7\n 5  1972          70.8\n 6  1977          71.9\n 7  1982          72.8\n 8  1987          73.6\n 9  1992          74.4\n10  1997          75.5\n11  2002          76.7\n12  2007          77.6\n\n\nLet’s show it as a plot\n\nggplot(data = gapminder_avg, aes(x = year, y = mean_life_exp)) +\n    geom_point() +\n    geom_line() +\n    labs(x = 'Year', y = 'Life expectancy (EU average)')\n\n\n\n\nWhat if we wanted to compare these curves across different continents?\n\ngapminder_cont &lt;- gapminder %&gt;%\n    group_by(continent, year) %&gt;%\n    summarise(mean_life_exp = mean(lifeExp))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\ngapminder_cont\n\n# A tibble: 60 × 3\n# Groups:   continent [5]\n   continent  year mean_life_exp\n   &lt;fct&gt;     &lt;int&gt;         &lt;dbl&gt;\n 1 Africa     1952          39.1\n 2 Africa     1957          41.3\n 3 Africa     1962          43.3\n 4 Africa     1967          45.3\n 5 Africa     1972          47.5\n 6 Africa     1977          49.6\n 7 Africa     1982          51.6\n 8 Africa     1987          53.3\n 9 Africa     1992          53.6\n10 Africa     1997          53.6\n# ℹ 50 more rows\n\n\n\nggplot(data = gapminder_cont, \n       aes(x = year, y = mean_life_exp, color = continent)) +\n    geom_point() +\n    geom_line() +\n    labs(x = 'Year', y = 'Average life expectancy')\n\n\n\n\nWow! this plot is definitely much more informative than all that huge table of numbers you get from View(gapminder).\nFrom the plot we can see the increasing trend over time of average life expectancy in each continent, and we see that Oceania consistently had the highest average life expectancy than any other country, while Africa had the lowest.\n\n\nExercise\nAnswer\n\n\n\nGroup the Gapminder data by continent and country, and compute for each the average (over the different years) life expectancy and average GDP per capita.\nCreate a plot where each country is shown as a point, the x-axis has the average GDP per capita, the y-axis the average life expectancy, and the points are coloured by continent.\n\n\n\ngapminder_le_gdp &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    summarise(mean_le = mean(lifeExp),\n              mean_gdp = mean(gdpPercap))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\ngapminder_le_gdp\n\n# A tibble: 142 × 4\n# Groups:   continent [5]\n   continent country                  mean_le mean_gdp\n   &lt;fct&gt;     &lt;fct&gt;                      &lt;dbl&gt;    &lt;dbl&gt;\n 1 Africa    Algeria                     59.0    4426.\n 2 Africa    Angola                      37.9    3607.\n 3 Africa    Benin                       48.8    1155.\n 4 Africa    Botswana                    54.6    5032.\n 5 Africa    Burkina Faso                44.7     844.\n 6 Africa    Burundi                     44.8     472.\n 7 Africa    Cameroon                    48.1    1775.\n 8 Africa    Central African Republic    43.9     959.\n 9 Africa    Chad                        46.8    1165.\n10 Africa    Comoros                     52.4    1314.\n# ℹ 132 more rows\n\n\n\nggplot(data = gapminder_le_gdp,\n       aes(x = mean_gdp, y = mean_le, color = continent)) +\n    geom_point() +\n    labs(x = 'Average GDP per capita', y = 'Average life expectancy')\n\n\n\n\nTo make it easier to read, we can change the x-axis to increase in powers of 10.\n\nggplot(data = gapminder_le_gdp,\n       aes(x = mean_gdp, y = mean_le, color = continent)) +\n    geom_point() +\n    scale_x_log10() +\n    labs(x = 'Average GDP per capita', y = 'Average life expectancy')\n\n\n\n\n\n\n\n\n\nExercise\nAnswer\n\n\n\nCreate a new column called pop_m, storing the population in millions.\nStore the result in a new tibble called gapminder_new.\n\n\n\ngapminder_new &lt;- gapminder %&gt;%\n    mutate(pop_m = pop / 1000000)\n\ngapminder_new\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap pop_m\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.  8.43\n 2 Afghanistan Asia       1957    30.3  9240934      821.  9.24\n 3 Afghanistan Asia       1962    32.0 10267083      853. 10.3 \n 4 Afghanistan Asia       1967    34.0 11537966      836. 11.5 \n 5 Afghanistan Asia       1972    36.1 13079460      740. 13.1 \n 6 Afghanistan Asia       1977    38.4 14880372      786. 14.9 \n 7 Afghanistan Asia       1982    39.9 12881816      978. 12.9 \n 8 Afghanistan Asia       1987    40.8 13867957      852. 13.9 \n 9 Afghanistan Asia       1992    41.7 16317921      649. 16.3 \n10 Afghanistan Asia       1997    41.8 22227415      635. 22.2 \n# ℹ 1,694 more rows\n\n\n\n\n\n\n\nExercise\nAnswer\n\n\n\nConsider now only the countries in the Americas. Plot the average GDP per capita as a function of year.\n\n\n\ngapminder_am &lt;- gapminder %&gt;%\n    filter(continent == 'Americas') %&gt;%\n    group_by(year) %&gt;%\n    summarise(mean_gdp = mean(gdpPercap))\ngapminder_am\n\n# A tibble: 12 × 2\n    year mean_gdp\n   &lt;int&gt;    &lt;dbl&gt;\n 1  1952    4079.\n 2  1957    4616.\n 3  1962    4902.\n 4  1967    5668.\n 5  1972    6491.\n 6  1977    7352.\n 7  1982    7507.\n 8  1987    7793.\n 9  1992    8045.\n10  1997    8889.\n11  2002    9288.\n12  2007   11003.\n\nggplot(data = gapminder_am, \n           aes(x = year, y = mean_gdp)) +\n    geom_point() +\n    geom_line()"
  },
  {
    "objectID": "03-exploring-data.html#recommended-activities",
    "href": "03-exploring-data.html#recommended-activities",
    "title": "3: Exploring data",
    "section": "Recommended activities",
    "text": "Recommended activities\n\n3.1 Working with tibbles\n3.2 Isolating data with dplyr\n3.3 Derive information with dplyr"
  },
  {
    "objectID": "04-insights-from-data.html",
    "href": "04-insights-from-data.html",
    "title": "4: Insights from data",
    "section": "",
    "text": "Research question\nHas the average body temperature for healthy humans changed from the long-thought 37 °C?\n\nYou might think that the average body temperature for healthy humans is about 37 °C. This is what most people would say and it kind of has been given for granted for many years.\nHowever, could it be possible that the average body temperature for healthy humans has changed over time? Perhaps this could be due to the climate change?\nWe will use data1 comprising measurements on body temperature and pulse rate for a random sample of \\(n = 50\\) healthy subjects. The data are available here and also at the following address: https://uoepsy.github.io/data/BodyTemp.csv"
  },
  {
    "objectID": "04-insights-from-data.html#investigation-body-temperature",
    "href": "04-insights-from-data.html#investigation-body-temperature",
    "title": "4: Insights from data",
    "section": "",
    "text": "Research question\nHas the average body temperature for healthy humans changed from the long-thought 37 °C?\n\nYou might think that the average body temperature for healthy humans is about 37 °C. This is what most people would say and it kind of has been given for granted for many years.\nHowever, could it be possible that the average body temperature for healthy humans has changed over time? Perhaps this could be due to the climate change?\nWe will use data1 comprising measurements on body temperature and pulse rate for a random sample of \\(n = 50\\) healthy subjects. The data are available here and also at the following address: https://uoepsy.github.io/data/BodyTemp.csv"
  },
  {
    "objectID": "04-insights-from-data.html#hypotheses",
    "href": "04-insights-from-data.html#hypotheses",
    "title": "4: Insights from data",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe first step is to translate the research question into hypothesis. This can be tricky sometimes and it involves understanding the research question.\nThe mean for an entire population is commonly denoted with the Greek symbol \\(\\mu\\) (pronounced mu). In loose terms we want to test whether the mean body temperature for all healthy humans is 37 °C or if it has changed in either direction (i.e. it is higher or lower).\nIn formal notation, we will investigate the following null and alternative hypotheses:\n\\[H_0 : \\mu = 37 °C\\] \\[H_1 : \\mu \\neq 37 °C\\]\nThe null hypothesis (\\(H_0\\)) represents the status quo, i.e. what is the currently accepted standard. The alternative hypothesis (\\(H_1\\)) typically encodes our research question.\n\nnull hypothesis\nalternative hypothesis"
  },
  {
    "objectID": "04-insights-from-data.html#data",
    "href": "04-insights-from-data.html#data",
    "title": "4: Insights from data",
    "section": "Data",
    "text": "Data\nFirst, we need to read the data into R.\n\nlibrary(tidyverse)\n\nbtdata &lt;- read_csv('data/BodyTemp.csv')\nhead(btdata)\n\n# A tibble: 6 × 2\n  BodyTemp Pulse\n     &lt;dbl&gt; &lt;dbl&gt;\n1     36.4    69\n2     37.4    77\n3     37.2    75\n4     37.1    84\n5     36.7    71\n6     37.2    76\n\ndim(btdata)\n\n[1] 50  2\n\n\nFrom the output above we see that we have measuremenets on 2 variables (BodyTemp and Pulse) on 50 participants.\nLet’s visualise the variable of interest, BodyTemp:\n\nggplot(btdata, aes(x = BodyTemp)) +\n    geom_histogram(color = 'white') +\n    labs(x = 'Body temperature')\n\n\n\n\n\n\n\nNext, we numerically describe the variable of interest, BodyTemp:\n\nbtstats &lt;- btdata %&gt;%\n    summarise(\n        Min = min(BodyTemp),\n        Med = median(BodyTemp),\n        M = mean(BodyTemp),\n        SD = sd(BodyTemp),\n        Max = max(BodyTemp)\n    )\n\nbtstats\n\n# A tibble: 1 × 5\n    Min   Med     M    SD   Max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  35.8  36.8  36.8 0.425  38.2\n\n\nThe numbers above are numerical descriptions of the body temperatures in the collected sample of 50 healthy subjects. Any number which summarises data in a sample is called a statistic (short for sample characteristic). The two most famous statistics are probably the sample mean (or average) \\(M = 36.81\\) and the standard deviation \\(SD = 0.43\\).\n\nstatistic\nOn a completely different level lies the concept of parameter, which is any number describing the whole population. Often, we do not have data for the entire population, and hence the parameter is unknown.\n\nparameter\nWhen we want to investigate a research question, we typically obtain a sample from the entire population, then collect data from the participants in the sample, and use the sample statistic as our best estimate of the unknown population parameter.\nThe process of drawing conclusions about the entire population using only a subset of the data (the sample) is called (statistical) inference.\n\nbest estimate\nstatistical inference"
  },
  {
    "objectID": "04-insights-from-data.html#statistics-vary-from-sample-to-sample",
    "href": "04-insights-from-data.html#statistics-vary-from-sample-to-sample",
    "title": "4: Insights from data",
    "section": "Statistics vary from sample to sample",
    "text": "Statistics vary from sample to sample\nRecall the original sample statistics which we previously computed:\n\nbtstats\n\n# A tibble: 1 × 5\n    Min   Med     M    SD   Max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  35.8  36.8  36.8 0.425  38.2\n\n\nIf we were to take another sample, the sample mean would be different. Let’s try resampling (with replacement) 50 people:\n\nbtdata %&gt;%\n    sample_n(50, replace = TRUE)\n\n# A tibble: 50 × 2\n   BodyTemp Pulse\n      &lt;dbl&gt; &lt;dbl&gt;\n 1     37.4    77\n 2     37.1    64\n 3     36.9    70\n 4     36.9    82\n 5     36.9    83\n 6     36.5    77\n 7     36.8    73\n 8     37.2    81\n 9     36.4    74\n10     37.2    75\n# ℹ 40 more rows\n\n\nNext, compute the mean:\n\nbtdata %&gt;%\n    sample_n(50, replace = TRUE) %&gt;%\n    summarise(M = mean(BodyTemp))\n\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.9\n\n\nLet’s do this many times, 1000 say — leading to 1000 sample means, one for each sample of 50 participants:\n\nmany_means &lt;- replicate(1000, \n                        btdata %&gt;%\n                          sample_n(50, replace = TRUE) %&gt;%\n                          summarise(M = mean(BodyTemp)),\n                        simplify = FALSE)\nhead(many_means)\n\n[[1]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.8\n\n[[2]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.9\n\n[[3]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.8\n\n[[4]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.7\n\n[[5]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.8\n\n[[6]]\n# A tibble: 1 × 1\n      M\n  &lt;dbl&gt;\n1  36.8\n\n\n\nreplicate(n, code, simplify = FALSE) runs the code n times, and simplify = FALSE is an option for how the output should be stored (don’t worry about this).\nWe will now combine into a single tibble the 1000 means, each one computed from a different sample of 50 people:\n\nmany_means &lt;- bind_rows(many_means)\nhead(many_means)\n\n# A tibble: 6 × 1\n      M\n  &lt;dbl&gt;\n1  36.8\n2  36.9\n3  36.8\n4  36.7\n5  36.8\n6  36.8\n\ndim(many_means)\n\n[1] 1000    1\n\n\nWe now plot the distribution of the sample means. The distribution shows the possible values of the sample mean on the x axis, and how often each value appears on the y axis:\nDistribution\nThe distribution of a variable tells us what values it takes and how often it takes these values.\n\nggplot(many_means, aes(x = M)) +\n    geom_histogram(color = 'white') +\n    labs(x = 'Sample mean')\n\n\n\n\n\n\n\nThe distribution of a statistic computed on many resamples is called the sampling distribution of the statistic: it shows the values the statistic can take and how often they appear across many samples.\n\nsampling distribution\nAs you can see, most of the sample means you would obtain from samples of size 50 people would be within 36.7 and 36.9 °C. Only a couple of sample means appear to be around 37 °C.\nMany years ago, the statistician William Sealy Gosset — while working as the chief experimental brewer for the Guinness Brewery in Dublin — found a mathematical curve that fits the distribution of the sample mean shown in the histogram above. This is called a t distribution and depends on a number called the degrees of freedom (df) which is simply the sample size minus one: \\(n - 1\\).\nThe t distribution with \\(n-1\\) df is written \\(t(n - 1)\\) and in this case as the sample has size \\(n = 50\\), it is \\(t(49)\\):\n\nt distribution\ndegrees of freedom\n\n\n\n\n\n\n\n\nAs you can see above, the distribution of the sample mean follows a t distribution (the red curve above). You don’t need to learn the mathematical formula of the red curve, as R can compute it for you (more on this later).\nThe width of the distribution is the standard deviation of the sample mean. It has a special name however: standard error of the sample mean, as it tells us how variable the sample means are from sample to sample.\nIn other words, it tells us how reliable those means are as estimates of the unknown population mean. The smaller the variation (the narrows the distribution), the better.\n\nstandard error\n\\[\nSE(\\text{sample mean}) =\n\\frac{SD(\\text{original sample})}{\\sqrt{\\text{sample size}}}\n\\]\nIn this case, the original sample is the collected data btdata. The SE of the mean is computed as follows:\n\nSD &lt;- sd(btdata$BodyTemp) # same as btstats$SD\nSD\n\n[1] 0.4251776\n\nn &lt;- nrow(btdata)\nn\n\n[1] 50\n\nSE &lt;- SD / sqrt(n)\nSE\n\n[1] 0.0601292\n\n\n\\[\nSE(\\text{mean}) = \\frac{0.4251776}{\\sqrt{50}} = 0.0601292\n\\]\nYou can confirm that it is approximately equal to the standard deviation of the 1000 generated sample means. The two numbers for the SE will become closer and closer as the number of resamples increases (from 1,000 to 10,000 or 100,000 for example):\n\nsd(many_means$M)\n\n[1] 0.05977689\n\n\nThey seen pretty close to me!\nTo sum up, the sample mean is 36.81 °C, with a standard error of 0.06 °C.\n\nThink about it\nNote that the standard deviation of the original data is 0.43, while the standard deviation of the means is 0.06. That is, the sample means will always be less variable than the original data. This is good! We don’t want our best estimates to vary too much if we had collected a slightly different sample in the first place."
  },
  {
    "objectID": "04-insights-from-data.html#t-test-for-a-mean",
    "href": "04-insights-from-data.html#t-test-for-a-mean",
    "title": "4: Insights from data",
    "section": "T-test for a mean",
    "text": "T-test for a mean\nRecall the sampling distribution of the mean:\n\n\n\n\n\n\n\n\nHow can we use this distribution to test whether the unknown mean body temperature for the entire population of healthy individuals is 37 °C?\nThe answer lies in comparing the difference between the (observed) sample mean and the hypothesised population value to the variability of the sample means due to random sampling.\nThis is called the t statistic and is computed as:\n\nt-statistic\n\\[\n\\begin{aligned}\nt\n&= \\frac{\\text{pattern}}{\\text{noise}} \\\\\n&= \\frac{\\text{observed - hypothesised mean}}{\\text{chance variation due to sampling}} \\\\\n&= \\frac{\\text{sample mean} - \\text{hypothesised mean}}{SE(\\text{sample mean})}\n\\end{aligned}\n\\]\nIn the formula above, the pattern is the difference between the sample (observed) mean and the hypothesised value in the null hypothesis.\nThe chance variation due to sampling is quantified by the standard error of the mean. The standard error (SE) is the standard deviation of the sampling distribution of the mean.Careful! This is NOT the SD of the original sample!\nWe typically decide that there is “something of interest going on” in the data that is worth reporting to the public if the t statistic is very far from 0, i.e. very large or very negative.\nIn those cases, the pattern is larger than the noise due to random sampling alone, and we say there is evidence against the null hypothesis and in favour of the alternative hypothesis.\nOn the other hand, if the t statistic is not big enough and fairly close to 0, the denominator in the fraction will be larger than the numerator, meaning that the noise (= natural chance variation due to random sampling) explained most of the variation in the data. In this case we say that there is not sufficient evidence against the null hypothesis and hence we do not reject it. That is, we maintain / fail to reject the status quo.\n\nCareful!\nNot finding sufficient evidence against the null hypothesis is NOT the same as finding evidence FOR the null hypothesis. You simply might not have enough data to assess whether the alternative hypothesis might be the correct one. So, because of not having enough data, you cannot distinguish which one of \\(H_0\\) or \\(H_1\\) is true. The test is inconclusive. Hence, the community agrees to use a conservative approach and maintain the null hypothesis.\n\nLet’s compute the t-statistic in our sample:\n\nM &lt;- mean(btdata$BodyTemp)\nM\n\n[1] 36.81111\n\nHyp &lt;- 37\nHyp\n\n[1] 37\n\nSE\n\n[1] 0.0601292\n\ntstat &lt;- (M - Hyp) / SE\ntstat\n\n[1] -3.141384\n\n\n\\[\nt\n= \\frac{36.81111 - 37}{0.0601292}\n= -3.141384\n\\]\nR does the computation above for you. So from now on, use R to do tests.\nThe call of the function is\nt.test(&lt;numeric vector&gt;, mu = &lt;hypothesis&gt;, alternative = &lt;direction&gt;)\nwhere\n\nnumeric vector should be the extracted column from the data table with the data;\nmu is the hypothesised value in the null hypothesis;\nalternative is the sign in the alternative hypothesis: ‘two.sided’ for \\(\\neq\\), ‘less’ for \\(&gt;\\) and ‘greater’ for \\(&gt;\\).\n\nThe result is:\n\nt.test(btdata$BodyTemp, mu = 37, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  btdata$BodyTemp\nt = -3.1414, df = 49, p-value = 0.002851\nalternative hypothesis: true mean is not equal to 37\n95 percent confidence interval:\n 36.69028 36.93195\nsample estimates:\nmean of x \n 36.81111 \n\n\nThe output gives us everything we need:\n\nt = -3.1414: the t-statistic\ndf = 49: the degrees of freedom = sample size - 1 = 50 - 1 = 49\nalternative hypothesis: true mean is not equal to 37: a reminder that the alternative hypothesis is \\(H_1 : \\mu \\neq 37 °C\\).\nA reminder that the sample mean in the original sample was 36.81111mean of x36.81111\n\nAs well as extra information:\n\np-value = 0.002851\n95 percent confidence interval: 36.69028 36.93195"
  },
  {
    "objectID": "04-insights-from-data.html#p-value",
    "href": "04-insights-from-data.html#p-value",
    "title": "4: Insights from data",
    "section": "P-value",
    "text": "P-value\nThe p-value is a measure of the strength of evidence that the data bring against the null hypothesis and in favour of the alternative hypothesis.\nIt is defined as the probability of obtaining sample results as extreme as, or more extreme than, the observed ones if the null hypothesis was true.\n\np-value\n\nUnderstanding the p-value\nImagine a world where the null hypothesis \\(H_0 : \\mu = 37 °C\\) is true — that is, the true mean body temperature for all healthy adults is 37 °C.\nNow, imagine obtaining many thousands of resamples, each of size 50, from the population. Each sample will have a sample mean and we can then plot the distribution of means.\nThis distribution is special, it was created assuming the null hypothesis to be true, and so it shows us the values of the sample mean we would expect if the null hypothesis was true in the population. It is important to notice that it is centred at the hypothesised value, in this case 37 °C.\nThe proportion of means in that distribution that are more distant to the hypothesised value (in either direction) than the actual sample mean obtained (36.81111) is the p-value.\n\n\n\n\n\n\n\n\n\nThe p-value is an important measure that science has used since the 1900s to quantify when the evidence that the data bring in favour of the alternative hypothesis is strong enough that it cannot simply be due to the variation in means due to random sampling (called natural variation), but it quantifies if there is “something else” going on, and that something else is the pattern of interest.\n\nGuidelines for evaluating strength of evidence from p-values:\n\n0.10 &lt; p-value\nnot much evidence against null hypothesis; null is plausible\n0.05 &lt; p-value \\(\\leq\\) 0.10\nmoderate evidence against the null hypothesis\n0.01 &lt; p-value \\(\\leq\\) 0.05\nstrong evidence against the null hypothesis\np-value \\(\\leq\\) 0.01\nvery strong evidence against the null hypothesis\n\nThe smaller the p-value, the stronger the evidence against the null hypothesis.\n\nThe p-values should be reported IN FULL — that is, with all its decimal places — unless it’s very very tiny, such as smaller than 0.001. In such cases the reader won’t be interested to the 6th or 7th decimal, so you can simply write p &lt; 0.001.\nAt the end of an analysis we may need to make a decision / recommend an action to policy makers. To actually make a decision there are some commonly used cutoffs for the p-value that are used to decide whether or not to reject the null hypotehsis. Typical values for this threshold are 0.05 or 0.01, depending on how strict you want to be. The threshold or cutoff value is called the significance level and is denoted by the greek letter \\(\\alpha\\).\nThe scientific investigation process requires you to decide on the significance level in advance, before analysing any data, and to clearly state it at the beginning of your reporting for the reader, as this chosen significance level will affect in all that will follow whether your results will be interpreted as significant or not.\n\nsignificance level\nIn our example, we would write up the results as follows:\nAt the 5% significance level, we performed a two-sided t-test against the null hypothesis that the mean body temperature for healthy humans is 37 °C (\\(t(49) = -3.141\\), \\(p = 0.003\\), two-sided). If the population mean body temperature was truly equal to 37 °C, the probability of obtaining a sample with a mean as extreme as, or more extreme than the obtained one (36.811) would be 3 in 1,000. In other words, only 3 samples out of 1,000 would have a mean as extreme as 36.811. The sample results provide very strong evidence that the population mean body temperature for healthy individuals is now different from 37 °C."
  },
  {
    "objectID": "04-insights-from-data.html#confidence-interval",
    "href": "04-insights-from-data.html#confidence-interval",
    "title": "4: Insights from data",
    "section": "Confidence interval",
    "text": "Confidence interval\nIf is important to always follow up a significant hypothesis test with a confidence interval.\nA confidence interval gives you a range of plausible value for the unknown parameter your are interested in. It is linked to the significance level \\(\\alpha\\) in the sense that if \\(\\alpha = 0.05\\), then the confidence interval is a 95% confidence interval. If \\(\\alpha = 0.01\\), the confidence interval is a 99% confidence interval.\n\nconfidence interval\nA 95% confidence interval (CI) gives you the range of values in the sampling distribution of the statistic (the mean in this case) that includes 95% of all sample means\n\n\n\n\n\n\n\n\nRecall the output from the t.test() function:\n95 percent confidence interval:\n 36.69028 36.93195\nWe are 95% confident that the mean body temperature for all healthy humans is between 36.69 °C and 36.93 °C.\n\n\n\n\n\n\nCIs and hypothesis tests\n\n\n\n\n\nThere is a deep link between confidence intervals and hypothesis tests. A confidence interval gives the range of plausible values that an unknown parameter of interest can take. A test of hypotheses checks whether an hypothesised value for the population parameter is plausible in light of the collected sample data.\nIf your hypothesised value in \\(H_0\\) lies inside the 95% confidence interval, then it is a plausible value for the population parameter and an hypothesis test at the 5% significance level will fail to reject the null.\nIf your hypothesised value in \\(H_0\\) falls outside the 95% confidence interval, then it is not a plausible value for the population parameter and an hypothesis test at the 5% significance level will reject the null.\nHowever, a CI does not tell you anything about the strength of evidence against the null hypothesis that the sample data bring. This is quantified by the p-value, provided by a test of hypotheses. Simply saying the hypothesised value is outside of the CI, it doesn’t tell you how far from the boundary it is. The furthest, the stronger the evidence, and this is why we also report p-values."
  },
  {
    "objectID": "04-insights-from-data.html#effect-size-cohens-d",
    "href": "04-insights-from-data.html#effect-size-cohens-d",
    "title": "4: Insights from data",
    "section": "Effect size: Cohen’s D",
    "text": "Effect size: Cohen’s D\nSometimes you may obtain a significant result, and think: “wow, amazing, I have made an impressive discovery!”\nHang on, it’s not that simple. Statistical significance does not guarantee practical significance or importance. These are two distinct concepts which should be understood. A common misunderstanding is thinking that significant has the same meaning as important. In science this is not the same!\n\n“Significant” means “Statistically significant”.\nA statistical significant result means that it is unlikely to happen just by random chance alone.\n“Important” means “Practically important”.\nThis means that the difference is large enough to matter in the real world and to decision makers.\n\nTo be clear, always use the adjective: statistically significant, practically important.\n\nThink about it\nDoes the fact that the true mean body temperature for all healthy humans in the population lies somewhere in between 36.69 °C and 36.93 °C, and in particular our best estimate 36.81 °C which was the sample mean, have any practical impact for decisions makers? Is it really such a big chance from the well-established 37 °C?\n\nEspecially with large sample sizes, even a fairly small difference between an observed and hypothesized value can be statistically significant. In such cases a confidence interval for the parameter can help to decide whether the difference is practically important.\nAlternatively, to quantify whether a difference between an observed and hypothesised value is important, in Psychology it is common to report also the effect size, also known as Cohen’s D.\nFirst, install the effectsize package:\ninstall.packages(\"effectsize\")\nThen use it as follows:\n\nlibrary(effectsize)\n\ncohens_d(btdata$BodyTemp, mu = 37)\n\nCohen's d |         95% CI\n--------------------------\n-0.44     | [-0.73, -0.15]\n\n- Deviation from a difference of 37.\n\n\nThe interpretation of whether the difference is of practical importance follows these heuristic cutoffs which were recommended by the author following extensive empirical investigations:\n\n\nCohen’s D\nEffect size\n\n\n\n\n\\(\\leq\\) 0.20\nSmall\n\n\n\n\\(\\approx\\) 0.50\nMedium\n\n\n\n\\(\\geq\\) 0.80\nLarge\n\n\n\nIn this case we are in the presence of a small effect, reinforcing that even if the results were statistical significance, i.e. unlikely to be due to random sampling variation alone, they are not of practical importance due to the very samll difference between the observed and hypothesised values."
  },
  {
    "objectID": "04-insights-from-data.html#validity-conditions-of-the-t-test",
    "href": "04-insights-from-data.html#validity-conditions-of-the-t-test",
    "title": "4: Insights from data",
    "section": "Validity conditions of the t-test",
    "text": "Validity conditions of the t-test\nThe conditions to satisfy for the CIs and p-values from the t-test to be trustworthy are as follows:\n\nData should be a simple random sample from population of interest\nEither one of 2a or 2b holds: 2a) The original sample follows a normal distribution. (This is a distribution similar to the t, and you do this test with the shapiro.test() function in R.) Or: 2b) You should have at least 30 observations in the sample, and the distribution of the data should not have strong outliers or be strongly skewed.\n\n\nNote.\nIt’s important to keep in mind that the above conditions are rough guidelines and not a guarantee.\n\nLet’s check each in turn:\n\nThe problem description says the data were collected on a random sample of 50 healthy subjects.\nLet’s start by checking 2a:\n\n\nshapiro.test(btdata$BodyTemp)\n\n\n    Shapiro-Wilk normality test\n\ndata:  btdata$BodyTemp\nW = 0.97322, p-value = 0.3115\n\n\nThe p-value is 0.31, which is larger than the chosen significance level of 5%, hence we do not reject the null hypothesis that the sample came from a normal distribution. The conditions for the validity of the t-test results are met.\nWe could then stop here. However, since this is to show you how to do everything, you would check 2b as follows:\n\ndim(btdata)\n\n[1] 50  2\n\n\nThe sample size is 50, which is larger than 30.\n\nggplot(btdata, aes(x = BodyTemp)) +\n    geom_histogram(color = 'white') +\n     geom_boxplot(color = 'darkorange')\n\n\n\n\n\n\n\nThe distribution of the data is roughly simmetric, with no strong outliers. The boxplot highlights one data point as an outlier, but the gap could easily be due to the sample size."
  },
  {
    "objectID": "04-insights-from-data.html#recommended-activities",
    "href": "04-insights-from-data.html#recommended-activities",
    "title": "4: Insights from data",
    "section": "Recommended activities",
    "text": "Recommended activities\n\n4.1 Exploratory Data Analysis\n4.2 Bar Charts\n4.3 Histograms\n4.4 Boxplots and counts"
  },
  {
    "objectID": "04-insights-from-data.html#footnotes",
    "href": "04-insights-from-data.html#footnotes",
    "title": "4: Insights from data",
    "section": "Footnotes",
    "text": "Footnotes\n\nShoemaker, A. L. (1996). What’s Normal: Temperature, Gender and Heartrate. Journal of Statistics Education, 4(2), 4.↩︎"
  },
  {
    "objectID": "05-making-decisions.html",
    "href": "05-making-decisions.html",
    "title": "5: Making decisions",
    "section": "",
    "text": "Research question\nCan a simple smile have an effect on punishment assigned following an infraction?\n\nResearchers LaFrance and Hecht (1995) conducted a study to examine the effect of a smile on the leniency of disciplinary action for wrongdoers. Participants in the experiment took on the role of members of a college disciplinary panel judging students accused of cheating. They were given, along with a description of the offence, a picture of the “suspect” who was either smiling or had a neutral facial expression. A leniency score (on a 10-point scale) was calculated based on the disciplinary decisions made by the participants. The full data can be found in the Smiles.csv dataset, also available at the following link: https://uoepsy.github.io/data/Smiles.csv\nThe experimenters have prior knowledge that smiling has a positive influence on people, and they are testing to see if the average lenience score is higher for smiling students than it is for students with a neutral facial expression (or, in other words, that smiling students are given more leniency and milder punishments.)"
  },
  {
    "objectID": "05-making-decisions.html#investigation-smiles-and-leniency",
    "href": "05-making-decisions.html#investigation-smiles-and-leniency",
    "title": "5: Making decisions",
    "section": "",
    "text": "Research question\nCan a simple smile have an effect on punishment assigned following an infraction?\n\nResearchers LaFrance and Hecht (1995) conducted a study to examine the effect of a smile on the leniency of disciplinary action for wrongdoers. Participants in the experiment took on the role of members of a college disciplinary panel judging students accused of cheating. They were given, along with a description of the offence, a picture of the “suspect” who was either smiling or had a neutral facial expression. A leniency score (on a 10-point scale) was calculated based on the disciplinary decisions made by the participants. The full data can be found in the Smiles.csv dataset, also available at the following link: https://uoepsy.github.io/data/Smiles.csv\nThe experimenters have prior knowledge that smiling has a positive influence on people, and they are testing to see if the average lenience score is higher for smiling students than it is for students with a neutral facial expression (or, in other words, that smiling students are given more leniency and milder punishments.)"
  },
  {
    "objectID": "05-making-decisions.html#data-reading",
    "href": "05-making-decisions.html#data-reading",
    "title": "5: Making decisions",
    "section": "Data reading",
    "text": "Data reading\nFirst, let’s read the data into R, and create an object storing the data called “smiles”:\n\nlibrary(tidyverse)\n\nsmiles &lt;- read_csv('https://uoepsy.github.io/data/Smiles.csv')\n\nRows: 68 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Group\ndbl (1): Leniency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(smiles)\n\n# A tibble: 6 × 2\n  Leniency Group\n     &lt;dbl&gt; &lt;chr&gt;\n1      7   smile\n2      3   smile\n3      6   smile\n4      4.5 smile\n5      3.5 smile\n6      4   smile\n\ndim(smiles)\n\n[1] 68  2\n\n\nThe data includes measurements on two variables (group and leniency) on 68 participants."
  },
  {
    "objectID": "05-making-decisions.html#data-visualisation",
    "href": "05-making-decisions.html#data-visualisation",
    "title": "5: Making decisions",
    "section": "Data visualisation",
    "text": "Data visualisation\nNext, let’s create a graph that compares the distributions of leniency scores between smiling and non-smiling participants:\n\nlibrary(patchwork) # to combine multiple ggplots into a single figure\n\nplt1 &lt;- ggplot(smiles, aes(x = Leniency, color = Group)) + \n    geom_density()\n\nplt2 &lt;- ggplot(smiles, aes(x = Group, y = Leniency)) + \n    geom_boxplot()\n\nplt1 | plt2\n\n\n\n\n\n\n\nThe left panel displays two right skewed distributions. The right panel shows that the median leniency score seems to be higher in the smiling group. There appears to be an outlier in the neutral face expression group, having a leniency score of approximately 8."
  },
  {
    "objectID": "05-making-decisions.html#data-summary",
    "href": "05-making-decisions.html#data-summary",
    "title": "5: Making decisions",
    "section": "Data summary",
    "text": "Data summary\nLet’s now compute the number of participants in each group, as well as the mean leniency score and its standard deviation in each group.\n\nstats &lt;- smiles %&gt;%\n  group_by(Group) %&gt;%\n  summarise(Count = n(), \n            M = mean(Leniency), \n            SD = sd(Leniency))\nstats\n\n# A tibble: 2 × 4\n  Group   Count     M    SD\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 neutral    34  4.12  1.52\n2 smile      34  4.91  1.68\n\n\nLet’s make the table a pretty HTML table with the gt package. We further format the numeric columns M and SD to be with 2 decimal places only:\n\nlibrary(knitr)\n\nkable(stats, digits = 2)\n\n\n\nGroup\nCount\nM\nSD\n\n\n\nneutral\n34\n4.12\n1.52\n\n\nsmile\n34\n4.91\n1.68\n\n\n\n\n\n\nThe argument digits = 2 displays numbers rounded to 2 decimal places. This is important for journal articles or reports.\nIn the sample, the mean leniency score was higher in the smiling group than the neutral expression group. The difference in mean leniency scores between the smiling and neutral group is 0.79.\nYou can get this by looking at the summary stats:\n\nstats\n\n# A tibble: 2 × 4\n  Group   Count     M    SD\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 neutral    34  4.12  1.52\n2 smile      34  4.91  1.68\n\n\nThe vector corresponding to the column M displays the neutral group first, then the smile group second\n\nstats$M\n\n[1] 4.117647 4.911765\n\n\nThe difference in means between the smiling (2nd group) and neutral group (1st) is:\n\ndiff_M &lt;- stats$M[2] - stats$M[1]\ndiff_M\n\n[1] 0.7941176"
  },
  {
    "objectID": "05-making-decisions.html#null-and-alternative-hypothesis",
    "href": "05-making-decisions.html#null-and-alternative-hypothesis",
    "title": "5: Making decisions",
    "section": "Null and alternative hypothesis",
    "text": "Null and alternative hypothesis\nThe null hypothesis is that facial expression has no effect on the punishment given, i.e. that the population mean leniency score for smiling students is the same as that of neutral students. In other words, there is no difference in population mean leniency score between smiling and neutral students.\nThe alternative hypothesis, instead, is the claim of interest to the researchers, i.e. that smiling results in a higher mean leniency score compared to non-smiling students.\nFrom the above, we can see that the researchers’ hypothesis that smiling students are given more leniency and milder punishments involves two parameters:\n\n\n\\(\\mu_s\\) = the population mean leniency score for the smiling students\n\n\\(\\mu_n\\) = the population mean leniency score for students with a neutral expression\n\nWe are now ready to formally write the hypotheses being tested:\n\\[H_0: \\mu_s = \\mu_n\\] \\[H_1: \\mu_s &gt; \\mu_n\\]\nThese can be equivalently written as follows:\n\\[H_0: \\mu_s - \\mu_n = 0\\] \\[H_1: \\mu_s - \\mu_n &gt; 0\\]\nNote that the statistical hypotheses is about the population difference in means (\\(\\mu_s - \\mu_n\\)) and not the sample difference in means, which we know as we have collected data for the sample (difference in means in the sample = 0.79).\nInstead, we do not have data for the entire population, and our interest lies in that difference in means for all those in the populations, so for data we don’t have.\nThis is why we need to perform a formal statistical test to check whether a difference of 0.79 (= the pattern) is large enough compared to the variation in the data due to random sampling (= the noise)."
  },
  {
    "objectID": "05-making-decisions.html#two-sample-t-test",
    "href": "05-making-decisions.html#two-sample-t-test",
    "title": "5: Making decisions",
    "section": "Two-sample t test",
    "text": "Two-sample t test\nTo compare the means across 2 groups, we will perform a two-sample t-test. The hypotheses involve two groups, “smile” and “neutral”.\nIn R this is performed with the t.test() function, but you need to be careful. You need to make the group variable into a factor, and clearly specify the order of the levels. The function t.test() will compute the difference between the mean of the first level - the the mean of the second level.\nFactor\nMake the column Group into a factor, the levels should be “smile” and “neutral”, and we want smile before neutral, so that t.test will do the mean of smile - mean of neutral.\n\nsmiles$Group &lt;- factor(smiles$Group, levels = c('smile', 'neutral'))\n\nEqual variances?\nDo the two groups have equal variances? Check this with a variance test var.test():\n\nvar.test(Leniency ~ Group, data = smiles)\n\n\n    F test to compare two variances\n\ndata:  Leniency by Group\nF = 1.2183, num df = 33, denom df = 33, p-value = 0.5738\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.6084463 2.4393924\nsample estimates:\nratio of variances \n          1.218294 \n\n\nYes, so we will set the var.equal = ... argument of t.test() to TRUE. If the p-value was significant, we would set var.equal = FALSE.\nt-test\nWe finally perform the t-test. In this case, our alternative hypothesis was directional, that the leniency was higher for smiling participants than non-smiling ones. The options for the alternative argument in R are alternative = “two.sided”, “less”, or “greater”. In our case we want “greater”:\n\nsmiles_test &lt;- t.test(Leniency ~ Group, data = smiles, var.equal = TRUE,\n                      alternative = \"greater\")\nsmiles_test\n\n\n    Two Sample t-test\n\ndata:  Leniency by Group\nt = 2.0415, df = 66, p-value = 0.0226\nalternative hypothesis: true difference in means between group smile and group neutral is greater than 0\n95 percent confidence interval:\n 0.1451938       Inf\nsample estimates:\n  mean in group smile mean in group neutral \n             4.911765              4.117647"
  },
  {
    "objectID": "05-making-decisions.html#understanding-the-output",
    "href": "05-making-decisions.html#understanding-the-output",
    "title": "5: Making decisions",
    "section": "Understanding the output",
    "text": "Understanding the output\nThe function returns the means of the two groups, which we already computed before in our table of summary statistics.\nsample estimates:\n  mean in group smile mean in group neutral \n             4.911765              4.117647 \n\nstats\n\n# A tibble: 2 × 4\n  Group   Count     M    SD\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 neutral    34  4.12  1.52\n2 smile      34  4.91  1.68\n\n\nIt gives us the t statistics, the degrees of freedom, and the p-value:\nt = 2.0415, df = 66, p-value = 0.0226\nRecall the t-statistic is the ratio of the pattern to noise, i.e.\n\\[\nt\n= \\frac{\\text{pattern}}{\\text{noise}}\n= \\frac{\\text{observed} - \\text{hypothesised difference}}{\\text{standard error}}\n\\]\nWe have the difference in means, computed before and stored into diff_M\n\ndiff_M\n\n[1] 0.7941176\n\n\nThe hypothesised difference in means between the smiling and neutral groups is 0, see the null hypothesis.\nThe standard error is obtained from the t test output:\n\nSE &lt;- smiles_test$stderr\nSE\n\n[1] 0.38898\n\n\nSo the t statistic is\n\n(diff_M - 0) / SE\n\n[1] 2.041538\n\n\nWhich matches the output of the t.test function!\nt = 2.0415, df = 66, p-value = 0.0226\nThe degrees of freedom is the sample size, 68, minus the two population means that we don’t know and need to be estimated: 68 - 2 = 66.\nThe p-value tells us the probability of observing a difference in means more extreme than the one we obtained in the sample, 0.79, if the null hypothesis were true. The p-value is smaller than the significance level of 0.05, meaning that we have strong evidence against the null hypothesis."
  },
  {
    "objectID": "05-making-decisions.html#reporting-the-results",
    "href": "05-making-decisions.html#reporting-the-results",
    "title": "5: Making decisions",
    "section": "Reporting the results",
    "text": "Reporting the results\nAt the 5% significance level, we performed a two-sample t-test against the null hypothesis that the difference in mean leniency score between smiling and non smiling students is equal to zero, with a one-sided alternative that the difference is greater than zero. The test results, \\(t(66) = 2.04, p = 0.02\\), one-sided, indicate that if smiling truly had no effect on leniency scores, the chance of getting a difference in mean leniency scores between smiling and neutral students as high as 0.79 is 0.02, or 2 in 100 times. The sample data provide strong evidence against the null hypothesis that smiling had no effect on leniency and in favour of the alternative.\nGeneral rule\nAlways report the test results in the context of the study. Link the numbers to the variables under investigation, and provide an interpretation on what the numbers mean in terms of the research question.\nThe actual t-test results are reported using t(df) = …, p = …\nYou should always report the p-value in full, perhaps rounded for consistency, unless the p-value is tiny. If it has many decimal places, the reader won’t be interested in the 6th or 7th decimal place. Report any p-value smaller than 0.001 as t(df) = …, p &lt; .001."
  },
  {
    "objectID": "05-making-decisions.html#validity-conditions-of-the-t-test",
    "href": "05-making-decisions.html#validity-conditions-of-the-t-test",
    "title": "5: Making decisions",
    "section": "Validity conditions of the t-test",
    "text": "Validity conditions of the t-test\nIn order for the results of the two-sample t-test to be valid, we need that:\n\nThe data arise from random assignment of subjects to two groups or from two independent random samples from the population\nEither: Each sample should be sufficiently large (\\(n_1 \\geq 30, n_2 \\geq 30\\) as a guideline) Or: Both samples came from normal distributions.\nShould be described by the study design.\nIs checked as follows\n\n\nsmiles %&gt;%\n    filter(Group == 'smile') %&gt;%\n    pull(Leniency) %&gt;%\n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94069, p-value = 0.06464\n\nsmiles %&gt;%\n    filter(Group == 'neutral') %&gt;%\n    pull(Leniency) %&gt;%\n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94253, p-value = 0.07335\n\n\nBoth groups appear to be consistent with samples from a normal distribution. The validity conditions are met.\nOn top of this, also in this case we have the other alternative condition to be true. Both distributions have enough data, 34 in each group, no strong outliers or skeweness:\n\nggplot(smiles, aes(x = Group, y = Leniency)) +\n    geom_boxplot()"
  },
  {
    "objectID": "05-making-decisions.html#recommended-activities",
    "href": "05-making-decisions.html#recommended-activities",
    "title": "5: Making decisions",
    "section": "Recommended activities",
    "text": "Recommended activities\n\n5.1 Customise plots\n5.2 Video: Introduction to RMarkdown"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "Dr Umberto Noè\nDepartment of Psychology\nThe University of Edinburgh\n\n\n\n Back to top"
  },
  {
    "objectID": "cpt-data.html",
    "href": "cpt-data.html",
    "title": "Checkpoint: Data",
    "section": "",
    "text": "Key question\nCan we trust self-reported voting turnout?\nConsider the data stored in the file turnout.csv. These represent measurements on US election turnout data and will be used to investigate whether there is a bias in self-reported voting turnout. In a nutshell, people are becoming more concerned about accuracy of answers to post-election surveys as people might lie about about whether or not they voted due to social desirability bias. Perhaps the respondent felt like they should have voted when in fact they didn’t vote. You will investigate whether this sort of bias is present in the survey conducted by the American National Election Studies (ANES).\nVariable\n      Description\n    \n\n\nyear\nelection year\n\n\nANES\nANES estimated turnout rate\n\n\nVEP\nvoting eligible population (in thousands)\n\n\nVAP\nvoting age population (in thousands)\n\n\ntotal\n\n                    total ballots cast for highest office (in thousands)\n\n\nfelons\ntotal ineligible felons (in thousands)\n\n\nnoncitizens\ntotal noncitizens (in thousands)\n\n\noverseas\ntotal eligible overseas voters (in thousands)\n\n\nosvoters\ntotal ballots counted by overseas voters (in thousands)\nHow do we measure turnout rates? The numerator should be the total votes that were cast, while we have two choices for the denominator:\nBoth VAP and VEP do not count overseas voters, so if those data are available we may want to use it. Furthermore, we have that\n\\[\n\\text{VEP = VAP + overseas voters} - \\text{ineligible voters}\n\\]\nwhere:"
  },
  {
    "objectID": "cpt-data.html#the-in-function.",
    "href": "cpt-data.html#the-in-function.",
    "title": "Checkpoint: Data",
    "section": "The %in% function.",
    "text": "The %in% function.\nWhat if we want to check whether each value in a vector is found in another collection of values?\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ncollection &lt;- c(4, 8)\ncollection\n\n[1] 4 8\n\nx %in% collection\n\n [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n\nAs you can see from the output above, %in% checks whether each value appears among the collection. If it does, it returns TRUE and, if it doesn’t, FALSE.\nYou can also use the logical TRUE/FALSE values for logical indexing.\n\nx[x %in% collection]\n\n[1] 4 8"
  },
  {
    "objectID": "cpt-data.html#filtering-rows-in-a-dataset",
    "href": "cpt-data.html#filtering-rows-in-a-dataset",
    "title": "Checkpoint: Data",
    "section": "Filtering rows in a dataset",
    "text": "Filtering rows in a dataset\nLet’s create a data table df with two columns: X, containing the letters “a” and “b” each repeated 5 times, and Y containing the whole numbers from 1 to 10:\n\ndf &lt;- tibble(\n  X = rep(c('a', 'b', 'c', 'd'), each = 5),\n  Y = 1:20\n)\ndf\n\n# A tibble: 20 × 2\n   X         Y\n   &lt;chr&gt; &lt;int&gt;\n 1 a         1\n 2 a         2\n 3 a         3\n 4 a         4\n 5 a         5\n 6 b         6\n 7 b         7\n 8 b         8\n 9 b         9\n10 b        10\n11 c        11\n12 c        12\n13 c        13\n14 c        14\n15 c        15\n16 d        16\n17 d        17\n18 d        18\n19 d        19\n20 d        20\n\n\nLet’s keep the rows where X is either b or d\n\ndf[df$X %in% c('b', 'd'), ]\n\n# A tibble: 10 × 2\n   X         Y\n   &lt;chr&gt; &lt;int&gt;\n 1 b         6\n 2 b         7\n 3 b         8\n 4 b         9\n 5 b        10\n 6 d        16\n 7 d        17\n 8 d        18\n 9 d        19\n10 d        20\n\n\nor, to avoid always writing the data name we can use the function filter from the library tidyverse, which automatically looks for the column X inside the data df:\n\nfilter(df, X %in% c('b', 'd'))\n\n# A tibble: 10 × 2\n   X         Y\n   &lt;chr&gt; &lt;int&gt;\n 1 b         6\n 2 b         7\n 3 b         8\n 4 b         9\n 5 b        10\n 6 d        16\n 7 d        17\n 8 d        18\n 9 d        19\n10 d        20\n\n\nor\n\ndf %&gt;%\n  filter(X %in% c('b', 'd'))\n\n# A tibble: 10 × 2\n   X         Y\n   &lt;chr&gt; &lt;int&gt;\n 1 b         6\n 2 b         7\n 3 b         8\n 4 b         9\n 5 b        10\n 6 d        16\n 7 d        17\n 8 d        18\n 9 d        19\n10 d        20\n\n\nQ4\n\n\nExercise\nAnswer\n\n\n\nPresidential elections occur every 4 years. Split the data into two, one for presidential and one for midterm elections.\nDoes the bias of the ANES estimates vary across election types?\n\n\n\nturnout$year\n\n [1] 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2008\n\nyrs_presid &lt;- seq(1980, 2008, by = 4)\nyrs_presid\n\n[1] 1980 1984 1988 1992 1996 2000 2004 2008\n\nturnout$year %in% yrs_presid\n\n [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[13]  TRUE  TRUE\n\npresid &lt;- filter(turnout, year %in% yrs_presid)\npresid\n\n# A tibble: 8 × 13\n   year    VEP    VAP  total  ANES felons noncit overseas osvoters VAPtr VEPtr\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1980 159635 164445  86515    71    802   5756     1803       NA  52.0  54.2\n2  1984 167702 173995  92653    74   1165   7482     2361       NA  52.5  55.2\n3  1988 173579 181955  91595    70   1594   9280     2257       NA  49.7  52.8\n4  1992 179656 190778 104405    75   2183  11447     2418       NA  54.0  58.1\n5  1996 186347 200016  96263    73   2586  13601     2499       NA  47.5  51.7\n6  2000 194331 210623 105375    73   3083  16218     2937       NA  49.3  54.2\n7  2004 203483 220336 122295    77   3158  18068     3862       NA  54.5  60.1\n8  2008 213314 230872 131304    78   3145  19392     4972      263  55.7  61.6\n# ℹ 2 more variables: diffVAP &lt;dbl&gt;, diffVEP &lt;dbl&gt;\n\nmidterm &lt;- filter(turnout, !(year %in% yrs_presid))\nmidterm\n\n# A tibble: 6 × 13\n   year    VEP    VAP total  ANES felons noncit overseas osvoters VAPtr VEPtr\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1982 160467 166028 67616    60    960   6641     1982       NA  40.2  42.1\n2  1986 170396 177922 64991    53   1367   8362     2216       NA  36.1  38.1\n3  1990 176629 186159 67859    47   1901  10239     2659       NA  35.9  38.4\n4  1994 182623 195258 75106    56   2441  12497     2229       NA  38.0  41.1\n5  1998 190420 205313 72537    52   2920  14988     2937       NA  34.8  38.1\n6  2002 198382 215462 78382    62   3168  17237     3308       NA  35.8  39.5\n# ℹ 2 more variables: diffVAP &lt;dbl&gt;, diffVEP &lt;dbl&gt;\n\n\nUsing the VEP turnout rate:\n\nmean(presid$ANES - presid$VEPtr)\n\n[1] 17.892\n\nmean(midterm$ANES - midterm$VEPtr)\n\n[1] 15.4288\n\n\nUsing the VAP turnout rate:\n\nmean(presid$ANES - presid$VAPtr)\n\n[1] 21.94519\n\nmean(midterm$ANES - midterm$VAPtr)\n\n[1] 18.17441\n\n\nUsing both the VEP or VAP estimates of the turnout rate, it seems that the bias is higher in the presidential elections than the midterm elections.\n\n\n\nQ5\n\n\nExercise\nAnswer\n\n\n\nDivide the data into half by election years such that you subset the data into two periods.\nCalculate the difference between the ANES turnout rate and the VEP turnout rate separately for each year within each period.\nHas the bias of ANES increased over time?\n\n\n\nturnout\n\n# A tibble: 14 × 13\n    year    VEP    VAP  total  ANES felons noncit overseas osvoters VAPtr VEPtr\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1980 159635 164445  86515    71    802   5756     1803       NA  52.0  54.2\n 2  1982 160467 166028  67616    60    960   6641     1982       NA  40.2  42.1\n 3  1984 167702 173995  92653    74   1165   7482     2361       NA  52.5  55.2\n 4  1986 170396 177922  64991    53   1367   8362     2216       NA  36.1  38.1\n 5  1988 173579 181955  91595    70   1594   9280     2257       NA  49.7  52.8\n 6  1990 176629 186159  67859    47   1901  10239     2659       NA  35.9  38.4\n 7  1992 179656 190778 104405    75   2183  11447     2418       NA  54.0  58.1\n 8  1994 182623 195258  75106    56   2441  12497     2229       NA  38.0  41.1\n 9  1996 186347 200016  96263    73   2586  13601     2499       NA  47.5  51.7\n10  1998 190420 205313  72537    52   2920  14988     2937       NA  34.8  38.1\n11  2000 194331 210623 105375    73   3083  16218     2937       NA  49.3  54.2\n12  2002 198382 215462  78382    62   3168  17237     3308       NA  35.8  39.5\n13  2004 203483 220336 122295    77   3158  18068     3862       NA  54.5  60.1\n14  2008 213314 230872 131304    78   3145  19392     4972      263  55.7  61.6\n# ℹ 2 more variables: diffVAP &lt;dbl&gt;, diffVEP &lt;dbl&gt;\n\nfirst_period &lt;- turnout[1:7, ]\nsecond_period &lt;- turnout[8:14, ]\n\nmean(first_period$ANES) - mean(first_period$VEPtr)\n\n[1] 15.85378\n\nmean(second_period$ANES) - mean(second_period$VEPtr)\n\n[1] 17.81891\n\n\nYes, it appears that in the second half of the data the bias was slightly higher."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "A compact course to take you from zero to R-hero!\nThe course involves a series of self-paced lessons, see the list below. At the end of each lesson, you will find a list of recommended activities and readings, which you are warmly encouraged to complete.\nA tip: make sure to type along the code you see, rather than simply reading it and thinking “oh I understand it, hence I can surely do it without guidance”. Programming is learnt by doing.\n\nLesson 0: Getting started\nLesson 1: R basics\nLesson 2: Working with data\nLesson 3: Exploring data\nLesson 4: Insights from data\nLesson 5: Making decisions\n\n\n\n\n Back to top"
  },
  {
    "objectID": "next.html",
    "href": "next.html",
    "title": "What’s next",
    "section": "",
    "text": "Now you have completed all lessons, well done!\nHowever, it’s not over. R is an evolving language, and new things added every day. Now it’s on you to keep up to date. A good way to keep up to date is to regularly check the following websites:\n\nR Views blog\nR Bloggers webpage\nR posts you might have missed!\n\nAlso remember to update R from time to time, and also to check for RStudio updates.\nGood luck and best wishes in your career!\n\n\n\n Back to top"
  }
]